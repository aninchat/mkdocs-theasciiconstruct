{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to The ASCII Construct","text":"<p>My name is Aninda Chatterjee and I am a network engineer. I run this site to share my experiences across various networking domains that I have worked in, including a wide range of technologies and solutions such as Cisco SD-Access, EVPN VXLAN data centers, network infrastructure for AI clusters.</p> <p>I am a published author, having written the book Deploying Juniper Data Centers with EVPN VXLAN, which can be bought direct from Pearson here or from the US Amazon store here. I am in the process of writing a second book for Pearson on mastering advanved Juniper data center deployments.</p> <p>You can also support the work on this website and its upkeep by buying me a coffee:</p> <p></p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2021/12/05/cumulus-basics-part-i---navigating-the-os/","title":"Cumulus Basics Part I - navigating the OS","text":"<p>This first blog on Cumulus introduces the reader to the basics of the operating system and Cumulus' NCLU.</p>","tags":["cumulus"]},{"location":"blog/2021/12/05/cumulus-basics-part-i---navigating-the-os/#introduction","title":"Introduction","text":"<p>This is going to be a new mini series that we will do in preparation for the first open networking certification that Cumulus Networks introduced,see here. </p> <p>Cumulus Networks has a great page (both free and paid content) where you can spend some time and learn all things Cumulus and open networking related, see here.</p> <p>So, where do we begin? Every time I learn a new product, I start at the start. Things we learned way back when. Because Cumulus Linux is a native Linux distribution (and it's interface may be unfamiliar to many), we'll start with some very simple aspects of working with the box - basic port bring up/down, port configurations, gathering information about a port and finally an introduction to Cumulus' NCLU! </p>","tags":["cumulus"]},{"location":"blog/2021/12/05/cumulus-basics-part-i---navigating-the-os/#topology","title":"Topology","text":"<p>As a reference, we'll be working on the following topology:</p> <p></p>","tags":["cumulus"]},{"location":"blog/2021/12/05/cumulus-basics-part-i---navigating-the-os/#basic-interface-configuration","title":"Basic interface configuration","text":"<p>With Linux networking, your interface configurations would be found in /etc/network/interfaces (there are several helpful pages that you can google and find to understand the syntax in this file so we're not going to go over that in too much detail). You can quickly view this via the 'cat' option.</p> <p>On both boxes, with a blank configuration, we see:</p> <pre><code>cumulus@cumulus:~$ cat /etc/network/interfaces\n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*.intf\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto eth0\niface eth0 inet dhcp\n</code></pre> <p>This file can directly be modified to change the configuration of various interface or to introduce new logical interfaces. To demonstrate this, let's go ahead and configure swp1 on each box to be a L3 interface with an IP address in the subnet 10.0.0.0/24. </p> <p>SW1:</p> <pre><code>cumulus@cumulus:/$ cat /etc/network/interfaces\n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*.intf\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto eth0\niface eth0 inet dhcp\n\nauto swp1\niface swp1\n    address 10.0.0.1/24\n</code></pre> <p>SW2:</p> <pre><code>cumulus@cumulus:/$ cat /etc/network/interfaces\n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*.intf\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto eth0\niface eth0 inet dhcp\n\nauto swp1\niface swp1\n    address 10.0.0.2/24\n</code></pre> <p>Bring these interfaces up using the 'ifup' option of the ifupdown2 module on both the switches.</p> <pre><code>cumulus@cumulus:/$ sudo ifup swp1\n</code></pre> <p>To confirm the status of the interface, you can use 'ip link show' to list all interfaces or specify a particular interface to look at using 'ip link show swp1'.</p> <pre><code>cumulus@cumulus:~$ ip link show\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:00 brd ff:ff:ff:ff:ff:ff\n3: swp1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:01 brd ff:ff:ff:ff:ff:ff\n4: swp2: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:02 brd ff:ff:ff:ff:ff:ff\n5: swp3: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:03 brd ff:ff:ff:ff:ff:ff\n6: swp4: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:04 brd ff:ff:ff:ff:ff:ff\n7: swp5: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:05 brd ff:ff:ff:ff:ff:ff\n8: swp6: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:06 brd ff:ff:ff:ff:ff:ff\n</code></pre> <p>Looking at only swp1:</p> <pre><code>cumulus@cumulus:~$ ip link show swp1\n3: swp1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:01 brd ff:ff:ff:ff:ff:ff\n</code></pre> <p>An interesting thing to note here - even if you shut down one side of the link (say, do a 'sudo ifdown swp1' on SPINE1), the other side would still show that the link is up. This behavior is specific to Cumulus VX in virtualized environments only and was confirmed by Cumulus folks (this is not just a Cumulus issue - this behavior is typical of virtual routers/switches and is seen across vendors).</p> <p>\"When using VBox there is a little switch in the middle of the link that holds it up.\" </p> <p>You can poke this middle switch to bring the links down if you're specifically testing link failures. But we're not going to get into that here. </p> <p>Clearly, manipulating these network configuration files can be a little tiresome and more importantly, prone to human error. You need to be aware of the kind of syntax that is used within these files and all of the different intricacies that go into the configuration here.</p>","tags":["cumulus"]},{"location":"blog/2021/12/05/cumulus-basics-part-i---navigating-the-os/#cumulus-nclu-introduction","title":"Cumulus NCLU introduction","text":"<p>This is where Cumulus' NCLU comes in. NCLU (Network Command Line Utility) is essentially a CLI that takes you away from manual manipulation of network files and provides a helpful CLI for the same instead. </p> <p>All NCLU commands start with 'net'. You can tab or use a question mark to get all the available options. </p> <pre><code>cumulus@SPINE1:~$ net \n    abort     :  abandon changes in the commit buffer\n    add       :  add/modify configuration\n    clear     :  clear counters, BGP neighbors, etc\n    commit    :  apply the commit buffer to the system\n    del       :  remove configuration\n    example   :  detailed examples of common workflows\n    help      :  context sensitive information; see section below\n    pending   :  show changes staged in the commit buffer\n    rollback  :  revert to a previous configuration state\n    show      :  show command output\n</code></pre> <p>To add configuration to an interface, you can use the 'net add interface' CLI. For example, instead of manipulating the /etc/network/interfaces file directly to add an IP address to swp1, I can do this instead:</p> <pre><code>cumulus@cumulus:~$ net add interface swp1 ip address 10.0.0.1/24\n</code></pre> <p>NCLU has three steps to it:</p> <ol> <li>Configure using 'net add [del]' commands.</li> <li>Confirm what is going to be configured using 'net pending'.</li> <li>Push this configuration using 'net commit'.</li> </ol> <p>A complete example of configuring an IP address on swp1 follows:</p> <pre><code>// net add\n\ncumulus@SPINE1:/$ net add interface swp1 ip address 10.0.0.1/24\n\n// net pending\n\ncumulus@SPINE1:~$ net pending\n--- /etc/network/interfaces     2019-03-05 16:40:58.268594116 +0000\n+++ /run/nclu/ifupdown2/interfaces.tmp  2019-03-05 17:23:54.585798862 +0000\n@@ -6,14 +6,12 @@\n # The loopback network interface\n auto lo\n iface lo inet loopback\n\n # The primary network interface\n auto eth0\n iface eth0 inet dhcp\n\n auto swp1\n iface swp1\n-       address 10.1.1.1/24\n-\n-\n-\n+    address 10.0.0.1/24\n+    address 10.1.1.1/24\n\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  ---------------------------------------------\ncumulus  2019-03-05 17:23:51.321383  net add interface swp1 ip address 10.0.0.1/24\n\n// net commit\n\ncumulus@SPINE1:~$ net commit\n--- /etc/network/interfaces     2019-03-05 16:40:58.268594116 +0000\n+++ /run/nclu/ifupdown2/interfaces.tmp  2019-03-05 17:24:00.025799294 +0000\n@@ -6,14 +6,12 @@\n # The loopback network interface\n auto lo\n iface lo inet loopback\n\n # The primary network interface\n auto eth0\n iface eth0 inet dhcp\n\n auto swp1\n iface swp1\n-       address 10.1.1.1/24\n-\n-\n-\n+    address 10.0.0.1/24\n+    address 10.1.1.1/24\n\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  ---------------------------------------------\ncumulus  2019-03-05 17:23:51.321383  net add interface swp1 ip address 10.0.0.1/24\n</code></pre> <p>Note</p> <p>From a link logging perspective, this is done via rsyslog. The defaults can be found in '/etc/rsyslog.d'. 22-linkstate.conf contains information on where link state changes are logged. By default, this goes to /var/log/linkstate. However, with Cumulus VX, you'd soon realize that there is no 'linkstate' file in /var/log. This is because switchd is responsible for this and switchd doesn't do much in Cumulus VX (as it is largely just an interface for the ASIC). Due to this, you cannot track link state changes in Cumulus VX. Shoutout to Eric Pulvino for clarifying this.  </p> <p>We will continue with bridging on Cumulus VX in part II. Each post in this series will show both NCLU configurations and manual changes needed to the relevant network files.</p>","tags":["cumulus"]},{"location":"blog/2021/12/07/cumulus-part-ii---bridging/","title":"Cumulus Part II - Bridging","text":"<p>This second blog on Cumulus looks at basic layer2 functionality in Cumulus Linux.</p> <p>This post is going to introduce you to basic Layer2 functionality on the Cumulus platform. Like before, we are going to be working with Cumulus VX. </p>","tags":["cumulus"]},{"location":"blog/2021/12/07/cumulus-part-ii---bridging/#topology","title":"Topology","text":"<p>We will be using the following network topology for this post:</p> <p></p> <p>PC1 and PC2 are two end clients in the same Layer2 domain (VLAN 10) that want to communicate with each other.</p>","tags":["cumulus"]},{"location":"blog/2021/12/07/cumulus-part-ii---bridging/#bridges-in-cumulus-linux","title":"Bridges in Cumulus Linux","text":"<p>Cumulus uses the traditional concept of a Linux bridge for bridging. It also allows you to configure a VLAN-aware bridge which is what we will be using today (more documentation about this can be found here. </p> <p>A 'bridge' interface will be created when the first VLAN is created on the system. So, let's start by creating a VLAN using Cumulus' NCLU and looking at the relevant changes that will be made to the /etc/network/interfaces file:</p> <pre><code>// create a VLAN\n\ncumulus@SPINE1:~$ net add vlan 10 \n\n// view changes that will be made and to what file using 'net pending'\n\ncumulus@SPINE1:~$ net pending\n--- /etc/network/interfaces     2018-11-13 18:46:52.000000000 +0000\n+++ /run/nclu/ifupdown2/interfaces.tmp  2019-03-25 15:05:05.712745615 +0000\n@@ -3,10 +3,15 @@\n\n source /etc/network/interfaces.d/*.intf\n\n # The loopback network interface\n auto lo\n iface lo inet loopback\n\n # The primary network interface\n auto eth0\n iface eth0 inet dhcp\n+\n+auto bridge\n+iface bridge\n+    bridge-vids 10\n+    bridge-vlan-aware yes\n\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  ---------------\ncumulus  2019-03-25 15:04:49.012290  net add vlan 10\n</code></pre> <p>From the above, you can see that a 'bridge' interface is created by adding 'iface bridge' and under this, you specify what VLANs are part of this bridge using 'bridge-vids &lt;&gt;'. The 'bridge-vlan-aware yes' option under the bridge makes this a VLAN-aware bridge as opposed to a traditional Linux bridge. </p> <p>Do the same thing on SPINE2 as well. </p> <p>Now, let's start mapping these VLANs to our interfaces. We want swp3 on SPINE1 and swp4 on SPINE2 to be untagged, host facing interfaces in VLAN 10 while the interconnection between SPINE1 and SPINE2 (via swp1) needs to allow multiple VLANs, making it a trunk. </p> <pre><code>// map bridge VLANs to interfaces\n\ncumulus@SPINE1:~$ net add interface swp3 bridge access 10 \ncumulus@SPINE1:~$ net add interface swp1 bridge trunk vlan 10-20\n\n// view changes that will be made and to what file using 'net pending'\n\ncumulus@SPINE1:~$ net pending\n--- /etc/network/interfaces     2018-11-13 18:46:52.000000000 +0000\n+++ /run/nclu/ifupdown2/interfaces.tmp  2019-03-25 15:08:04.817750296 +0000\n@@ -3,10 +3,24 @@\n\n source /etc/network/interfaces.d/*.intf\n\n # The loopback network interface\n auto lo\n iface lo inet loopback\n\n # The primary network interface\n auto eth0\n iface eth0 inet dhcp\n+\n+auto swp1\n+iface swp1\n+    bridge-vids 10-20\n+\n+auto swp3\n+iface swp3\n+    bridge-access 10\n+\n+auto bridge\n+iface bridge\n+    bridge-ports swp1 swp3\n+    bridge-vids 10-20\n+    bridge-vlan-aware yes\n\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  -----------------------------------------------\ncumulus  2019-03-25 15:04:49.012290  net add vlan 10\ncumulus  2019-03-25 15:07:10.463736  net add interface swp3 bridge access 10\ncumulus  2019-03-25 15:08:01.690646  net add interface swp1 bridge trunk vlans 10-20\n</code></pre> <p>Notice how under the bridge interface, there is an interface mapping that is created using 'bridge-ports'. This tells the bridge what interfaces are part of it.</p> <p>Similar configuration needs to be done on SPINE2. Finally, we need to do a 'net commit' to commit these changes. </p> <p>At this point, PC1 can successfully ping PC2:</p> <pre><code>PC-1&gt; ping 10.1.1.2\n84 bytes from 10.1.1.2 icmp_seq=1 ttl=64 time=3.108 ms\n84 bytes from 10.1.1.2 icmp_seq=2 ttl=64 time=2.412 ms\n84 bytes from 10.1.1.2 icmp_seq=3 ttl=64 time=1.585 ms\n\n*snip*\n</code></pre>","tags":["cumulus"]},{"location":"blog/2021/12/07/cumulus-part-ii---bridging/#final-configuration","title":"Final configuration","text":"<p>Let's take a final look at the configuration of both SPINE1 and SPINE2 that was needed to make this work:</p> <pre><code>SPINE1:\n\ncumulus@SPINE1:~$ cat /etc/network/interfaces\n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*.intf\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto eth0\niface eth0 inet dhcp\n\nauto swp1\niface swp1\n    bridge-vids 10-20\n\nauto swp3\niface swp3\n    bridge-access 10\n\nauto bridge\niface bridge\n    bridge-ports swp1 swp3\n    bridge-vids 10-20\n    bridge-vlan-aware yes\n\nSPINE2:\n\ncumulus@SPINE2:~$ cat /etc/network/interfaces\n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*.intf\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto eth0\niface eth0 inet dhcp\n\nauto swp1\niface swp1\n    bridge-vids 10-20\n\nauto swp4\niface swp4\n    bridge-access 10\n\nauto bridge\niface bridge\n    bridge-ports swp1 swp4\n    bridge-vids 10-20\n    bridge-vlan-aware yes\n</code></pre> <p>I hope this was informative. See y'all in the next post!</p>","tags":["cumulus"]},{"location":"blog/2021/12/09/cumulus-basics-part-iv---bgp-introduction/","title":"Cumulus Basics Part IV - BGP introduction","text":"<p>In this post, we introduce BGP on Cumulus Linux.</p>","tags":["cumulus","bgp"]},{"location":"blog/2021/12/09/cumulus-basics-part-iv---bgp-introduction/#introduction","title":"Introduction","text":"<p>The goal of this post is to introduce BGP on Cumulus Linux and then move towards a BGP unnumbered design, in the following post. </p>","tags":["cumulus","bgp"]},{"location":"blog/2021/12/09/cumulus-basics-part-iv---bgp-introduction/#topology","title":"Topology","text":"<p>We'll be using the following network topology for this post:</p> <p></p> <p>First, we will try to create a traditional BGP scenario with OSPF as an IGP. For now, OSPF is up and running and I have learnt the loopback of each LEAF switch.</p> <pre><code>// RIB lookup on LEAF1 for LEAF2s loopback\n\ncumulus@LEAF1:~$ net show route 2.2.2.2\nRIB entry for 2.2.2.2\n=====================\nRouting entry for 2.2.2.2/32\n  Known via \"ospf\", distance 110, metric 200, best\n  Last update 00:02:22 ago\n  * 172.16.11.11, via swp1\n  * 172.16.12.22, via swp2\n\n\nFIB entry for 2.2.2.2\n=====================\n2.2.2.2  proto ospf  metric 20 \n        nexthop via 172.16.11.11  dev swp1 weight 1\n        nexthop via 172.16.12.22  dev swp2 weight 1 \n\n// RIB lookup on LEAF1 for LEAF3s loopback\n\ncumulus@LEAF1:~$ net show route 3.3.3.3\nRIB entry for 3.3.3.3\n=====================\nRouting entry for 3.3.3.3/32\n  Known via \"ospf\", distance 110, metric 200, best\n  Last update 00:02:54 ago\n  * 172.16.11.11, via swp1\n  * 172.16.12.22, via swp2\n\n\nFIB entry for 3.3.3.3\n=====================\n3.3.3.3  proto ospf  metric 20 \n        nexthop via 172.16.11.11  dev swp1 weight 1\n        nexthop via 172.16.12.22  dev swp2 weight 1 \n</code></pre> <p>I can use these loopbacks now to form an iBGP session between LEAF1 and LEAF2 to provide connectivity between PC1 and PC2.</p> <pre><code>// LEAF1 configuration\n\ncumulus@LEAF1:~$ net add bgp autonomous-system 1\ncumulus@LEAF1:~$ net add bgp router-id 1.1.1.1  \ncumulus@LEAF1:~$ net add bgp neighbor 2.2.2.2 remote-as 1\ncumulus@LEAF1:~$ net add bgp neighbor 2.2.2.2 update-source lo\ncumulus@LEAF1:~$ net commit \n\n// LEAF2 configuration\n\ncumulus@LEAF2:~$ net add bgp autonomous-system 1\ncumulus@LEAF2:~$ net add bgp router-id 2.2.2.2\ncumulus@LEAF2:~$ net add bgp neighbor 1.1.1.1 remote-as 1\ncumulus@LEAF2:~$ net add bgp neighbor 1.1.1.1 update-source lo\ncumulus@LEAF2:~$ net commit  \n</code></pre> <p>An iBGP session is now up between the two:</p> <pre><code>cumulus@LEAF1:~$ net show bgp ipv4 unicast summary \nBGP router identifier 1.1.1.1, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 1, using 19 KiB of memory\n\nNeighbor        V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLEAF2(2.2.2.2)  4          1      21      21        0    0    0 00:00:56            0\n</code></pre> <p>Advertise the host subnets now into BGP:</p> <pre><code>cumulus@LEAF1:~$ net add bgp network 10.1.1.0/24\ncumulus@LEAF1:~$ net commit \n\ncumulus@LEAF2:~$ net add bgp network 20.1.1.0/24\ncumulus@LEAF2:~$ net commit    \n\n// IPv4 unicast BGP table on LEAF1\n\ncumulus@LEAF1:~$ net show bgp ipv4 unicast \nBGP table version is 2, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, = multipath,\n              i internal, r RIB-failure, S Stale, R Removed\nOrigin codes: i - IGP, e - EGP, ? - incomplete\n\n   Network          Next Hop            Metric LocPrf Weight Path\n*&gt; 10.1.1.0/24      0.0.0.0                  0         32768 i\n*&gt;i20.1.1.0/24      2.2.2.2                  0    100      0 i\n\nDisplayed  2 routes and 2 total paths \n\n// IPv4 unicast BGP table on LEAF2\n\ncumulus@LEAF2:~$ net show bgp ipv4 unicast                  \nBGP table version is 2, local router ID is 2.2.2.2\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, = multipath,\n              i internal, r RIB-failure, S Stale, R Removed\nOrigin codes: i - IGP, e - EGP, ? - incomplete\n\n   Network          Next Hop            Metric LocPrf Weight Path\n*&gt;i10.1.1.0/24      1.1.1.1                  0    100      0 i\n*&gt; 20.1.1.0/24      0.0.0.0                  0         32768 i\n</code></pre> <p>Let's try and ping from PC1 to PC2 now:</p> <pre><code>PC1&gt; ping 20.1.1.1\n\n20.1.1.1 icmp_seq=1 timeout\n20.1.1.1 icmp_seq=2 timeout\n20.1.1.1 icmp_seq=3 timeout\n20.1.1.1 icmp_seq=4 timeout\n20.1.1.1 icmp_seq=5 timeout\n</code></pre> <p>All pings timeout. This is a common problem that you may run into -  look at what is happening here with more thought. PC1 sources a packet with 10.1.1.1 with an IP destination of 20.1.1.1, with an ICMP header trailing it. This reaches the default gateway, LEAF1. LEAF1 does a mac lookup, realizes it owns the destination mac and thus moves into the IP header to do a RIB lookup and forwards it towards SPINE1 (assuming the packet hash is in such a way that it goes to SPINE1).</p> <pre><code>// RIB lookup on LEAF1 for 20.1.1.1\n\ncumulus@LEAF1:~$ net show route 20.1.1.1\nRIB entry for 20.1.1.1\n======================\nRouting entry for 20.1.1.0/24\n  Known via \"bgp\", distance 200, metric 0, best\n  Last update 00:03:17 ago\n    2.2.2.2 (recursive)\n  *   172.16.11.11, via swp1\n  *   172.16.12.22, via swp2\n\n\nFIB entry for 20.1.1.1\n======================\n20.1.1.0/24  proto bgp  metric 20 \n        nexthop via 172.16.11.11  dev swp1 weight 1\n        nexthop via 172.16.12.22  dev swp2 weight 1 \n\nThe packet can be sent towards SPINE1/SPINE2. Let's take SPINE1 as the next hop as an example here. Does SPINE1 know how to reach 20.1.1.1? \n\n// RIB lookup on SPINE1 for 20.1.1.1\n\ncumulus@SPINE1:~$ net show route 20.1.1.1\nRIB entry for 20.1.1.1\n======================\n% Network not in table \n</code></pre> <p>SPINE1 has no entry for this prefix and thus your packets get blackholed here. To get around this problem, you either redistribute your BGP table into your IGP (which doesn't make sense considering your BGP table might grow substantially) or you have some sort of a meshed iBGP peering to ensure all boxes receive this route. The cleanest way of doing this would be with route reflectors. So, let's go ahead and make SPINE1/SPINE2 as RRs and have LEAF1/2/3 peer with them as route reflector clients.</p> <p>I have now modified the configuration appropriately:</p> <pre><code>// IPv4 unicast BGP table on LEAF1\n\ncumulus@LEAF1:~$ net show bgp ipv4 unicast summary \nBGP router identifier 1.1.1.1, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 2, using 39 KiB of memory\n\nNeighbor            V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nSPINE1(11.11.11.11) 4          1      22      22        0    0    0 00:00:57            0\nSPINE2(22.22.22.22) 4          1       9       9        0    0    0 00:00:20            0\n\nTotal number of neighbors 2 \n\n// IPv4 unicast BGP table on LEAF2\n\ncumulus@LEAF2:~$ net show bgp ipv4 unicast summary \nBGP router identifier 2.2.2.2, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 2, using 39 KiB of memory\n\nNeighbor            V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nSPINE1(11.11.11.11) 4          1      13      13        0    0    0 00:00:30            0\nSPINE2(22.22.22.22) 4          1      23      23        0    0    0 00:01:01            0\n\nTotal number of neighbors 2 \n</code></pre> <p>Go back in and advertise the host subnets again and now we see the intermediate devices (SPINE1/SPINE2) also having the prefixes in RIB.</p> <pre><code>cumulus@SPINE1:~$ net show route 20.1.1.1\nRIB entry for 20.1.1.1\n======================\nRouting entry for 20.1.1.0/24\n  Known via \"bgp\", distance 200, metric 0, best\n  Last update 00:00:05 ago\n    2.2.2.2 (recursive)\n  *   172.16.21.2, via swp2\n\n\nFIB entry for 20.1.1.1\n======================\n20.1.1.0/24 via 172.16.21.2 dev swp2  proto bgp  metric 20\n</code></pre> <p>PC1 can now reach PC2:</p> <pre><code>PC1&gt; ping 20.1.1.1\n\n84 bytes from 20.1.1.1 icmp_seq=1 ttl=61 time=2.354 ms\n84 bytes from 20.1.1.1 icmp_seq=2 ttl=61 time=1.812 ms\n84 bytes from 20.1.1.1 icmp_seq=3 ttl=61 time=1.590 ms\n84 bytes from 20.1.1.1 icmp_seq=4 ttl=61 time=1.872 ms\n84 bytes from 20.1.1.1 icmp_seq=5 ttl=61 time=0.829 ms\n</code></pre> <p>In the next post, we'll take a look at the ingenious BGP unnumbered design and understand how it truly works.</p>","tags":["cumulus","bgp"]},{"location":"blog/2021/12/10/cumulus-basics-part-v---bgp-unnumbered/","title":"Cumulus Basics Part V - BGP unnumbered","text":"<p>In this post, we'll look at BGP unnumbered on Cumulus Linux.</p>","tags":["cumulus","bgp"]},{"location":"blog/2021/12/10/cumulus-basics-part-v---bgp-unnumbered/#introduction","title":"Introduction","text":"<p>The last post introduced basic BGP bringup on a Cumulus box with OSPF as the IGP. Let's now move towards a BGP unnumbered design and understand how that works.</p>","tags":["cumulus","bgp"]},{"location":"blog/2021/12/10/cumulus-basics-part-v---bgp-unnumbered/#topology","title":"Topology","text":"<p>We will use the same network topology as before:</p> <p></p> <p>The idea behind BGP unnumbered is to use the IPv6 link local addressing on hop by hop basis. When you're building a L3 fabric, what is the goal of the underlay? Outside of any multicast replication that may be required, the main goal (from a unicast perspective) is to provide connectivity from one tunnel end point to another. Typically, you would use something like OSPF or IS-IS to advertise the loopbacks of the tunnel endpoints and thus, provide connectivity from one loopback to another. </p> <p>Now, with that premise in mind, let's break it down some more. What is really done on a per hop basis? Each node is simply doing a L3 lookup, resolving the next hop's address, rewriting the L2 header and forwarding it on towards the next hop. This entire process can be lifted away from an IGP and done via BGP itself, by utilizing  link local IPv6 addressing and RFC 5549, which allows you to advertise an IPv4 NLRI with an IPv6 next hop. And how do you resolve the IPv6 next hop? Using the IPv6 neighbor discovery process.</p> <p>Let's start putting some of these pieces together now. First, we enable IPv6 ND on the point to point links and disable RA suppression (which appears to be enabled by default). </p> <pre><code>cumulus@LEAF1:~$ net add interface swp1-2 ipv6 nd ra-interval 10 \ncumulus@LEAF1:~$ net del interface swp1-2 ipv6 nd suppress-ra \ncumulus@LEAF1:~$ net commit \n</code></pre> <p>This adds the following to '/etc/frr/frr.conf' file:</p> <pre><code>cumulus@LEAF1:~$ sudo cat /etc/frr/frr.conf\n[sudo] password for cumulus: \nSorry, try again.\n[sudo] password for cumulus: \nfrr version 4.0+cl3u10\nfrr defaults datacenter\nhostname LEAF1\nusername cumulus nopassword\n!\nservice integrated-vtysh-config\n!\nlog syslog informational\n!\ninterface swp1\n ipv6 nd ra-interval 10\n no ipv6 nd suppress-ra\n!\ninterface swp2\n ipv6 nd ra-interval 10\n no ipv6 nd suppress-ra\n!\nline vty\n!\n</code></pre> <p>From 'net show interface &lt;&gt;' you can confirm the link local IPv6 address, the mac address associated with this interface, Router Advertisement (abbreviated to 'RA' going forward) interval and so on:</p> <pre><code>cumulus@LEAF1:~$ net show interface swp1\n    Name  MAC                Speed  MTU   Mode\n--  ----  -----------------  -----  ----  -------------\nUP  swp1  50:00:00:03:00:01  1G     1500  NotConfigured\n\ncl-netstat counters\n-------------------\nRX_OK  RX_ERR  RX_DRP  RX_OVR  TX_OK  TX_ERR  TX_DRP  TX_OVR\n-----  ------  ------  ------  -----  ------  ------  ------\n54708       0      18       0  57169       0       0       0\n\nRouting\n-------\n  Interface swp1 is up, line protocol is up\n  Link ups:       1    last: 2019/04/29 03:51:10.27\n  Link downs:     1    last: 2019/04/29 03:01:38.14\n  PTM status: disabled\n  vrf: default\n  index 3 metric 0 mtu 1500 speed 1000\n  flags: &lt;UP,BROADCAST,RUNNING,MULTICAST&gt;\n  Type: Ethernet\n  HWaddr: 50:00:00:03:00:01\n  inet6 fe80::5200:ff:fe03:1/64\n  Interface Type Other\n  ND advertised reachable time is 0 milliseconds\n  ND advertised retransmit interval is 0 milliseconds\n  ND router advertisements sent: 51 rcvd: 2\n  ND router advertisements are sent every 10 seconds\n  ND router advertisements lifetime tracks ra-interval\n  ND router advertisement default router preference is medium\n  Hosts use stateless autoconfig for addresses.\n  Neighbor address(s):\n  inet6 fe80::5200:ff:fe01:1/128\n</code></pre> <p>Let's mimic the configuration on SPINE1 as well now.</p> <pre><code>cumulus@SPINE1:~$ net add interface swp1 ipv6 nd ra-interval 10\ncumulus@SPINE1:~$ net del interface swp1 ipv6 nd suppress-ra \ncumulus@SPINE1:~$ net commit\n</code></pre> <p>Again, look at 'net show interface swp1' to confirm the mac address and the IPv6 link local address:</p> <pre><code>cumulus@SPINE1:~$ net show interface swp1\n    Name  MAC                Speed  MTU   Mode\n--  ----  -----------------  -----  ----  -------------\nUP  swp1  50:00:00:01:00:01  1G     1500  NotConfigured\n\ncl-netstat counters\n-------------------\nRX_OK  RX_ERR  RX_DRP  RX_OVR  TX_OK  TX_ERR  TX_DRP  TX_OVR\n-----  ------  ------  ------  -----  ------  ------  ------\n57216       0      44       0  54847       0       0       0\n\nLLDP Details\n------------\nLocalPort  RemotePort(RemoteHost)\n---------  ----------------------\nswp1       swp1(LEAF1)\n\nRouting\n-------\n  Interface swp1 is up, line protocol is up\n  Link ups:       1    last: 2019/04/29 04:01:52.99\n  Link downs:     1    last: 2019/04/29 03:04:24.45\n  PTM status: disabled\n  vrf: default\n  index 3 metric 0 mtu 1500 speed 1000\n  flags: &lt;UP,BROADCAST,RUNNING,MULTICAST&gt;\n  Type: Ethernet\n  HWaddr: 50:00:00:01:00:01\n  inet6 fe80::5200:ff:fe01:1/64\n  Interface Type Other\n  ND advertised reachable time is 0 milliseconds\n  ND advertised retransmit interval is 0 milliseconds\n  ND router advertisements sent: 81 rcvd: 80\n  ND router advertisements are sent every 10 seconds\n  ND router advertisements lifetime tracks ra-interval\n  ND router advertisement default router preference is medium\n  Hosts use stateless autoconfig for addresses.\n  Neighbor address(s):\n  inet6 fe80::5200:ff:fe03:1/128\n</code></pre> <p>Take a look at the neighbor discovery process between LEAF1 and SPINE1 now. </p> <p>First, SPINE1 sends out a neighbor solicitation (abbreviated to 'NS' going forward) message with a target address of itself. This is sent to a well known multicast address:</p> <p></p> <p>After a back and forth RA, another NS is sent by SPINE1 but this time, with a target address of 'fe80::5200:ff:fe03:1', which corresponds to the link local IPv6 address assigned to swp1 of LEAF1. Notice how the ICMPv6 option also specifies the link-layer address, which corresponds to SPINE1, swp1's mac address. </p> <p></p> <p>LEAF1 responds to this with a Neighbor Advertisement (abbreviated to NA going forward) message.</p> <p></p> <p>Notice that the link-layer address in the NA sent by LEAF1 is the mac address of its port, swp1. SPINE1 can now use this information to build its IP neighbor table. The same process happens the other way around, with LEAF1 sending a NS and SPINE1 responding back with a NA. At the end of this, both should have their IP neighbor tables correctly populated. </p> <p>You can confirm this using:</p> <pre><code>cumulus@LEAF1:~$ ip -6 neighbor show\nfe80::5200:ff:fe01:1 dev swp1 lladdr 50:00:00:01:00:01 router REACHABLE\n\ncumulus@SPINE1:~$ ip -6 neighbor show\nfe80::5200:ff:fe03:1 dev swp1 lladdr 50:00:00:03:00:01 router REACHABLE\n</code></pre> <p>Let's bring up BGP over this link now. The configuration needs to be modified a little bit since these links do not have any IPv4 address anymore (apart from their default link local IPv4 addresses). Instead of specifying an IP address in the BGP neighbor statement, Cumulus allows you to specify the port number.</p> <pre><code>// LEAF1 configuration\n\ncumulus@LEAF1:~$ net add bgp autonomous-system 1\ncumulus@LEAF1:~$ net add bgp router-id 1.1.1.1  \ncumulus@LEAF1:~$ net add bgp neighbor swp1 remote-as internal\n\n// SPINE1 configuration\n\ncumulus@LEAF1:~$ net add bgp autonomous-system 1\ncumulus@LEAF1:~$ net add bgp router-id 11.11.11.11 \ncumulus@LEAF1:~$ net add bgp neighbor swp1 remote-as internal\n</code></pre> <p>A packet capture shows us the bringup sequence for BGP between these two boxes:</p> <p></p> <p>Let's break this down quickly - initially we see several TCP resets. Why is that? Because BGP port is not open yet on SPINE1 (it was not configured at that point in time), hence any TCP SYN coming for a destination port of 179 (BGP) would be rejected by SPINE1. Once the configuration is complete on both sides, we see the 3-way TCP handshake complete and the OPEN messages being sent. </p> <p>Among other capabilities exchanged in the OPEN message, an important one is highlighted in the capture - the extended next hop encoding. This allows for an IPv4 NLRI to have an IPv6 next hop. You need to make sure this capability is exchanged. To force this, you can use this 'net add bgp neighbor  capability extended-nexthop' command on a Cumulus box. <p>Using a similar approach, we can complete our BGP peerings for this entire infrastructure. Remember to make each LEAF switch a route reflector client of the SPINE switches (otherwise an update from a LEAF switch will not be sent to the other LEAF switches by the SPINE because of iBGP peering rules). At the end of this, each SPINE should see three peerings - one to each of the LEAF switches:</p> <pre><code>//SPINE1\n\ncumulus@SPINE1:~$ net show bgp ipv4 unicast summary \nBGP router identifier 11.11.11.11, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 3, using 58 KiB of memory\n\nNeighbor        V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLEAF1(swp1)     4          1     374     376        0    0    0 00:04:19            0\nLEAF2(swp2)     4          1      31      31        0    0    0 00:01:26            0\nLEAF3(swp3)     4          1       6       6        0    0    0 00:00:11            0\n\nTotal number of neighbors 3 \n\n// SPINE2\n\ncumulus@SPINE2:~$ net show bgp ipv4 unicast summary\nBGP router identifier 22.22.22.22, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 3, using 58 KiB of memory\n\nNeighbor        V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLEAF2(swp1)     4          1      34      34        0    0    0 00:01:33            0\nLEAF1(swp2)     4          1     125     127        0    0    0 00:04:52            0\nLEAF3(swp3)     4          1       9       9        0    0    0 00:00:18            0\n\nTotal number of neighbors 3 \n</code></pre> <p>Each SPINE has three BGP neighbors, as expected. We have not advertised the host subnets yet so let's do that and take a packet capture to analyze how this is advertised.</p> <pre><code>cumulus@LEAF1:~$ net add bgp network 10.1.1.0/24\ncumulus@LEAF1:~$ net commit\n</code></pre> <p>Take a look at the following capture taken on LEAF2 as it receives a BGP update from SPINE1:</p> <p></p> <p>The NLRI describes an IPv4 subnet but the next hop is an IPv6 address. How cool is that? Look at the RIB/FIB on LEAF2 to confirm how this is installed:</p> <pre><code>cumulus@LEAF2:~$ net show route 10.1.1.1\nRIB entry for 10.1.1.1\n======================\nRouting entry for 10.1.1.0/24\n  Known via \"bgp\", distance 200, metric 0, best\n  Last update 00:12:21 ago\n  * fe80::5200:ff:fe01:2, via swp2\n  * fe80::5200:ff:fe02:1, via swp1\n\n\nFIB entry for 10.1.1.1\n======================\n10.1.1.0/24  proto bgp  metric 20 \n        nexthop via 169.254.0.1  dev swp2 weight 1 onlink\n        nexthop via 169.254.0.1  dev swp1 weight 1 onlink \n</code></pre> <p>The RIB installs the prefix against the link local IPv6 address while the FIB installs them against the link local IPv4 address. </p> <p>After advertising all host subnets, LEAF1s RIB looks like this:</p> <pre><code>cumulus@LEAF1:~$ net show route ipv4\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR,\n       &gt; - selected route, * - FIB route\n\nC&gt;* 1.1.1.1/32 is directly connected, lo, 16:51:39\nC&gt;* 10.1.1.0/24 is directly connected, swp3, 16:28:27\nB&gt;* 20.1.1.0/24 [200/0] via fe80::5200:ff:fe01:1, swp1, 00:00:24\n  *                     via fe80::5200:ff:fe02:2, swp2, 00:00:24\nB&gt;* 30.1.1.0/24 [200/0] via fe80::5200:ff:fe01:1, swp1, 00:00:14\n  *                     via fe80::5200:ff:fe02:2, swp2, 00:00:14 \n</code></pre> <p>PC1 should be able to ping PC2 and PC3 now:</p> <pre><code>PC1&gt; ping 20.1.1.1\n\n84 bytes from 20.1.1.1 icmp_seq=1 ttl=61 time=2.743 ms\n84 bytes from 20.1.1.1 icmp_seq=2 ttl=61 time=1.295 ms\n84 bytes from 20.1.1.1 icmp_seq=3 ttl=61 time=1.648 ms\n84 bytes from 20.1.1.1 icmp_seq=4 ttl=61 time=1.637 ms\n84 bytes from 20.1.1.1 icmp_seq=5 ttl=61 time=1.543 ms\n\nPC1&gt; ping 30.1.1.1\n\n84 bytes from 30.1.1.1 icmp_seq=1 ttl=61 time=2.585 ms\n84 bytes from 30.1.1.1 icmp_seq=2 ttl=61 time=1.402 ms\n84 bytes from 30.1.1.1 icmp_seq=3 ttl=61 time=1.528 ms\n84 bytes from 30.1.1.1 icmp_seq=4 ttl=61 time=1.975 ms\n84 bytes from 30.1.1.1 icmp_seq=5 ttl=61 time=1.638 ms\n</code></pre> <p>And there it is. A thing of beauty!</p>","tags":["cumulus","bgp"]},{"location":"blog/2021/12/11/cumulus-basics-part-vi---vxlan-l2vnis-with-bgp-evpn/","title":"Cumulus Basics Part VI - VXLAN L2VNIs with BGP EVPN","text":"<p>In this post, we introduce BGP EVPN and a VXLAN fabric in Cumulus Linux, with L2VNIs.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/11/cumulus-basics-part-vi---vxlan-l2vnis-with-bgp-evpn/#introduction","title":"Introduction","text":"<p>Now that we've covered the basics of BGP unnumbered in the last post, we'll start building a VXLAN based fabric with BGP EVPN. </p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/11/cumulus-basics-part-vi---vxlan-l2vnis-with-bgp-evpn/#topology","title":"Topology","text":"<p>Our topology remains the same, with a minor change - PC2 has now been moved to the same subnet as PC1, 10.1.1.0/24:</p> <p></p> <p>The PCs are lightweight emulations here. Take note of the mac addresses assigned to them as they will be important and constantly referenced through the post:</p> <pre><code>PC1&gt; show ip\n\nNAME        : PC1[1]\nIP/MASK     : 10.1.1.1/24\nGATEWAY     : 10.1.1.254\nDNS         : \nMAC         : 00:50:79:66:68:06\nLPORT       : 20000\nRHOST:PORT  : 127.0.0.1:30000\nMTU         : 1500\n\nPC2&gt; show ip\n\nNAME        : PC2[1]\nIP/MASK     : 10.1.1.2/24\nGATEWAY     : 10.1.1.254\nDNS         : \nMAC         : 00:50:79:66:68:07\nLPORT       : 20000\nRHOST:PORT  : 127.0.0.1:30000\nMTU         : 1500\n</code></pre> <p>Start by activating the neighbors against the L2VPN EVPN AFI/SAFI. Outside of this, I am re-doing some of the configurations from the previous post for completeness sake. We are adding a loopback per switch, adding BGP neighbors and marking each LEAF switch as a route reflector client on the SPINEs</p> <pre><code>// SPINE1\n\ncumulus@SPINE1:~$ net add loopback lo ip address 11.11.11.11/32\ncumulus@SPINE1:~$ net add bgp autonomous-system 1\ncumulus@SPINE1:~$ net add bgp neighbor swp1-3 remote-as internal\ncumulus@SPINE1:~$ net add bgp neighbor swp1-3 route-reflector-client\ncumulus@SPINE1:~$ net add bgp l2vpn evpn neighbor swp1-3 activate\ncumulus@SPINE1:~$ net add bgp l2vpn evpn neighbor swp1-3 route-reflector-client\n\n// SPINE2 \n\ncumulus@SPINE2:~$ net add loopback lo ip address 22.22.22.22/32\ncumulus@SPINE2:~$ net add bgp autonomous-system 1\ncumulus@SPINE2:~$ net add bgp neighbor swp1-3 remote-as internal\ncumulus@SPINE2:~$ net add bgp neighbor swp1-3 route-reflector-client\ncumulus@SPINE2:~$ net add bgp l2vpn evpn neighbor swp1-3 activate\ncumulus@SPINE2:~$ net add bgp l2vpn evpn neighbor swp1-3 route-reflector-client\n\n// LEAF1\n\ncumulus@LEAF1:~$ net add loopback lo ip add 1.1.1.1/32\ncumulus@LEAF1:~$ net add bgp autonomous-system 1\ncumulus@LEAF1:~$ net add bgp neighbor swp1-2 remote-as internal \ncumulus@LEAF1:~$ net add bgp l2vpn evpn neighbor swp1-2 activate \ncumulus@LEAF1:~$ net commit \n</code></pre> <p>The LEAF1 configuration must be appropriately replicated on the other two LEAF switches (modify interfaces as needed and add unique loopback IPs).</p> <p>Post this, confirm that BGP peerings for L2VPN EVPN is up. Each SPINE should have three peerings, one to each LEAF switch:</p> <pre><code>// SPINE1 peerings:\n\ncumulus@SPINE1:~$ net show bgp l2vpn evpn summary \nBGP router identifier 11.11.11.11, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 3, using 58 KiB of memory\n\nNeighbor        V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLEAF1(swp1)     4          1      63      64        0    0    0 00:02:57            0\nLEAF2(swp2)     4          1      42      43        0    0    0 00:01:56            0\nLEAF3(swp3)     4          1      29      30        0    0    0 00:01:16            0\n\nTotal number of neighbors 3 \n\n// SPINE2 peerings:\n\ncumulus@SPINE2:~$ net show bgp l2vpn evpn summary \nBGP router identifier 22.22.22.22, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 3, using 58 KiB of memory\n\nNeighbor        V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLEAF2(swp1)     4          1      47      48        0    0    0 00:02:10            0\nLEAF1(swp2)     4          1      70      71        0    0    0 00:03:19            0\nLEAF3(swp3)     4          1      36      36        0    0    0 00:01:38            0\n\nTotal number of neighbors 3 \n</code></pre> <p>Remember that the underlay provides reachability from one tunnel endpoint to another. Since we are using BGP unnumbered as the underlay itself, we need to advertise the loopbacks of each LEAF switch into the IPv4 address family.</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net add bgp network 1.1.1.1/32\ncumulus@LEAF1:~$ net commit\n\n// LEAF2\n\ncumulus@LEAF2:~$ net add bgp network 2.2.2.2/32\ncumulus@LEAF2:~$ net commit\n\n// LEAF3\n\ncumulus@LEAF3:~$ net add bgp network 3.3.3.3/32\ncumulus@LEAF3:~$ net commit\n</code></pre> <p>Each LEAF switch should now be aware of the loopbacks of the other to LEAF switches.</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net show bgp ipv4 unicast \nBGP table version is 7, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, = multipath,\n              i internal, r RIB-failure, S Stale, R Removed\nOrigin codes: i - IGP, e - EGP, ? - incomplete\n\n   Network          Next Hop            Metric LocPrf Weight Path\n*&gt; 1.1.1.1/32       0.0.0.0                  0         32768 i\n*=i2.2.2.2/32       swp2                     0    100      0 i\n*&gt;i                 swp1                     0    100      0 i\n*=i3.3.3.3/32       swp2                     0    100      0 i\n*&gt;i                 swp1                     0    100      0 i\n\nDisplayed  3 routes and 5 total paths \n\n// LEAF2\n\ncumulus@LEAF2:~$ net show bgp ipv4 unicast \nBGP table version is 6, local router ID is 2.2.2.2\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, = multipath,\n              i internal, r RIB-failure, S Stale, R Removed\nOrigin codes: i - IGP, e - EGP, ? - incomplete\n\n   Network          Next Hop            Metric LocPrf Weight Path\n*&gt;i1.1.1.1/32       swp2                     0    100      0 i\n*=i                 swp1                     0    100      0 i\n*&gt; 2.2.2.2/32       0.0.0.0                  0         32768 i\n*&gt;i3.3.3.3/32       swp2                     0    100      0 i\n*=i                 swp1                     0    100      0 i\n\nDisplayed  3 routes and 5 total paths \n\n// LEAF3\n\ncumulus@LEAF3:~$ net show bgp ipv4 unicast \nBGP table version is 6, local router ID is 3.3.3.3\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, = multipath,\n              i internal, r RIB-failure, S Stale, R Removed\nOrigin codes: i - IGP, e - EGP, ? - incomplete\n\n   Network          Next Hop            Metric LocPrf Weight Path\n*&gt;i1.1.1.1/32       swp3                     0    100      0 i\n*=i                 swp1                     0    100      0 i\n*&gt;i2.2.2.2/32       swp3                     0    100      0 i\n*=i                 swp1                     0    100      0 i\n*&gt; 3.3.3.3/32       0.0.0.0                  0         32768 i\n\nDisplayed  3 routes and 5 total paths \n</code></pre> <p>Confirm reachability from loopback to loopback. Running this test from one of the LEAF switches should be enough:</p> <pre><code>// LEAF1 to LEAF2 reachability\n\ncumulus@LEAF1:~$ ping 2.2.2.2 -I 1.1.1.1\nPING 2.2.2.2 (2.2.2.2) from 1.1.1.1 : 56(84) bytes of data.\n64 bytes from 2.2.2.2: icmp_seq=1 ttl=63 time=1.94 ms\n64 bytes from 2.2.2.2: icmp_seq=2 ttl=63 time=2.05 ms\n64 bytes from 2.2.2.2: icmp_seq=3 ttl=63 time=2.73 ms\n64 bytes from 2.2.2.2: icmp_seq=4 ttl=63 time=1.91 ms\n64 bytes from 2.2.2.2: icmp_seq=5 ttl=63 time=1.01 ms\n^C\n--- 2.2.2.2 ping statistics ---\n5 packets transmitted, 5 received, 0% packet loss, time 4008ms\nrtt min/avg/max/mdev = 1.012/1.931/2.730/0.550 ms\n\n// LEAF1 to LEAF3 reachability\n\ncumulus@LEAF1:~$ ping 3.3.3.3 -I 1.1.1.1\nPING 3.3.3.3 (3.3.3.3) from 1.1.1.1 : 56(84) bytes of data.\n64 bytes from 3.3.3.3: icmp_seq=1 ttl=63 time=18.2 ms\n64 bytes from 3.3.3.3: icmp_seq=2 ttl=63 time=1.78 ms\n64 bytes from 3.3.3.3: icmp_seq=3 ttl=63 time=2.41 ms\n64 bytes from 3.3.3.3: icmp_seq=4 ttl=63 time=1.85 ms\n64 bytes from 3.3.3.3: icmp_seq=5 ttl=63 time=2.28 ms\n^C\n--- 3.3.3.3 ping statistics ---\n5 packets transmitted, 5 received, 0% packet loss, time 4008ms\nrtt min/avg/max/mdev = 1.788/5.314/18.230/6.462 ms\n</code></pre> <p>Now, we start with a simple premise - PC1 cannot ping PC2:</p> <pre><code>PC1&gt; ping 10.1.1.2\n\nhost (10.1.1.2) not reachable\n</code></pre> <p>Our goal is to allow PC1 to talk to PC2 via a VXLAN fabric. The idea is straightforward - you map your VLAN to a VNID (VXLAN network ID) and associate this to a virtual interface that acts as your entry/exit point for VXLAN packets. </p> <p>The configuration on a Cumulus box for this is like so (scroll right to understand what each configuration is doing):</p> <pre><code>cumulus@LEAF1:~$ net add vxlan vni10 vxlan id 10010             // adds VNID to virtual interface, vni10\ncumulus@LEAF1:~$ net add vxlan vni10 bridge access 10           // adds VLAN 10 to vni10\ncumulus@LEAF1:~$ net add vxlan vni10 bridge learning off        // disables mac learning on vni10\ncumulus@LEAF1:~$ net add vxlan vni10 vxlan local-tunnelip 1.1.1.1   // specifies local source IP to be 1.1.1.1 for VXLAN packets\n</code></pre> <p>It is also important to understand what changes the above commands translate to, so take a look at that when you commit them:</p> <pre><code>cumulus@LEAF1:~$ net commit \n--- /etc/network/interfaces     2019-05-13 05:23:36.667000000 +0000\n+++ /run/nclu/ifupdown2/interfaces.tmp  2019-05-13 05:29:13.613000000 +0000\n@@ -18,20 +18,29 @@\n\n auto swp2\n iface swp2\n\n auto swp3\n iface swp3\n     bridge-access 10\n\n auto bridge\n iface bridge\n-    bridge-ports swp3\n+    bridge-ports swp3 vni10\n     bridge-vids 10\n     bridge-vlan-aware yes\n\n auto vlan10\n iface vlan10\n     address 10.1.1.254/24\n     vlan-id 10\n     vlan-raw-device bridge\n\n+auto vni10\n+iface vni10\n+    bridge-access 10\n+    bridge-learning off\n+    mstpctl-bpduguard yes\n+    mstpctl-portbpdufilter yes\n+    vxlan-id 10010\n+    vxlan-local-tunnelip 1.1.1.1\n+\n\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  ------------------------------------------------\ncumulus  2019-05-13 05:27:26.613573  net add vxlan vni10 vxlan id 10010\ncumulus  2019-05-13 05:28:16.336833  net add vxlan vni10 bridge access 10\ncumulus  2019-05-13 05:28:27.454434  net add vxlan vni10 bridge learning off\ncumulus  2019-05-13 05:29:08.227404  net add vxlan vni10 vxlan local-tunnelip 1.1.1.1\n</code></pre> <p>The changes we made creates a new virtual interface with the name 'vni10' (we gave this name in the first command), associates VLAN 10 and a VXLAN network identifier (VNID) of 10010 to it, disables mac learning on this, associates a source IP of 1.1.1.1 to this and adds this interface to the VLAN-aware bridge. </p> <p>Mimic this configuration on LEAF2 - the only change you need to make there is the local tunnel-ip that is used (configure that as LEAF2s loopback, 2.2.2.2).</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/11/cumulus-basics-part-vi---vxlan-l2vnis-with-bgp-evpn/#control-plane-flow","title":"Control-plane flow","text":"<p>Let's try and understand the flow of control-plane learning now. As a baseline, we have not learnt PC1/PC2 macs nor are there any type-2 routes in the BGP table:</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net show bridge macs vlan 10\n\nVLAN  Master  Interface  MAC                TunnelDest  State      Flags  LastSeen\n----  ------  ---------  -----------------  ----------  ---------  -----  --------\n  10  bridge  bridge     50:00:00:04:00:03              permanent         02:40:27\n\ncumulus@LEAF1:~$ net show bgp l2vpn evpn route type macip \nNo EVPN prefixes (of requested type) exist \n\n// LEAF2\n\ncumulus@LEAF2:~$ net show bridge macs vlan 10\n\nVLAN  Master  Interface  MAC                TunnelDest  State      Flags  LastSeen\n----  ------  ---------  -----------------  ----------  ---------  -----  --------\n  10  bridge  bridge     50:00:00:05:00:03              permanent         02:40:41\n\ncumulus@LEAF2:~$ net show bgp l2vpn evpn route type macip\nNo EVPN prefixes (of requested type) exist \n</code></pre> <p>I am going to restart PC1 now. This forces a GARP to be sent out when it comes up and this GARP causes PC1s mac to be learnt on swp3 of LEAF1:</p> <pre><code>cumulus@LEAF1:~$ net show bridge macs vlan 10\n\nVLAN  Master  Interface  MAC                TunnelDest  State      Flags  LastSeen\n----  ------  ---------  -----------------  ----------  ---------  -----  --------\n  10  bridge  bridge     50:00:00:04:00:03              permanent         02:49:50\n  10  bridge  swp3       00:50:79:66:68:06                                00:00:17\n</code></pre> <p>Once the mac is learnt, it gets pushed into the BGP table as well:</p> <pre><code>cumulus@LEAF1:~$ net show bgp l2vpn evpn route type macip \nBGP table version is 13, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:2\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                            32768 i\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                            32768 i\n\nDisplayed 2 prefixes (2 paths) (of requested type) \n</code></pre> <p>This will now be sent as an update to the SPINEs and the SPINEs reflect it to LEAF2. A packet capture on LEAF2 shows this BGP UPDATE message:</p> <p></p> <p>There is a lot of interesting information in this update. The NLRI is an EVPN NLRI, describing a mac advertisement route (type-2). The RD is 1.1.1.1:2 and you can see the mac address inside this NLRI with no IP address included. Also, look at the final attribute which is for extended communities - this includes the RT (which is a combination of the AS and the VNID itself). The extended community also describes the encapsulation which is of type VXLAN. </p> <p>Immediately after this update, another update is sent with the IP address included this time:</p> <p></p> <p>LEAF2 gets this update and installs it in its BGP table as well against the RD that was in the update (1.1.1.1:2):</p> <pre><code>cumulus@LEAF2:~$ net show bgp l2vpn evpn route type macip\nBGP table version is 16, local router ID is 2.2.2.2\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:2\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                  0    100      0 i\n\nDisplayed 2 prefixes (2 paths) (of requested type) \n</code></pre> <p>From here, the type-2 route gets pushed into the mac address table and associated to vni10, with the 'offload' flag set and the VXLAN destination IP set to 1.1.1.1:</p> <pre><code>cumulus@LEAF2:~$ net show bridge macs | grep 00:50:79:66:68:06\nVLAN      Master  Interface  MAC                TunnelDest  State      Flags          LastSeen\n--------  ------  ---------  -----------------  ----------  ---------  -------------  --------\n10        bridge  vni10      00:50:79:66:68:06                         offload        00:16:43\nuntagged          vni10      00:50:79:66:68:06  1.1.1.1                self, offload  00:16:43\n</code></pre> <p>You can see that with the BGP EVPN control-plane, the data-plane is already built and ready without any conversation between the hosts.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/11/cumulus-basics-part-vi---vxlan-l2vnis-with-bgp-evpn/#data-plane-flow","title":"Data-plane flow","text":"<p>From a data-plane perspective, when PC1 pings PC2, it tries to resolve the IP address first. This is achieved via an ARP request that goes through the fabric. This is typically done in two ways - either with headend replication on the ingress tunnel endpoint (meaning the ARP is packaged into a unicast packet sent over the VXLAN fabric) or via multicast (the underlay needs to be multicast aware). We are not going to go into BUM (broadcast/unknown unicast/multicast) traffic handling right now since that needs to be a separate post in itself.</p> <p>Assuming that ARP is resolved, PC1 generates a unicast ICMP request message that hits LEAF1. LEAF1 does a mac table lookup and finds that this needs to be sent over vni10, with a VXLAN encapsulation, a VNID of 10010 and a destination IP of 2.2.2.2:</p> <pre><code>cumulus@LEAF1:~$ net show bridge macs 00:50:79:66:68:07\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags          LastSeen\n--------  ------  ---------  -----------------  ----------  -----  -------------  --------\n10        bridge  vni10      00:50:79:66:68:07                     offload        00:59:05\nuntagged          vni10      00:50:79:66:68:07  2.2.2.2            self, offload  00:59:05\n</code></pre> <p>On the wire, the packets look like this:</p> <p></p> <p>It is an ICMP header inside an IP header inside a VXLAN header inside a UDP header inside an IP header. The logic is that the top most IP header carries the packet from one VXLAN tunnel endpoint to the other. At the other end, the top most IP header is stripped off (since it owns the destination IP address), revealing the UDP and the VXLAN headers which are subsequently stripped off as well. Another mac lookup is done on the packet and a forwarding decision is made based on that:</p> <pre><code>cumulus@LEAF2:~$ net show bridge macs 00:50:79:66:68:07\n\nVLAN  Master  Interface  MAC                TunnelDest  State  Flags  LastSeen\n----  ------  ---------  -----------------  ----------  -----  -----  --------\n  10  bridge  swp3       00:50:79:66:68:07                            00:00:03\n</code></pre> <p>In this case, the mac table says that the packet needs to be forwarded out of swp3. This gets the packet to PC2 which responds back with an ICMP reply. The entire data-plane process happens again, in the reverse direction with LEAF2 now being the source of the VXLAN encapsulation:</p> <p></p> <p>Notice how the top most IP header now has a source IP address of 2.2.2.2. Effectively, you can visualize this as a bi-directional tunnel carrying traffic over a routed infrastructure between two hosts in the same subnet.</p> <p></p> <p>I hope this was informative. I'll see y'all in the next post!</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/12/cumulus-basics-part-vii---vxlan-routing---asymmetric-irb/","title":"Cumulus Basics Part VII - VXLAN routing - asymmetric IRB","text":"<p>In this post, we look at VXLAN routing with asymmetric IRB on Cumulus Linux.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/12/cumulus-basics-part-vii---vxlan-routing---asymmetric-irb/#topology","title":"Topology","text":"<p>We continue on with the same topology as the last post:</p> <p></p> <p>Our goal is to have PC1 talk to PC3 over the VXLAN fabric that we have built out. VXLAN routing can broadly be done using two methods - asymmetric and symmetric IRB. In this post, we will discuss the asymmetric methodology.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/12/cumulus-basics-part-vii---vxlan-routing---asymmetric-irb/#understanding-asymmetric-irb","title":"Understanding Asymmetric IRB","text":"<p>The logic behind asymmetric IRB is that the routing between VXLANs/VNIs is done at each LEAF. This is analogous to inter-VLAN routing where the routing from one VLAN to another happens on the first routed hop itself. To facilitate said inter-VLAN routing, you are required to have the L3 interfaces for both VLANs defined on the first hop (LEAF1 here, for example), so that the box is capable of taking in the packet from one VLAN, routing it into the other VLAN where it can then ARP for the end host directly.</p> <p>Following the same logic, for inter-VXLAN routing, you are required to have the L3 interface for both VNIs on the first routed hop. Let's break this down into chunks and start our configuration. </p> <p>First, we need to have L3 interfaces for the VLANs themselves that will act as gateways for the end hosts:</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net add vlan 10 ip address 10.1.1.254/24\ncumulus@LEAF1:~$ net add vlan 30 ip address 30.1.1.254/24\n\n// LEAF3\n\ncumulus@LEAF3:~$ net add vlan 10 ip address 10.1.1.254/24\ncumulus@LEAF3:~$ net add vlan 30 ip address 30.1.1.254/24\n</code></pre> <p>Notice how the same IP address is defined at both LEAF switches for the same VLAN. Typically, what you should do here is define another virtual address with a virtual mac address common to all LEAF switches sharing this VLAN (VRR, in Cumulus terms). However, for simplicity sake, we are not doing that here.  </p> <p>Next, the routed interfaces for the corresponding VXLANs (or rather VNIs) are also created on both LEAF1 and LEAF3:</p> <pre><code>// LEAF1\n\n    // for VNI 10010, mapped to VLAN 10\n\n    cumulus@LEAF1:~$ net add vxlan vni10 vxlan id 10010\n    cumulus@LEAF1:~$ net add vxlan vni10 bridge access 10\n    cumulus@LEAF1:~$ net add vxlan vni10 vxlan local-tunnelip 1.1.1.1\n\n    // for VNI 10030, mapped to VLAN 30\n\n    cumulus@LEAF1:~$ net add vxlan vni30 vxlan id 10030\n    cumulus@LEAF1:~$ net add vxlan vni30 bridge access 30\n    cumulus@LEAF1:~$ net add vxlan vni30 vxlan local-tunnelip 1.1.1.1\n\n// LEAF3\n\n    // for VNI 10010 mapped to VLAN 10\n\n    cumulus@LEAF3:~$ net add vxlan vni10 vxlan id 10010\n    cumulus@LEAF3:~$ net add vxlan vni10 bridge access 10\n    cumulus@LEAF3:~$ net add vxlan vni10 vxlan local-tunnelip 3.3.3.3\n\n    // for VNI 10030 mapped to VLAN 30\n\n    cumulus@LEAF3:~$ net add vxlan vni30 vxlan id 10030\n    cumulus@LEAF3:~$ net add vxlan vni30 bridge access 30\n    cumulus@LEAF3:~$ net add vxlan vni30 vxlan local-tunnelip 3.3.3.3\n</code></pre> <p>You can confirm the RD/RT values that are auto-generated for these VNIs using:</p> <pre><code>cumulus@LEAF1:~$ net show bgp evpn vni \nAdvertise Gateway Macip: Disabled\nAdvertise SVI Macip: Disabled\nAdvertise All VNI flag: Enabled\nBUM flooding: Head-end replication\nNumber of L2 VNIs: 2\nNumber of L3 VNIs: 0\nFlags: * - Kernel\n  VNI        Type RD                    Import RT                 Export RT                 Tenant VRF                           \n* 10030      L2   1.1.1.1:3             1:10030                   1:10030                  default                              \n* 10010      L2   1.1.1.1:2             1:10010                   1:10010                  default\n</code></pre> <p>Remember, the default gateway of the hosts is the corresponding SVI IP address of the LEAF switch:</p> <pre><code>PC1&gt; show ip\n\nNAME        : PC1[1]\nIP/MASK     : 10.1.1.1/24\nGATEWAY     : 10.1.1.254\nDNS         : \nMAC         : 00:50:79:66:68:06\nLPORT       : 20000\nRHOST:PORT  : 127.0.0.1:30000\nMTU         : 1500\n\nPC3&gt; show ip\n\nNAME        : PC3[1]\nIP/MASK     : 30.1.1.1/24\nGATEWAY     : 30.1.1.254\nDNS         : \nMAC         : 00:50:79:66:68:08\nLPORT       : 20000\nRHOST:PORT  : 127.0.0.1:30000\nMTU         : 1500\n</code></pre> <p>So, how does this all work?</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/12/cumulus-basics-part-vii---vxlan-routing---asymmetric-irb/#the-control-plane","title":"The control-plane","text":"<p>PC1 initiates a ping. It first does an 'and' operation to determine if the destination is in the same subnet or not. In this case, it finds out that it is not, so it forwards the packet to its default gateway. In order to do this, it must first ARP for the default gateway, resolve that and then build the ICMP request packet. </p> <p>This ARP request causes PC1s mac address to be learnt on swp3 of LEAF1. 'Zebra' notifies this to BGP; BGP adds this into its EVPN table and sends an update regarding this to its peers. This BGP update is carried to LEAF3, where BGP populates its EVPN table and then informs 'Zebra', which pushes this into the L2 table. This entire control-plane learning process can be visualized like so:</p> <p></p> <p>Debugs on the box can confirm this process. This is the first time we are introducing debugging on Cumulus boxes, so it's good to make a note of this. Debugs are typically redirected to the '/var/log/frr/frr.log' file but you need to set your logging level correctly first:</p> <pre><code>cumulus@LEAF3:~$ sudo vtysh\n\nHello, this is FRRouting (version 4.0+cl3u10).\nCopyright 1996-2005 Kunihiro Ishiguro, et al.\n\nLEAF3# conf t\nLEAF3(config)# log syslog debug\nLEAF3(config)# end\n</code></pre> <p>'sudo vtysh' allows you to enter a Cisco IOS type prompt. From here, you set the syslog level to 'debug'. This allows for debug logs to be logged in the /var/log/frr/frr.log file. Enable the relevant debugs now:</p> <pre><code>LEAF3# debug bgp updates \nLEAF3# debug bgp zebra\nLEAF3# debug zebra vxlan\nLEAF3# debug zebra rib\nLEAF3# debug zebra events \n</code></pre> <p>From the log file, I have taken relevant snippets of the debugs. On LEAF1, when PC1s mac address is first learnt:</p> <pre><code>// zebra processes mac and sends update to bgpd\n\n2019-05-19T13:03:40.248233+00:00 LEAF1 zebra[865]: UPD MAC 00:50:79:66:68:06 intf swp3(5) VID 10 -&gt; VNI 10010 curFlags 0x4\n2019-05-19T13:03:40.249155+00:00 LEAF1 zebra[865]: Send MACIP Add flags 0x0 MAC 00:50:79:66:68:06 IP  seq 0 L2-VNI 10010 to bgp\n2019-05-19T13:03:40.250227+00:00 LEAF1 zebra[865]: Processing neighbors on local MAC 00:50:79:66:68:06 ADD, VNI 10010\n2019-05-19T13:03:40.250601+00:00 LEAF1 zebra[865]: Send MACIP Add flags 0x0 MAC 00:50:79:66:68:06 IP 10.1.1.1 seq 0 L2-VNI 10010 to bgp\n\n// bgpd receives this update and adds to its table\n\n2019-05-19T13:03:40.250845+00:00 LEAF1 bgpd[874]: 0:Recv MACIP Add flags 0x0 MAC 00:50:79:66:68:06 IP  VNI 10010 seq 0 state 0\n2019-05-19T13:03:40.251222+00:00 LEAF1 bgpd[874]: 0:Recv MACIP Add flags 0x0 MAC 00:50:79:66:68:06 IP 10.1.1.1 VNI 10010 seq 0 state 0\n2019-05-19T13:03:40.301259+00:00 LEAF1 bgpd[874]: group_announce_route_walkcb: afi=l2vpn, safi=evpn, p=[2]:[00:50:79:66:68:06]/224\n2019-05-19T13:03:40.301930+00:00 LEAF1 bgpd[874]: subgroup_process_announce_selected: p=[2]:[00:50:79:66:68:06]/224, selected=0xaf8cac41e0\n2019-05-19T13:03:40.302449+00:00 LEAF1 bgpd[874]: group_announce_route_walkcb: afi=l2vpn, safi=evpn, p=[2]:[00:50:79:66:68:06]:[10.1.1.1]/224\n2019-05-19T13:03:40.302762+00:00 LEAF1 bgpd[874]: subgroup_process_announce_selected: p=[2]:[00:50:79:66:68:06]:[10.1.1.1]/224, selected=0xaf8cac3860\n\n// bgp sends an update to its peers\n\n2019-05-19T13:03:40.303018+00:00 LEAF1 bgpd[874]: u2:s2 send UPDATE w/ attr: nexthop 1.1.1.1, localpref 100, extcommunity ET:8 RT:1:10010, path\n2019-05-19T13:03:40.303266+00:00 LEAF1 bgpd[874]: u2:s2 send MP_REACH for afi/safi 25/70\n2019-05-19T13:03:40.303782+00:00 LEAF1 bgpd[874]: u2:s2 send UPDATE RD 1.1.1.1:2 [2]:[00:50:79:66:68:06]/224 label 10010 l2vpn evpn\n2019-05-19T13:03:40.304096+00:00 LEAF1 bgpd[874]: u2:s2 send UPDATE RD 1.1.1.1:2 [2]:[00:50:79:66:68:06]:[10.1.1.1]/224 label 10010 l2vpn evpn\n2019-05-19T13:03:40.304332+00:00 LEAF1 bgpd[874]: u2:s2 send UPDATE len 144 numpfx 2\n2019-05-19T13:03:40.304744+00:00 LEAF1 bgpd[874]: u2:s2 swp1 send UPDATE w/ nexthop 1.1.1.1\n2019-05-19T13:03:40.304961+00:00 LEAF1 bgpd[874]: u2:s2 swp2 send UPDATE w/ nexthop 1.1.1.1\n</code></pre> <p>Similar debugs from LEAF3, run in parallel:</p> <pre><code>// bgp on LEAF3 receives update from SPINE1\n\n2019-05-19T13:03:40.017491+00:00 LEAF3 bgpd[887]: swp3 rcvd UPDATE w/ attr: nexthop 1.1.1.1, localpref 100, metric 0, extcommunity RT:1:10010 ET:8, originator 1.1.1.1, clusterlist 11.11.11.11, path\n2019-05-19T13:03:40.017949+00:00 LEAF3 bgpd[887]: swp3 rcvd UPDATE wlen 0 attrlen 142 alen 0\n2019-05-19T13:03:40.018244+00:00 LEAF3 bgpd[887]: swp3 rcvd RD 1.1.1.1:2 [2]:[00:50:79:66:68:06]/224 label 10010 l2vpn evpn\n2019-05-19T13:03:40.018402+00:00 LEAF3 bgpd[887]: Tx ADD MACIP, VNI 10010 MAC 00:50:79:66:68:06 IP  flags 0x0 seq 0 remote VTEP 1.1.1.1\n2019-05-19T13:03:40.018523+00:00 LEAF3 bgpd[887]: swp3 rcvd RD 1.1.1.1:2 [2]:[00:50:79:66:68:06]:[10.1.1.1]/224 label 10010 l2vpn evpn\n2019-05-19T13:03:40.018650+00:00 LEAF3 bgpd[887]: Tx ADD MACIP, VNI 10010 MAC 00:50:79:66:68:06 IP 10.1.1.1 flags 0x0 seq 0 remote VTEP 1.1.1.1\n\n// bgpd informs zebra, which installs the mac address\n\n2019-05-19T13:03:40.021415+00:00 LEAF3 zebra[880]: zebra message comes from socket [16]\n2019-05-19T13:03:40.021681+00:00 LEAF3 zebra[880]: Recv MACIP ADD VNI 10010 MAC 00:50:79:66:68:06 flags 0x0 seq 0 VTEP 1.1.1.1 from bgp\n2019-05-19T13:03:40.021893+00:00 LEAF3 zebra[880]: Processing neighbors on remote MAC 00:50:79:66:68:06 ADD, VNI 10010\n2019-05-19T13:03:40.022072+00:00 LEAF3 zebra[880]: zebra message comes from socket [16]\n2019-05-19T13:03:40.022363+00:00 LEAF3 zebra[880]: Recv MACIP ADD VNI 10010 MAC 00:50:79:66:68:06 IP 10.1.1.1 flags 0x0 seq 0 VTEP 1.1.1.1 from bgp\n</code></pre> <p>You can now look at the BGP EVPN table to confirm what was installed:</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net show bgp evpn route\nBGP table version is 1, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:2\n*&gt; [3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                            32768 i\nRoute Distinguisher: 1.1.1.1:3\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                            32768 i\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                            32768 i\n*&gt; [3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                            32768 i\nRoute Distinguisher: 2.2.2.2:2\n* i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\nRoute Distinguisher: 3.3.3.3:2\n* i[2]:[0]:[0]:[48]:[00:50:79:66:68:08]\n                    3.3.3.3                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:08]\n                    3.3.3.3                  0    100      0 i\n* i[2]:[0]:[0]:[48]:[00:50:79:66:68:08]:[32]:[30.1.1.1]\n                    3.3.3.3                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:08]:[32]:[30.1.1.1]\n                    3.3.3.3                  0    100      0 i\n* i[3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                  0    100      0 i\nRoute Distinguisher: 3.3.3.3:3\n* i[3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                  0    100      0 i\n\nDisplayed 9 prefixes (14 paths) \n\n// LEAF3\n\ncumulus@LEAF3:~$ net show bgp evpn route\nBGP table version is 1, local router ID is 3.3.3.3\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:2\n* i[3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                  0    100      0 i\nRoute Distinguisher: 1.1.1.1:3\n* i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                  0    100      0 i\n* i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                  0    100      0 i\n* i[3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                  0    100      0 i\nRoute Distinguisher: 2.2.2.2:2\n* i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\nRoute Distinguisher: 3.3.3.3:2\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:08]\n                    3.3.3.3                            32768 i\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:08]:[32]:[30.1.1.1]\n                    3.3.3.3                            32768 i\n*&gt; [3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                            32768 i\nRoute Distinguisher: 3.3.3.3:3\n*&gt; [3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                            32768 i\n\nDisplayed 9 prefixes (14 paths) \n</code></pre> <p>Also confirm the entries in the mac address table, which should now include the mac address of the the host learnt via BGP EVPN:</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net show bridge macs 00:50:79:66:68:08\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags          LastSeen\n--------  ------  ---------  -----------------  ----------  -----  -------------  --------\n30        bridge  vni30      00:50:79:66:68:08                     offload        00:00:26\nuntagged          vni30      00:50:79:66:68:08  3.3.3.3            self, offload  00:10:48\n\n// LEAF3\n\ncumulus@LEAF3:~$ net show bridge macs 00:50:79:66:68:06\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags          LastSeen\n--------  ------  ---------  -----------------  ----------  -----  -------------  --------\n10        bridge  vni10      00:50:79:66:68:06                     offload        00:11:02\nuntagged          vni10      00:50:79:66:68:06  1.1.1.1            self, offload  00:11:02\n</code></pre> <p>Remember that the presence of the mac address in the BGP EVPN table is purely controlled by the presence of the same mac address in the L2 table. When the mac is added to the L2 table, a notification is sent to BGP to add it to its EVPN table and advertise the NLRI. The mac address is subjected to its normal mac ageing timers in the L2 table - if/when the mac expires and it gets deleted from the L2 table, again, BGP is notified of the same and it sends a withdraw message for that mac address.</p> <p>This concludes the workings of the control-plane. </p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/12/cumulus-basics-part-vii---vxlan-routing---asymmetric-irb/#the-data-plane","title":"The data-plane","text":"<p>Once PC1 resolves its default gateway, it generates the native ICMP packet. LEAF1 gets this:</p> <p></p> <p>Since the destination mac address is of LEAF1, it strips off the Ethernet header and does a route lookup on the destination IP address in the IP header.</p> <pre><code>cumulus@LEAF1:~$ net show route 30.1.1.1\nRIB entry for 30.1.1.1\n======================\nRouting entry for 30.1.1.0/24\n  Known via \"connected\", distance 0, metric 0, best\n  Last update 00:10:17 ago\n  * directly connected, vlan30\n\n\nFIB entry for 30.1.1.1\n======================\n30.1.1.0/24 dev vlan30  proto kernel  scope link  src 30.1.1.254\n</code></pre> <p>The destination is directly connected, so LEAF1 can ARP for it. Here's where things get a bit interesting - if you take a packet capture, you would see no ARPs generated. This is because of an enhancement that reduces flooding within the VXLAN fabric (called ARP suppression). These VTEPs (VXLAN tunnel endpoints) maintain an arp-cache that can be populated via a type-2 mac+ip route. This local ARP cache can be used to proxy ARP (note - this is supposed to be disabled by default and enabled using the command 'net add vxlan  bridge arp-nd-suppress on' but the behavior on 3.7.5 appears to indicate that it is enabled by default).  <p>You can confirm what is there in the arp-cache using the following command:</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net show evpn arp-cache vni all\nVNI 10030 #ARP (IPv4 and IPv6, local and remote) 3\n\nIP                   Type   State    MAC               Remote VTEP          \n30.1.1.1             remote active   00:50:79:66:68:08 3.3.3.3              \nfe80::5200:ff:fe04:3 local  active   50:00:00:04:00:03\n30.1.1.254           local  active   50:00:00:04:00:03\n\nVNI 10010 #ARP (IPv4 and IPv6, local and remote) 3\n\nIP                   Type   State    MAC               Remote VTEP          \n10.1.1.254           local  active   50:00:00:04:00:03\n10.1.1.1             local  active   00:50:79:66:68:06\nfe80::5200:ff:fe04:3 local  active   50:00:00:04:00:03 \n\n// LEAF3\n\ncumulus@LEAF3:~$ net show evpn arp-cache vni all\nVNI 10030 #ARP (IPv4 and IPv6, local and remote) 3\n\nIP                   Type   State    MAC               Remote VTEP          \nfe80::5200:ff:fe03:2 local  active   50:00:00:03:00:02\n30.1.1.1             local  active   00:50:79:66:68:08\n30.1.1.254           local  active   50:00:00:03:00:02\n\nVNI 10010 #ARP (IPv4 and IPv6, local and remote) 3\n\nIP                   Type   State    MAC               Remote VTEP          \n10.1.1.254           local  active   50:00:00:03:00:02\nfe80::5200:ff:fe03:2 local  active   50:00:00:03:00:02\n10.1.1.1             remote active   00:50:79:66:68:06 1.1.1.1 \n</code></pre> <p>LEAF1 has all the required information to forward this packet. It encapsulates this with the relevant headers and forwards the packet. Assuming that the link to SPINE1 is chosen as the hash, the following packet is sent:</p> <p></p> <p>The VXLAN header contains the egress VNI - 10030 in this case. The outer IP header has a source IP address of the VTEP encapsulating the packet (1.1.1.1, in this case) and a destination IP address of the peer VTEP (3.3.3.3, in this case).</p> <p>Assuming SPINE1 gets it, it simply does a route lookup for the destination (since the destination mac address belongs to itself):</p> <pre><code>cumulus@SPINE1:~$ net show route 3.3.3.3\nRIB entry for 3.3.3.3\n=====================\nRouting entry for 3.3.3.3/32\n  Known via \"bgp\", distance 200, metric 0, best\n  Last update 02:28:05 ago\n  * fe80::5200:ff:fe03:3, via swp3\n\n\nFIB entry for 3.3.3.3\n=====================\n3.3.3.3 via 169.254.0.1 dev swp3  proto bgp  metric 20 onlink\n</code></pre> <p>It rewrites the layer2 header and forwards the packet to LEAF3. LEAF3 decapsulates it and forwards the native ICMP packet to PC3. The data-plane process can be visualized like so:</p> <p></p> <p>The same process happens backwards from PC3 to PC1 now. The egress VNI (once LEAF3 encapsulates the ICMP reply) would be 10010 in this case. The return packet from LEAF3 is this:</p> <p></p> <p>This has been fun, hasn't it? We looked at the basic workflows for both data-plane and control-plane of an asymmetric IRB based VXLAN fabric and successfully routed traffic between VNIs 10010 and 10030.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/08/cumulus-basics-part-iii---static-routing-and-ospf/","title":"Cumulus Basics Part III - static routing and OSPF","text":"<p>In this post, we will look at an introduction to routing on Cumulus Linux, with static routing and OSPF.</p>","tags":["cumulus"]},{"location":"blog/2021/12/08/cumulus-basics-part-iii---static-routing-and-ospf/#introduction","title":"Introduction","text":"<p>Initially, Cumulus OS used the Quagga suite for routing capability. However, more recently, there has been a general adoption of a fork of Quagga called FRRouting (FRR) - Cumulus now includes FRR instead of Quagga. Like always, you can either edit the files directly or using Cumulus' NCLU to enable the respective routing features as well. </p>","tags":["cumulus"]},{"location":"blog/2021/12/08/cumulus-basics-part-iii---static-routing-and-ospf/#topology","title":"Topology","text":"<p>We'll be using the following network topology for this post:</p> <p></p> <p>Routing configuration is stored in /etc/frr/frr.conf (like how interface configuration is stored in /etc/network/interfaces). It is important to note that the protocols under FRR run as deamons on the OS and are not enabled by default. To see the list of daemons and their status, you can view the /etc/frr/daemons file:</p> <pre><code>cumulus@SW1:~$ cat /etc/frr/daemons\nzebra=yes\nbgpd=no\nospfd=no\nospf6d=no\nripd=no\nripngd=no\nisisd=no\npimd=no\nldpd=no\nnhrpd=no\neigrpd=no\nbabeld=no\nsharpd=no\npbrd=no\n</code></pre> <p>'zebra' is the IP routing manager and controls things like static routes. It provides kernel routing table updates, interface lookups, and redistribution of routes between different routing protocols (quoted from 'http://docs.frrouting.org/en/latest/zebra.html').</p> <p>Let's configure some static routes to provide connectivity between PC1 and PC2. The option to add static routes comes under 'net add routing':</p> <pre><code>cumulus@SW1:~$ net add routing \n    agentx                :  Enable SNMP support for OSPF, OSPFV3, and BGP4 MIBS\n    as-path               :  AS_PATH attribute\n    community-list        :  Add a community list entry\n    defaults              :  Set of configuration defaults used\n    enable                :  To make able\n    extcommunity-list     :  An extended community list\n    import-table          :  Import routes from non-main kernel table\n    large-community-list  :  BGP large community-list\n    line                  :  A terminal line\n    log                   :  Logging control\n    mroute                :  Static unicast routes in MRIB for multicast RPF lookup\n    password              :  Set a password\n    prefix-list           :  Filter updates to/from this neighbor\n    protocol              :  Filter routing info exchanged between zebra and protocol\n    ptm-enable            :  Enable neighbor check with specified topology\n    route                 :  Static routes\n    route-map             :  Route-map\n    service               :  Service\n    zebra                 :  Zebra information\n</code></pre> <p>Among other things, you can debug zebra, create route-maps from here as well. To serve the purpose of this topology, we simply need to create a static route for 20.1.1.0/24 on SW1 with a next hop of SW2 and a static route for 10.1.1.0/24 on SW2 with a next hop of SW1.</p> <pre><code>cumulus@SW1:~$ net add routing route 20.1.1.0/24 172.16.12.2\ncumulus@SW1:~$ net commit\n\n--- /run/nclu/frr/frr.conf.scratchpad.baseline  2019-04-29 14:28:53.585648028 +0000\n+++ /run/nclu/frr/frr.conf.scratchpad   2019-04-29 14:28:53.585648028 +0000\n@@ -1,9 +1,11 @@\n frr version 4.0+cl3u8\n frr defaults datacenter\n hostname SW1\n username cumulus nopassword\n service integrated-vtysh-config\n log syslog informational\n line vty\n\n end\n+ip route 20.1.1.0/24 172.16.12.2\n+end\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  ---------------------------------------------\ncumulus  2019-04-29 14:28:53.587527  net add routing route 20.1.1.0/24 172.16.12.2\n</code></pre> <pre><code>cumulus@SW2:~$ net add routing route 10.1.1.0/24 172.16.12.1\ncumulus@SW2:~$ net commit \n--- /run/nclu/frr/frr.conf.scratchpad.baseline  2019-04-29 14:29:37.915465608 +0000\n+++ /run/nclu/frr/frr.conf.scratchpad   2019-04-29 14:29:37.915465608 +0000\n@@ -1,9 +1,11 @@\n frr version 4.0+cl3u8\n frr defaults datacenter\n hostname SW2\n username cumulus nopassword\n service integrated-vtysh-config\n log syslog informational\n line vty\n\n end\n+ip route 10.1.1.0/24 172.16.12.1\n+end\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  ---------------------------------------------\ncumulus  2019-04-29 14:29:37.918018  net add routing route 10.1.1.0/24 172.16.12.1\n</code></pre> <p>The changes made to /etc/frr/frr.conf are highlighted in the previous outputs (in blue). PC1 can ping PC2 now: </p> <pre><code>PC-1&gt; ping 20.1.1.1\n84 bytes from 20.1.1.1 icmp_seq=1 ttl=62 time=8.933 ms\n84 bytes from 20.1.1.1 icmp_seq=2 ttl=62 time=1.999 ms\n84 bytes from 20.1.1.1 icmp_seq=3 ttl=62 time=2.573 ms\n84 bytes from 20.1.1.1 icmp_seq=4 ttl=62 time=2.043 ms\n84 bytes from 20.1.1.1 icmp_seq=5 ttl=62 time=2.237 ms\n</code></pre> <p>Pretty straightforward, wasn't it?</p> <p>I am going to undo these changes now and move towards a routing protocol like OSPF instead.</p> <pre><code>cumulus@SW1:~$ net del routing route 20.1.1.0/24 172.16.12.2\ncumulus@SW1:~$ net commit\n\ncumulus@SW2:~$ net del routing route 10.1.1.0/24 172.16.12.1\ncumulus@SW2:~$ net commit \n</code></pre> <p>Let's bring up OSPF now.</p> <pre><code>// SW1 configuration\n\ncumulus@SW1:~$ net add ospf router-id 1.1.1.1\ncumulus@SW1:~$ net add interface swp2 ospf area 0     \ncumulus@SW1:~$ net add interface swp2 ospf network point-to-point \ncumulus@SW1:~$ net add interface swp1 ospf area 0\ncumulus@SW1:~$ net add interface swp1 ospf passive\ncumulus@SW1:~$ net commit\n\n// SW2 configuration\n\ncumulus@SW2:~$ net add ospf router-id 2.2.2.2\ncumulus@SW2:~$ net add interface swp2 ospf area 0\ncumulus@SW2:~$ net add interface swp2 ospf network point-to-point\ncumulus@SW2:~$ net add interface swp1 ospf area 0\ncumulus@SW2:~$ net add interface swp1 ospf passive \ncumulus@SW2:~$ net commit \n</code></pre> <p>Verify that OSPF is up and that the LSDB has the correct information:</p> <pre><code>cumulus@SW1:~$ net show ospf neighbor \n\nNeighbor ID     Pri State           Dead Time Address         Interface            RXmtL RqstL DBsmL\n2.2.2.2           1 Full/DROther      36.571s 172.16.12.2     swp2:172.16.12.1         0     0     0\n\n\ncumulus@SW2:~$ net show ospf neighbor \n\nNeighbor ID     Pri State           Dead Time Address         Interface            RXmtL RqstL DBsmL\n1.1.1.1           1 Full/DROther      35.519s 172.16.12.1     swp2:172.16.12.2         0     0     0\n</code></pre> <pre><code>cumulus@SW1:~$ net show ospf database \n\n       OSPF Router with ID (1.1.1.1)\n\n                Router Link States (Area 0.0.0.0)\n\nLink ID         ADV Router      Age  Seq#       CkSum  Link count\n1.1.1.1         1.1.1.1           79 0x80000006 0x470d 3\n2.2.2.2         2.2.2.2          100 0x80000006 0x1e27 3\n\ncumulus@SW1:~$ net show ospf database router 1.1.1.1\n\n       OSPF Router with ID (1.1.1.1)\n\n\n                Router Link States (Area 0.0.0.0)\n\n  LS age: 89\n  Options: 0x2  : *|-|-|-|-|-|E|-\n  LS Flags: 0x3  \n  Flags: 0x0\n  LS Type: router-LSA\n  Link State ID: 1.1.1.1 \n  Advertising Router: 1.1.1.1\n  LS Seq Number: 80000006\n  Checksum: 0x470d\n  Length: 60\n\n   Number of Links: 3\n\n    Link connected to: another Router (point-to-point)\n     (Link ID) Neighboring Router ID: 2.2.2.2\n     (Link Data) Router Interface address: 172.16.12.1\n      Number of TOS metrics: 0\n       TOS 0 Metric: 100\n\n    Link connected to: Stub Network\n     (Link ID) Net: 172.16.12.0\n     (Link Data) Network Mask: 255.255.255.0\n      Number of TOS metrics: 0\n       TOS 0 Metric: 100\n\n    Link connected to: Stub Network\n     (Link ID) Net: 10.1.1.0\n     (Link Data) Network Mask: 255.255.255.0\n      Number of TOS metrics: 0\n       TOS 0 Metric: 100\n\n\ncumulus@SW1:~$ net show ospf database router 2.2.2.2\n\n       OSPF Router with ID (1.1.1.1)\n\n\n                Router Link States (Area 0.0.0.0)\n\n  LS age: 114\n  Options: 0x2  : *|-|-|-|-|-|E|-\n  LS Flags: 0x6  \n  Flags: 0x0\n  LS Type: router-LSA\n  Link State ID: 2.2.2.2 \n  Advertising Router: 2.2.2.2\n  LS Seq Number: 80000006\n  Checksum: 0x1e27\n  Length: 60\n\n   Number of Links: 3\n\n    Link connected to: another Router (point-to-point)\n     (Link ID) Neighboring Router ID: 1.1.1.1\n     (Link Data) Router Interface address: 172.16.12.2\n      Number of TOS metrics: 0\n       TOS 0 Metric: 100\n\n    Link connected to: Stub Network\n     (Link ID) Net: 172.16.12.0\n     (Link Data) Network Mask: 255.255.255.0\n      Number of TOS metrics: 0\n       TOS 0 Metric: 100\n\n    Link connected to: Stub Network\n     (Link ID) Net: 20.1.1.0\n     (Link Data) Network Mask: 255.255.255.0\n      Number of TOS metrics: 0\n       TOS 0 Metric: 100\n</code></pre> <p>SW1 and SW2 should have installed 10.1.1.0/24 and 20.1.1.0/24 respectively into RIB/FIB as well.</p> <pre><code>cumulus@SW1:~$ net show route ospf\nRIB entry for ospf\n==================\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR,\n       &gt; - selected route, * - FIB route\n\nO   10.1.1.0/24 [110/100] is directly connected, swp1, 00:02:52\nO&gt;* 20.1.1.0/24 [110/200] via 172.16.12.2, swp2, 00:02:42\nO   172.16.12.0/24 [110/100] is directly connected, swp2, 00:02:52\n\ncumulus@SW2:~$ net show route ospf\nRIB entry for ospf\n==================\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR,\n       &gt; - selected route, * - FIB route\n\nO&gt;* 10.1.1.0/24 [110/200] via 172.16.12.1, swp2, 00:03:51\nO   20.1.1.0/24 [110/100] is directly connected, swp1, 00:04:21\nO   172.16.12.0/24 [110/100] is directly connected, swp2, 00:04:21\n</code></pre> <p>The only thing left to verify is the connectivity from PC1 to PC2. </p> <pre><code>PC-1&gt; ping 20.1.1.1\n84 bytes from 20.1.1.1 icmp_seq=1 ttl=62 time=3.140 ms\n84 bytes from 20.1.1.1 icmp_seq=2 ttl=62 time=2.152 ms\n84 bytes from 20.1.1.1 icmp_seq=3 ttl=62 time=2.563 ms\n84 bytes from 20.1.1.1 icmp_seq=4 ttl=62 time=2.020 ms\n84 bytes from 20.1.1.1 icmp_seq=5 ttl=62 time=3.031 ms\n</code></pre> <p>Works like a charm! In our next post, we'll take a look at implementing BGP with OSPF as an IGP on Cumulus VX. </p>","tags":["cumulus"]},{"location":"blog/2021/12/13/cumulus-basics-part-viii---vxlan-symmetric-routing-and-multi-tenancy/","title":"Cumulus Basics Part VIII - VXLAN symmetric routing and multi-tenancy","text":"<p>In this post, we look at VXLAN routing with symmetric IRB and multi-tenancy on Cumulus Linux.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/13/cumulus-basics-part-viii---vxlan-symmetric-routing-and-multi-tenancy/#introduction","title":"Introduction","text":"<p>Now that we've configured and verified a working asymmetric VXLAN routing solution in the previous post, let's take a look at the greener side of the grass (well, it depends on where you stand) - symmetric IRB. This post is going to introduce VRFs into the picture that pave the way for multi-tenancy in VXLAN solutions. </p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/13/cumulus-basics-part-viii---vxlan-symmetric-routing-and-multi-tenancy/#topology","title":"Topology","text":"<p>We continue to use the same topology as our previous post:</p> <p></p> <p>Any configuration we added for asymmetric routing has been removed. We simply have our L2VNIs created on LEAF1 and LEAF3 and the BGP peerings are up. </p> <p>Before we begin with the actual configuration, let's understand the logic behind symmetric IRB. This functions as a bridge-route-route-bridge model where the packet is bridged to your source VTEP from the host and is then routed into a new type of VNI called the L3VNI. This takes the packet through your fabric core to the destination VTEP, where it is routed from the L3VNI into the local L2VNI and then bridged across to the destination host. </p> <p>In addition to the L3VNIs, we also introduce VRFs here. VRFs allow for multi-tenancy. Imagine this - instead of having a dedicated core infrastructure for every customer, you could have this core infrastructure common to multiple customers with the kind of segregation that VRFs can provide. The L3VNIs are tied to the customer VRFs directly and in that sense, the L3VNIs should be unique per VRF.</p> <p>The configuration for symmetric IRB is not very complicated. Let's take LEAF1 as an example and start to configure this:</p> <pre><code>// first, create a VLAN for your L3VNI \n\ncumulus@LEAF1:~$ net add vlan 40 alias VLAN for L3VNI 10040\n\n// next, create the L3VNI, map it to the VLAN and configure the tunnel source \n\ncumulus@LEAF1:~$ net add vxlan vni40 vxlan id 10040\ncumulus@LEAF1:~$ net add vxlan vni40 vxlan local-tunnelip 1.1.1.1\ncumulus@LEAF1:~$ net add vxlan vni40 bridge access 40\n\n// now create the tenant VRF and map it to the L3VNI\n\ncumulus@LEAF1:~$ net add vrf TENANT1 vni 10040\n</code></pre> <p>Now that we have some of these pieces configured, we need to start making sense of this and putting it all together. </p> <p>Naturally, to segregate your customers, they need to be put in their respective tenant VRFs. This means that the customer VLAN (particularly, the first L3 hop, which is the corresponding SVI in this case) needs to be in the tenant VRF. Additionally, you add the corresponding VLAN for the L3VNI in the same tenant VRF. </p> <pre><code>// customer VLAN goes into the tenant VRF\n\ncumulus@LEAF1:~$ net add vlan 10 vrf TENANT1\n\n// VLAN corresponding to the L3VNI goes into tenant VRF as well\n\ncumulus@LEAF1:~$ net add vlan 40 vrf TENANT1\n</code></pre> <p>Confirm that the customer facing SVI and the SVI corresponding to the L3VNI are up and in the correct VRF:</p> <pre><code>// customer facing SVI \n\ncumulus@LEAF1:~$ net show interface vlan10\n    Name    MAC                Speed  MTU   Mode\n--  ------  -----------------  -----  ----  ------------\nUP  vlan10  50:00:00:04:00:03  N/A    1500  Interface/L3\n\nIP Details\n-------------------------  -------------\nIP:                        10.1.1.254/24\nIP Neighbor(ARP) Entries:  1\n\ncl-netstat counters\n-------------------\nRX_OK  RX_ERR  RX_DRP  RX_OVR  TX_OK  TX_ERR  TX_DRP  TX_OVR\n-----  ------  ------  ------  -----  ------  ------  ------\n   22       0       0       0     36       0       0       0\n\nRouting\n-------\n  Interface vlan10 is up, line protocol is up\n  Link ups:       1    last: 2019/06/06 03:30:36.79\n  Link downs:     1    last: 2019/06/06 03:30:36.79\n  PTM status: disabled\n  vrf: TENANT1\n  index 10 metric 0 mtu 1500 speed 0\n  flags: &lt;UP,BROADCAST,RUNNING&gt;,MULTICAST&gt;\n  Type: Ethernet\n  HWaddr: 50:00:00:04:00:03\n  inet 10.1.1.254/24\n  inet6 fe80::5200:ff:fe04:3/64\n  Interface Type Vlan\n  VLAN Id 10\n  Link ifindex 9(bridge)\n\n// SVI corresponding to the L3VNI  \n\ncumulus@LEAF1:~$ net show interface vlan40\n    Name    MAC                Speed  MTU   Mode\n--  ------  -----------------  -----  ----  -------------\nUP  vlan40  50:00:00:04:00:03  N/A    1500  NotConfigured\n\nAlias\n-----\nVLAN for L3VNI 10040\n\ncl-netstat counters\n-------------------\nRX_OK  RX_ERR  RX_DRP  RX_OVR  TX_OK  TX_ERR  TX_DRP  TX_OVR\n-----  ------  ------  ------  -----  ------  ------  ------\n   13       0       0       0     20       0       0       0\n\nRouting\n-------\n  Interface vlan40 is up, line protocol is up\n  Link ups:       3    last: 2019/06/06 03:32:58.69\n  Link downs:     2    last: 2019/06/06 03:32:58.69\n  PTM status: disabled\n  vrf: TENANT1\n  Description: VLAN for L3VNI 10040\n  index 12 metric 0 mtu 1500 speed 0\n  flags: &lt;UP,BROADCAST,RUNNING&gt;,MULTICAST&gt;\n  Type: Unknown\n  HWaddr: 50:00:00:04:00:03\n  inet6 fe80::5200:ff:fe04:3/64\n  Interface Type Vlan\n  VLAN Id 40\n  Link ifindex 9(bridge)\n</code></pre> <p>Both the L2VNI and the L3VNI is going to be assigned a RD/RT value. You can confirm this using 'net show bgp evpn vni\":</p> <pre><code>cumulus@LEAF1:~$ net show bgp evpn vni \nAdvertise Gateway Macip: Disabled\nAdvertise SVI Macip: Disabled\nAdvertise All VNI flag: Enabled\nBUM flooding: Head-end replication\nNumber of L2 VNIs: 1\nNumber of L3 VNIs: 1\nFlags: * - Kernel\n  VNI        Type RD                    Import RT                 Export RT                 Tenant VRF                           \n* 10010      L2   1.1.1.1:3             1:10010                   1:10010                  TENANT1                              \n* 10040      L3   10.1.1.254:2          1:10040                   1:10040                  TENANT1\n</code></pre> <p>Similar configuration is done on LEAF3:</p> <pre><code>cumulus@LEAF3:~$ net add vlan 40\ncumulus@LEAF3:~$ net add vxlan vni40 vxlan id 10040\ncumulus@LEAF3:~$ net add vxlan vni40 vxlan local-tunnelip 3.3.3.3\ncumulus@LEAF3:~$ net add vxlan vni40 bridge access 40\ncumulus@LEAF3:~$ net add vrf TENANT1 vni 10040\ncumulus@LEAF3:~$ net add vlan 30 vrf TENANT1\ncumulus@LEAF3:~$ net add vlan 40 vrf TENANT1\n</code></pre> <p>Let's take a quick peek at the BGP EVPN table and see what is in there:</p> <pre><code>// LEAF1s BGP EVPN table\n\ncumulus@LEAF1:~$ net show bgp l2vpn evpn route\nBGP table version is 10, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:3\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                            32768 i\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                            32768 i\n*&gt; [3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                            32768 i\nRoute Distinguisher: 2.2.2.2:2\n* i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\n\n\n// LEAF3s BGP EVPN table\n\ncumulus@LEAF3:~$ net show bgp l2vpn evpn route\nBGP table version is 3, local router ID is 3.3.3.3\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:3\n* i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                  0    100      0 i\n* i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                  0    100      0 i\n* i[3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                  0    100      0 i\nRoute Distinguisher: 2.2.2.2:2\n* i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\nRoute Distinguisher: 3.3.3.3:2\n*&gt; [3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                            32768 i\n</code></pre> <p>It appears that PC1s mac and IP address have already been learnt and installed in the BGP EVPN table. However, there is no information about PC3 in here (most likely because PC3 has not sent any traffic so far). So, let's take this opportunity to generate some traffic from PC3 and understand how the control-plane is built with L3VNIs.</p> <p>We will enable some debugs to understand what is going on. As explained in the previous post, you must enable logging of syslogs at the debugging level and then enable the debugs. The following debugs were enabled:</p> <pre><code>LEAF3# show debug\nZebra debugging status:\n  Zebra event debugging is on\n  Zebra packet detail debugging is on\n  Zebra kernel debugging is on\n  Zebra RIB debugging is on\n  Zebra VXLAN debugging is on\n\nBGP debugging status:\n  BGP updates debugging is on (inbound)\n  BGP updates debugging is on (outbound)\n  BGP zebra debugging is on\n  BGP vpn label event debugging is on\n</code></pre> <p>The same debugs were enabled on LEAF1 as well so as to capture simultaneous debugging information from both. </p> <p>We generate some traffic from PC3 now (by pinging its default gateway, which is SVI30 on LEAF3). Remember, all debugs are redirected to /var/log/frr/frr.log. Snippets of relevant debug logs are below:</p> <pre><code>// LEAF3\n\n    // LEAF3 zebra learns PC3s mac address and notifies BGP\n\n    2019-06-06T10:01:51.630081+00:00 LEAF3 zebra[936]: ADD MAC 00:50:79:66:68:08 intf swp2(4) VID 30 -&gt; VNI 10030\n    2019-06-06T10:01:51.630191+00:00 LEAF3 zebra[936]: Send MACIP Add flags 0x0 MAC 00:50:79:66:68:08 IP  seq 0 L2-VNI 10030 to bgp\n    2019-06-06T10:01:51.630301+00:00 LEAF3 zebra[936]: netlink_parse_info: netlink-listen (NS 0) type RTM_NEWROUTE(24), len=116, seq=0, pid=0\n    2019-06-06T10:01:51.630570+00:00 LEAF3 zebra[936]: RTM_NEWROUTE ipv6 unicast proto  NS 0\n\n    // bgpd receives the notification from zebra\n\n    2019-06-06T10:01:51.632750+00:00 LEAF3 bgpd[948]: 0:Recv MACIP Add flags 0x0 MAC 00:50:79:66:68:08 IP  VNI 10030 seq 0 state 0\n    2019-06-06T10:01:51.683158+00:00 LEAF3 bgpd[948]: group_announce_route_walkcb: afi=l2vpn, safi=evpn, p=[2]:[00:50:79:66:68:08]/224\n    2019-06-06T10:01:51.683739+00:00 LEAF3 bgpd[948]: subgroup_process_announce_selected: p=[2]:[00:50:79:66:68:08]/224, selected=0x338517eed0\n\n    // bgpd sends an update to its peer about this new learn\n\n    2019-06-06T10:01:51.684041+00:00 LEAF3 bgpd[948]: u2:s2 send UPDATE w/ attr: nexthop 3.3.3.3, localpref 100, extcommunity ET:8 RT:1:10030 RT:1:10040    Rmac:50:00:00:03:00:02, path\n    2019-06-06T10:01:51.684266+00:00 LEAF3 bgpd[948]: u2:s2 send MP_REACH for afi/safi 25/70\n    2019-06-06T10:01:51.684441+00:00 LEAF3 bgpd[948]: u2:s2 send UPDATE RD 3.3.3.3:2 [2]:[00:50:79:66:68:08]/224 label 10030/10040 l2vpn evpn\n    2019-06-06T10:01:51.684808+00:00 LEAF3 bgpd[948]: u2:s2 send UPDATE len 121 numpfx 1\n    2019-06-06T10:01:51.685071+00:00 LEAF3 bgpd[948]: u2:s2 swp1 send UPDATE w/ nexthop 3.3.3.3\n    2019-06-06T10:01:51.685410+00:00 LEAF3 bgpd[948]: u2:s2 swp3 send UPDATE w/ nexthop 3.3.3.3\n\n// LEAF1\n\n    // LEAF1s bgpd receives the update from SPINE1\n\n    2019-06-06T10:01:51.929153+00:00 LEAF1 bgpd[890]: swp1 rcvd UPDATE w/ attr: nexthop 3.3.3.3, localpref 100, metric 0, extcommunity RT:1:10030 RT:1:10040 ET:8   Rmac:50:00:00:03:00:02, originator 3.3.3.3, clusterlist 11.11.11.11, path\n    2019-06-06T10:01:51.929462+00:00 LEAF1 bgpd[890]: swp1 rcvd UPDATE wlen 0 attrlen 126 alen 0\n    2019-06-06T10:01:51.929665+00:00 LEAF1 bgpd[890]: swp1 rcvd RD 3.3.3.3:2 [2]:[00:50:79:66:68:08]:[30.1.1.1]/224 label 10030/10040 l2vpn evpn\n\n    // LEAF1s bgpd installs the prefix in the BGP EVPN table\n\n    2019-06-06T10:01:51.929940+00:00 LEAF1 bgpd[890]: installing evpn prefix [2]:[00:50:79:66:68:08]:[30.1.1.1]/224 as ip prefix 30.1.1.1/32 in vrf TENANT1\n\n    // LEAF1s bgpd receives the update from SPINE2\n\n    2019-06-06T10:01:51.930173+00:00 LEAF1 bgpd[890]: swp2 rcvd UPDATE w/ attr: nexthop 3.3.3.3, localpref 100, metric 0, extcommunity RT:1:10030 RT:1:10040 ET:8   Rmac:50:00:00:03:00:02, originator 3.3.3.3, clusterlist 22.22.22.22, path\n    2019-06-06T10:01:51.930381+00:00 LEAF1 bgpd[890]: swp2 rcvd UPDATE wlen 0 attrlen 126 alen 0\n    2019-06-06T10:01:51.930550+00:00 LEAF1 bgpd[890]: swp2 rcvd RD 3.3.3.3:2 [2]:[00:50:79:66:68:08]:[30.1.1.1]/224 label 10030/10040 l2vpn evpn\n\n    // LEAF1s bgpd installs the prefix in the BGP EVPN table\n\n    2019-06-06T10:01:51.930799+00:00 LEAF1 bgpd[890]: installing evpn prefix [2]:[00:50:79:66:68:08]:[30.1.1.1]/224 as ip prefix 30.1.1.1/32 in vrf TENANT1\n\n    // LEAF1s bgpd notifies zebra\n\n    2019-06-06T10:01:51.981956+00:00 LEAF1 bgpd[890]: bgp_zebra_announce: p=30.1.1.1/32, bgp_is_valid_label: 2\n    2019-06-06T10:01:51.982328+00:00 LEAF1 bgpd[890]: Tx route add VRF 14 30.1.1.1/32 metric 0 tag 0 flags 0x1409 nhnum 1\n    2019-06-06T10:01:51.982524+00:00 LEAF1 bgpd[890]:   nhop [1]: 303:303:: if 12 VRF 14\n    2019-06-06T10:01:51.982848+00:00 LEAF1 bgpd[890]: bgp_zebra_announce: 30.1.1.1/32: announcing to zebra (recursion set)\n\n\n    // LEAF1s zebra gets this message from bgpd and processes it\n\n    2019-06-06T10:01:51.983503+00:00 LEAF1 zebra[881]: zebra message comes from socket [16]\n    2019-06-06T10:01:51.983768+00:00 LEAF1 zebra[881]: Tx RTM_NEWNEIGH family ipv4 IF vlan40(12) Neigh 3.3.3.3 MAC 50:00:00:03:00:02 flags 0x10 state 0x40\n    2019-06-06T10:01:51.983959+00:00 LEAF1 zebra[881]: netlink_talk: netlink-cmd (NS 0) type RTM_NEWNEIGH(28), len=48 seq=46 flags 0x505\n    2019-06-06T10:01:51.984200+00:00 LEAF1 zebra[881]: netlink_parse_info: netlink-cmd (NS 0) ACK: type=RTM_NEWNEIGH(28), seq=46, pid=4294963174\n    2019-06-06T10:01:51.984498+00:00 LEAF1 zebra[881]: Tx RTM_NEWNEIGH family bridge IF vni40(13) VLAN 40 MAC 50:00:00:03:00:02 dst 3.3.3.3\n    2019-06-06T10:01:51.984770+00:00 LEAF1 zebra[881]: netlink_talk: netlink-cmd (NS 0) type RTM_NEWNEIGH(28), len=64 seq=47 flags 0x505\n    2019-06-06T10:01:51.985005+00:00 LEAF1 zebra[881]: netlink_parse_info: netlink-cmd (NS 0) ACK: type=RTM_NEWNEIGH(28), seq=47, pid=4294963174\n    2019-06-06T10:01:51.985202+00:00 LEAF1 zebra[881]: rib_add_multipath: 14:30.1.1.1/32: Inserting route rn 0x653ba3c640, re 0x653b8a7e60 (type 9) existing (nil)\n</code></pre> <p>As you can see, this gets learnt in the BGP EVPN table on LEAF1 and pushed to RIB/FIB as well (for simplicity sake, I have shut down all links to SPINE2):</p> <pre><code>// LEAF1s BGP EVPN table\n\ncumulus@LEAF1:~$ net show bgp l2vpn evpn route\nBGP table version is 4, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:3\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                            32768 i\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                            32768 i\n*&gt; [3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                            32768 i\nRoute Distinguisher: 2.2.2.2:2\n*&gt;i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\nRoute Distinguisher: 3.3.3.3:2\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:08]\n                    3.3.3.3                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:08]:[32]:[30.1.1.1]\n                    3.3.3.3                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                  0    100      0 i\n\n// LEAF1s RIB/FIB                    \n\ncumulus@LEAF1:~$ net show route vrf TENANT1 30.1.1.1\nRIB entry for 30.1.1.1 in vrf TENANT1\n=====================================\nRouting entry for 30.1.1.1/32\n  Known via \"bgp\", distance 200, metric 0, vrf TENANT1, best\n  Last update 00:04:26 ago\n  * 3.3.3.3, via vlan40 onlink\n\n\nFIB entry for 30.1.1.1 in vrf TENANT1\n=====================================\n30.1.1.1 via 3.3.3.3 dev vlan40  proto bgp  metric 20 onlink\n</code></pre> <p>The BGP update itself looks like this:</p> <p></p> <p>The control-plane exchange can be visualized as following:</p> <p></p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/13/cumulus-basics-part-viii---vxlan-symmetric-routing-and-multi-tenancy/#understanding-the-data-plane","title":"Understanding the data-plane","text":"<p>The ICMP request from PC1 hits LEAF1. Since the destination mac address is owned by LEAF1, it strips off the Layer2 header and does a lookup against the destination IP address in the IP header.</p> <pre><code>cumulus@LEAF1:~$ net show route vrf TENANT1 30.1.1.1\nRIB entry for 30.1.1.1 in vrf TENANT1\n=====================================\nRouting entry for 30.1.1.1/32\n  Known via \"bgp\", distance 200, metric 0, vrf TENANT1, best\n  Last update 00:04:26 ago\n  * 3.3.3.3, via vlan40 onlink\n\n\nFIB entry for 30.1.1.1 in vrf TENANT1\n=====================================\n30.1.1.1 via 3.3.3.3 dev vlan40  proto bgp  metric 20 onlink\n</code></pre> <p>This gets encapsulated with the appropriate headers and the outer destination IP address is set to 3.3.3.3. The VNI inside the VXLAN header is set to 10040 which is what VLAN 40 is associated to. </p> <p></p> <p>The data-plane packet, post encapsulation:</p> <p></p> <p>As you can see, the source and destination IP addresses in the outer header belong to the source VTEP and the destination VTEP respectively. A UDP header follows, wherein the destination port signifies the following header as VXLAN. Inside the VXLAN header, the VNI is set to 10040, which is the L3VNI shared between VTEPs, uniquely identifying the VRF. </p> <p>This encapsulated packet is sent towards the SPINE1/SPINE2. </p> <p></p> <p>Assuming it hits SPINE1, a traditional route lookup is done against the destination IP address in the outer header (which is 3.3.3.3 in this case). This is advertised in the underlay and SPINE1 knows that the next-hop for this is LEAF3. </p> <pre><code>cumulus@SPINE1:~$ net show route 3.3.3.3\nRIB entry for 3.3.3.3\n=====================\nRouting entry for 3.3.3.3/32\n  Known via \"bgp\", distance 200, metric 0, best\n  Last update 02:35:42 ago\n  * fe80::5200:ff:fe03:3, via swp3\n\n\nFIB entry for 3.3.3.3\n=====================\n3.3.3.3 via 169.254.0.1 dev swp3  proto bgp  metric 20 onlink \n</code></pre> <p>SPINE1 now forwards this to LEAF3 (remember, the packet is still encapsulated and is simply being routed in the underlay till it reaches the destination VTEP):</p> <p></p> <p>The packet reaches LEAF3 now. The destination mac address in the outer Ethernet header is owned by it so this gets stripped. The destination IP address in the outer IP header is also owned by it, so this gets stripped as well. </p> <p>LEAF3 parses the UDP header and understands that a VXLAN header follows. It then parses the VXLAN header - the most relevant information here is the embedded VNI. </p> <p>Why is this VNI (the L3VNI) so important? LEAF3 (the destination VTEP) uses this VNI to determine which VRF table should be consulted to do the inner destination IP address lookup in. This is how the separation of customers is achieved end to end. </p> <pre><code>cumulus@LEAF3:~$ net show vrf vni \nVRF                                   VNI        VxLAN IF             L3-SVI               State Rmac              \nTENANT1                               10040      vni40                vlan40               Up    50:00:00:03:00:02 \nTENANT2                               10400      vni400               vlan400              Up    50:00:00:03:00:02\n</code></pre> <p>LEAF3 can now look at the VRF table for TENANT1 to determine how to get to 30.1.1.1:</p> <pre><code>cumulus@LEAF3:~$ net show route vrf TENANT1 30.1.1.1\nRIB entry for 30.1.1.1 in vrf TENANT1\n=====================================\nRouting entry for 30.1.1.0/24\n  Known via \"connected\", distance 0, metric 0, vrf TENANT1, best\n  Last update 00:10:47 ago\n  * directly connected, vlan30\n\n\nFIB entry for 30.1.1.1 in vrf TENANT1\n=====================================\n30.1.1.0/24 dev vlan30  proto kernel  scope link  src 30.1.1.254 \n</code></pre> <p>Remember, the EVPN arp-cache table should also be populated with information about 30.1.1.1:</p> <pre><code>cumulus@LEAF3:~$ net show evpn arp-cache vni all\nVNI 10030 #ARP (IPv4 and IPv6, local and remote) 3\n\nIP                   Type   State    MAC               Remote VTEP          \nfe80::5200:ff:fe03:2 local  active   50:00:00:03:00:02\n30.1.1.1             local  active   00:50:79:66:68:08\n30.1.1.254           local  active   50:00:00:03:00:02\n\nVNI 10300 #ARP (IPv4 and IPv6, local and remote) 2\n\nIP                   Type   State    MAC               Remote VTEP          \nfe80::5200:ff:fe03:2 local  active   50:00:00:03:00:02\n30.1.1.254           local  active   50:00:00:03:00:02 \n</code></pre> <p>The packet is now forwarded to PC3 and it can respond back. The same process occurs in the reverse direction as well. </p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/15/cumulus-part-x---vxlan-evpn-and-mlag/","title":"Cumulus Part X - VXLAN EVPN and MLAG","text":"<p>In this post, we take a look at the interaction of MLAG with an EVPN based VXLAN fabric on Cumulus Linux.</p>","tags":["cumulus","vxlan","evpn","mlag"]},{"location":"blog/2021/12/15/cumulus-part-x---vxlan-evpn-and-mlag/#introduction","title":"Introduction","text":"<p>MLAG or MC-LAG (multi-chassis link aggregation) is a fairly common deployment model at the access/leaf layer of both Enterprise and Data Center networks, typically offered by most leading vendors (with different terminologies - vPC, VSS, stackwise-virtual and so on).</p> <p>The general idea is to offer redundancy at the access layer by pairing together two access/leaf switches into a common logical switch, from the perspective of any devices downstream.  Details of Cumulus' implementation can be found here. </p>","tags":["cumulus","vxlan","evpn","mlag"]},{"location":"blog/2021/12/15/cumulus-part-x---vxlan-evpn-and-mlag/#topology","title":"Topology","text":"<p>For this post, we're going to be using the following topology (tested with Cumulus VX 4.2):</p> <p></p> <p>We have three servers, Server5, Server6 and Server3 in VLAN 10, with another server, Server2, in VLAN 20. Server5 is uplinked to both MLAG peers, while Server6 is an orphan device, off of Leaf4 only. </p> <p>We also have external connectivity via PE1, again, connected only to one of the MLAG peers - Leaf3, in this case. PE1 is advertising 99.99.99.99/32, an external network, to Leaf3. </p> <p>Logically, for BGP peering, the spines share a common AS, while each leaf is it's own unique AS. This is a standard eBGP type design, meant to avoid BGP path hunting issues. </p> <p></p>","tags":["cumulus","vxlan","evpn","mlag"]},{"location":"blog/2021/12/15/cumulus-part-x---vxlan-evpn-and-mlag/#basic-configuration","title":"Basic configuration","text":"<p>Each of these devices have a loopback configured. For the leaf's, these loopbacks are the VTEP IPs. The MLAG pair have unique loopacks, but also an anycast CLAG VTEP IP that is configured (similar to a secondary IP):</p> <p></p> <p>This CLAG anycast IP is configured under the loopback itself, for both Leaf3 and Leaf4:</p> <pre><code>// Leaf3\n\ncumulus@Leaf3:mgmt:~$ net show configuration interface lo\ninterface lo\n  # The primary network interface\n  address 3.3.3.3/32\n  clagd-vxlan-anycast-ip 34.34.34.34\n  vxlan-local-tunnelip 3.3.3.3\n\n// Leaf4\n\ncumulus@Leaf4:mgmt:~$ net show configuration interface lo\ninterface lo\n  # The primary network interface\n  address 4.4.4.4/32\n  clagd-vxlan-anycast-ip 34.34.34.34\n  vxlan-local-tunnelip 4.4.4.4\n</code></pre> <p>Each of the MLAG peers form an eBGP peering with the spines, and an iBGP peering with each other. This iBGP peering is important for failure conditions (we'll look at this in a little bit).</p> <pre><code>// IPv4 unicast peering\n\ncumulus@Leaf3:mgmt:~$ net show bgp ipv4 unicast summary \nBGP router identifier 3.3.3.3, local AS number 64523 vrf-id 0\nBGP table version 26\nRIB entries 11, using 2112 bytes of memory\nPeers 3, using 64 KiB of memory\n\nNeighbor             V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLeaf4(peerlink.4094) 4      64523     87384     87976        0    0    0 07:26:27            5\nSpine1(swp1)         4      65550     87974     87971        0    0    0 07:34:50            3\nSpine2(swp2)         4      65550     87973     87968        0    0    0 07:34:50            3\n\nTotal number of neighbors 3 \n\n// L2VPN EVPN unicast peering\n\ncumulus@Leaf3:mgmt:~$ net show bgp l2vpn evpn summary \nBGP router identifier 3.3.3.3, local AS number 64523 vrf-id 0\nBGP table version 0\nRIB entries 25, using 4800 bytes of memory\nPeers 3, using 64 KiB of memory\n\nNeighbor             V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLeaf4(peerlink.4094) 4      64523     87387     87979        0    0    0 07:26:37           30\nSpine1(swp1)         4      65550     87977     87974        0    0    0 07:35:00           30\nSpine2(swp2)         4      65550     87976     87971        0    0    0 07:35:00           30\n\nTotal number of neighbors 3\n</code></pre>","tags":["cumulus","vxlan","evpn","mlag"]},{"location":"blog/2021/12/15/cumulus-part-x---vxlan-evpn-and-mlag/#control-plane-learning-with-mlag","title":"Control-plane learning with MLAG","text":"<p>When a MAC address is learnt over the MLAG, it is synced to the MLAG peer as well. Both the peers would insert the entry in their BGP EVPN tables and advertise it out. As an example, Server5s MAC address is learnt by both Leaf3 and Leaf4 and advertised via BGP EVPN to the spines and to each other, over the iBGP peering.</p> <pre><code>// Leaf3\n\ncumulus@Leaf3:mgmt:~$ net show bridge macs 00:00:00:00:00:05\n\nVLAN  Master  Interface     MAC                TunnelDest  State  Flags  LastSeen\n----  ------  ------------  -----------------  ----------  -----  -----  --------\n  10  bridge  bond-server5  00:00:00:00:00:05                            00:01:03\n\ncumulus@Leaf3:mgmt:~$ net show bgp l2vpn evpn route rd 3.3.3.3:2 mac 00:00:00:00:00:05 \nBGP routing table entry for 3.3.3.3:2:[2]:[0]:[48]:[00:00:00:00:00:05]\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Leaf4(peerlink.4094) Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:00:00:00:00:05] VNI 10010/10040\n  Local\n    34.34.34.34 from 0.0.0.0 (3.3.3.3)\n      Origin IGP, weight 32768, valid, sourced, local, bestpath-from-AS Local, best (First path received)\n      Extended Community: ET:8 RT:3:10010 RT:3:10040 Rmac:44:38:39:ff:00:05 MM:1\n      Last update: Wed Aug  4 06:28:11 2021\n\n// Leaf4\n\ncumulus@Leaf4:mgmt:~$ net show bridge macs 00:00:00:00:00:05\n\nVLAN  Master  Interface     MAC                TunnelDest  State  Flags  LastSeen\n----  ------  ------------  -----------------  ----------  -----  -----  --------\n  10  bridge  bond-server5  00:00:00:00:00:05                            00:00:11\n\ncumulus@Leaf4:mgmt:~$ net show bgp l2vpn evpn route rd 4.4.4.4:3 mac 00:00:00:00:00:05\nBGP routing table entry for 4.4.4.4:3:[2]:[0]:[48]:[00:00:00:00:00:05]\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Leaf3(peerlink.4094) Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:00:00:00:00:05] VNI 10010/10040\n  Local\n    34.34.34.34 from 0.0.0.0 (4.4.4.4)\n      Origin IGP, weight 32768, valid, sourced, local, bestpath-from-AS Local, best (First path received)\n      Extended Community: ET:8 RT:4:10010 RT:4:10040 Rmac:44:38:39:ff:00:05\n      Last update: Wed Aug  4 12:17:01 2021\n</code></pre> <p>There are two big things to remember with MLAG and BGP EVPN advertisements:</p> <ol> <li> <p>Type-2 EVPN prefixes are sent using the anycast VTEP IP address as the next-hop. </p> </li> <li> <p>Type-5 EVPN prefixes are sent using the local VTEP IP address (default behavior in Cumulus Linux, other vendors provide a knob to optionally enable it).</p> </li> </ol> <p>The first big why - why do we need an anycast VTEP IP address for the MLAG peers? This just allows for easy BGP filtering - remember, when Leaf3 advertises a prefix into BGP EVPN, it adds it's own AS number, since the update is being sent to eBGP peers (Spine1/Spine2). When Leaf4 gets this, the update is denied because it sees it's own AS - basic BGP loop prevention. </p> <p>However, this doesn't apply to the iBGP peering that is created over the peer-link. This is where the anycast VTEP IP is useful - because the next-hop is owned by the peers, they will drop any BGP NLRI which has this next-hop (due to self next-hop/martian check). This is important because we wouldn't want the MLAG peers to see each other as next hops (over VXLAN) for locally attached hosts. </p> <p>With a simple BGP updates debug, you can confirm that the peers drop this because of the self next-hop check:</p> <pre><code>Leaf4 bgpd[895]: peerlink.4094 rcvd UPDATE w/ attr: nexthop 34.34.34.34, \nlocalpref 100, extcommunity RT:3:10010 RT:3:10040 ET:8 MM:1 Rmac:44:38:39:ff:00:05, path\n\nLeaf4 bgpd[895]: peerlink.4094 rcvd UPDATE about RD 3.3.3.3:2 \n[2]:[00:00:00:00:00:05]/320 label 10010 \nl2vpn evpn -- DENIED due to: martian or self next-hop;\n\nLeaf4 bgpd[895]: peerlink.4094 rcvd UPDATE about RD 3.3.3.3:2 \n[2]:[00:00:00:00:00:05]:[10.10.10.105]/320 label 10010/10040 \nl2vpn evpn -- DENIED due to: martian or self next-hop;\n</code></pre> <p>Remember, even orphan devices are sent with this anycast VTEP address. For example, in our case, Server6 is an orphan device. Leaf4 sends the BGP EVPN update with the anycast VTEP address:</p> <pre><code>cumulus@Leaf4:mgmt:~$ net show bgp l2vpn evpn route rd 4.4.4.4:3 mac 00:00:00:00:00:06 ip 10.10.10.106\nBGP routing table entry for 4.4.4.4:3:[2]:[0]:[48]:[00:00:00:00:00:06]:[32]:[10.10.10.106]\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Leaf3(peerlink.4094) Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:00:00:00:00:06]:[32]:[10.10.10.106] VNI 10010/10040\n  Local\n    34.34.34.34 from 0.0.0.0 (4.4.4.4)\n      Origin IGP, weight 32768, valid, sourced, local, bestpath-from-AS Local, best (First path received)\n      Extended Community: ET:8 RT:4:10010 RT:4:10040 Rmac:44:38:39:ff:00:05\n      Last update: Wed Aug  4 12:17:01 2021\n</code></pre> <p>On remote VTEPs (taking Leaf5, as an example), this is installed with 34.34.34.34 as the next-hop:</p> <pre><code>// BGP table\n\ncumulus@Leaf5:mgmt:~$ net show bgp l2vpn evpn route rd 4.4.4.4:3 mac 00:00:00:00:00:06 ip 10.10.10.106\nBGP routing table entry for 4.4.4.4:3:[2]:[00:00:00:00:00:06]:[10.10.10.106]/352\nPaths: (2 available, best #2)\n  Advertised to non peer-group peers:\n  Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:00:00:00:00:06]:[32]:[10.10.10.106] VNI 10010/10040\n  65550 64523\n    34.34.34.34 from Spine2(swp2) (22.22.22.22)\n      Origin IGP, valid, external\n      Extended Community: RT:4:10010 RT:4:10040 ET:8 Rmac:44:38:39:ff:00:05\n      Last update: Wed Aug  4 12:17:02 2021\n  Route [2]:[0]:[48]:[00:00:00:00:00:06]:[32]:[10.10.10.106] VNI 10010/10040\n  65550 64523\n    34.34.34.34 from Spine1(swp1) (11.11.11.11)\n      Origin IGP, valid, external, bestpath-from-AS 65550, best (Older Path)\n      Extended Community: RT:4:10010 RT:4:10040 ET:8 Rmac:44:38:39:ff:00:05\n      Last update: Wed Aug  4 12:17:02 2021\n\n// RIB table\n\ncumulus@Leaf5:mgmt:~$ net show route vrf VRF1 ipv4 \nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR, f - OpenFabric,\n       &gt; - selected route, * - FIB route, q - queued, r - rejected, b - backup\n       t - trapped, o - offload failure\n\nVRF VRF1:\nK&gt;* 0.0.0.0/0 [255/8192] unreachable (ICMP unreachable), 2d04h24m\nC&gt;* 10.10.10.0/24 is directly connected, vlan10, 2d04h21m\nB&gt;* 10.10.10.105/32 [20/0] via 34.34.34.34, vlan40 onlink, weight 1, 05:58:40\nB&gt;* 10.10.10.106/32 [20/0] via 34.34.34.34, vlan40 onlink, weight 1, 1d07h40m\nC&gt;* 20.20.20.0/24 is directly connected, vlan20, 2d04h21m\n</code></pre> <p>This can cause traffic for this prefix to hash towards Leaf3 (which does not have a direct connection to Server6). Let's take an example of Server3 pinging Server6. </p> <p>Because this is a same subnet ping, Server3 tries to ARP for the destination directly. Leaf5 responds back because it already has an entry for Server6 in it's EVPN ARP cache:</p> <pre><code>cumulus@Leaf5:mgmt:~$ net show evpn arp vni 10010 ip 10.10.10.106\nIP: 10.10.10.106\n Type: remote\n State: active\n MAC: 00:00:00:00:00:06\n Sync-info: -\n Remote VTEP: 34.34.34.34\n Local Seq: 0 Remote Seq: 0 \n</code></pre> <p>Server3 can now send the ICMP request to Leaf5. The destination MAC address is 00:00:00:00:00:06. Leaf5 does a lookup in it's MAC address table, and sends the packet out with VNI 10010, towards a destination of 34.34.34.34 (the anycast VTEP address, owned by both Leaf3 and Leaf4):</p> <pre><code>cumulus@Leaf5:mgmt:~$ net show bridge macs 00:00:00:00:00:06\n\nVLAN      Master  Interface  MAC                TunnelDest   State  Flags               LastSeen\n--------  ------  ---------  -----------------  -----------  -----  ------------------  --------\n10        bridge  vni10      00:00:00:00:00:06                      extern_learn        01:20:27\nuntagged          vni10      00:00:00:00:00:06  34.34.34.34         self, extern_learn  01:20:27\n</code></pre> <p>A packet capture confirms the L2VNI:</p> <p></p> <p>This can be hashed towards Leaf3. Leaf3 simply does a MAC address lookup (for 00:00:00:00:00:06) now, and find's that is it reachable via the peer-link:</p> <pre><code>cumulus@Leaf3:mgmt:~$ net show bridge macs 00:00:00:00:00:06\n\nVLAN  Master  Interface  MAC                TunnelDest  State  Flags  LastSeen\n----  ------  ---------  -----------------  ----------  -----  -----  --------\n  10  bridge  peerlink   00:00:00:00:00:06                            00:00:53\n</code></pre> <p>Thus, visually, the path of the packet in this case would be:</p> <p></p> <p>The next big why - why are type-5 EVPN routes sent with the local VTEP address, instead of the anycast VTEP address? It is quite common to see external prefixes advertised via one of the MLAG peers only, and not both. In such cases, you can potentially black hole traffic by advertising these type-5 prefixes with the anycast VTEP address (because traffic may be ECMP'd to the peer which does not have a route to these external prefixes). Of course, this can be fixed by having per VRF iBGP peering between the MLAG peers but it doesn't scale well and is a lot of administrative overhead.</p> <p>In general, the 'advertise-pip' BGP EVPN option is used for this - the local VTEP IP address is the 'Primary IP'. Cumulus Linux introduced this in their 4.0 release and it is enabled by default. </p> <p>However, it needs to be configured in a particular way for it to work. Prior to 4.0, you could use the 'hwaddress' option to specify a MAC address for an interface. From 4.0 onward, you need to use the 'address-virtual' option to create the common router MAC that is shared between the two MLAG peers. This allows for each MLAG peer to retain it's unique system MAC and share this common router MAC. </p> <p>This change is done under the SVI that maps to the L3VNI:</p> <pre><code>// Leaf3\n\ncumulus@Leaf3:mgmt:~$ net show configuration interface vlan40\ninterface vlan40\n  address-virtual 44:38:39:FF:00:05\n  vlan-id 40\n  vlan-raw-device bridge\n  vrf VRF1\n\n// Leaf4\n\ncumulus@Leaf4:mgmt:~$ net show configuration interface vlan40\ninterface vlan40\n  address-virtual 44:38:39:FF:00:05\n  vlan-id 40\n  vlan-raw-device bridge\n  vrf VRF1\n</code></pre> <p>You should now see the router MAC changed to this common anycast MAC address (general practice is to just set it as the CLAG MAC address), while the system MAC is retained. </p> <pre><code>// Leaf3\n\ncumulus@Leaf3:mgmt:~$ net show bgp l2vpn evpn vni 10040\nVNI: 10040 (known to the kernel)\n  Type: L3\n  Tenant VRF: VRF1\n  RD: 12.12.12.1:3\n  Originator IP: 34.34.34.34\n  Advertise-gw-macip : n/a\n  Advertise-svi-macip : n/a\n  Advertise-pip: Yes\n  System-IP: 3.3.3.3\n  System-MAC: 50:00:00:05:00:05\n  Router-MAC: 44:38:39:ff:00:05\n  Import Route Target:\n    1:10040\n    2:10040\n    4:10040\n    5:10040\n  Export Route Target:\n    3:10040 \n\n// Leaf4\n\ncumulus@Leaf4:mgmt:~$ net show bgp l2vpn evpn vni 10040\nVNI: 10040 (known to the kernel)\n  Type: L3\n  Tenant VRF: VRF1\n  RD: 50.50.50.1:2\n  Originator IP: 34.34.34.34\n  Advertise-gw-macip : n/a\n  Advertise-svi-macip : n/a\n  Advertise-pip: Yes\n  System-IP: 4.4.4.4\n  System-MAC: 50:00:00:06:00:05\n  Router-MAC: 44:38:39:ff:00:05\n  Import Route Target:\n    1:10040\n    2:10040\n    3:10040\n    5:10040\n  Export Route Target:\n    4:10040 \n</code></pre> <p>This gives a lot of good information - it tells you that 'advertise-pip' is enabled, what the system IP is, what the system MAC and the router MAC is. Thus, for type-5 prefixes, the system IP and the system MAC are used, and for type-2 prefix (regardless of the host being an orphan host), the router MAC and the anycast VTEP IP address is used. </p> <p>The type-5 routes are now generated using the system IP and MAC itself. </p> <pre><code>cumulus@Leaf3:mgmt:~$ net show bgp l2vpn evpn route rd 12.12.12.1:3 prefix 99.99.99.99/32\nBGP routing table entry for 12.12.12.1:3:[5]:[0]:[32]:[99.99.99.99]\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Leaf4(peerlink.4094) Spine1(swp1) Spine2(swp2)\n  Route [5]:[0]:[32]:[99.99.99.99] VNI 10040\n  12\n    3.3.3.3 from 0.0.0.0 (3.3.3.3)\n      Origin IGP, metric 0, valid, sourced, local, bestpath-from-AS 12, best (First path received)\n      Extended Community: ET:8 RT:3:10040 Rmac:50:00:00:05:00:05\n      Last update: Tue Aug  3 04:46:12 2021\n</code></pre> <p>This is advertised only with the L3VNI. On other VTEPs, this should be installed in the VRF table:</p> <pre><code>cumulus@Leaf5:mgmt:~$ net show route vrf VRF1 ipv4\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR, f - OpenFabric,\n       &gt; - selected route, * - FIB route, q - queued, r - rejected, b - backup\n       t - trapped, o - offload failure\n\nVRF VRF1:\nK&gt;* 0.0.0.0/0 [255/8192] unreachable (ICMP unreachable), 5d00h05m\nC&gt;* 10.10.10.0/24 is directly connected, vlan10, 5d00h02m\nB&gt;* 10.10.10.105/32 [20/0] via 34.34.34.34, vlan40 onlink, weight 1, 04:16:51\nB&gt;* 10.10.10.106/32 [20/0] via 34.34.34.34, vlan40 onlink, weight 1, 04:16:51\nC&gt;* 20.20.20.0/24 is directly connected, vlan20, 5d00h02m\nB&gt;* 99.99.99.99/32 [20/0] via 3.3.3.3, vlan40 onlink, weight 1, 00:00:01\n</code></pre> <p>Before we wrap this up, it is important to talk about some failure scenarios with MLAG. An important design consideration (and we'll see this more prominently when we talk about Ethernet Segments in EVPN), is that losing a downlink (towards the server itself) has no control-plane implications. There is absolutely no control-plane convergence because of this - it is purely a data plane forwarding change. </p> <p>For example, say Leaf3s interface going to Server5 goes down.  Traffic can still be hashed towards Leaf3, destined for Server5. It would just be forwarded over the peer-link. This is why capacity planning of the peer-link is equally important. </p> <p>A second failure scenario to consider is what would happen if all fabric links go down. So, for example, Leaf3 loses all its spine facing links and the packet from Server5 (destined to Server2) is hashed to Leaf3. </p> <p>This is where the iBGP peering is useful. Prior to this event, the route is received via the spines, and via the iBGP peering to the MLAG peer. </p> <pre><code>cumulus@Leaf3:mgmt:~$ net show bgp l2vpn evpn route rd 5.5.5.5:2 mac 00:00:00:00:00:02 ip 20.20.20.102\nBGP routing table entry for 5.5.5.5:2:[2]:[0]:[48]:[00:00:00:00:00:02]:[32]:[20.20.20.102]\nPaths: (3 available, best #2)\n  Advertised to non peer-group peers:\n  Leaf4(peerlink.4094) Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:00:00:00:00:02]:[32]:[20.20.20.102] VNI 10020/10040\n  65550 64525\n    5.5.5.5 from Leaf4(peerlink.4094) (4.4.4.4)\n      Origin IGP, localpref 100, valid, internal\n      Extended Community: RT:5:10020 RT:5:10040 ET:8 Rmac:00:20:00:20:00:20\n      Last update: Sat Aug  7 08:18:29 2021\n  Route [2]:[0]:[48]:[00:00:00:00:00:02]:[32]:[20.20.20.102] VNI 10020/10040\n  65550 64525\n    5.5.5.5 from Spine1(swp1) (11.11.11.11)\n      Origin IGP, valid, external, bestpath-from-AS 65550, best (Older Path)\n      Extended Community: RT:5:10020 RT:5:10040 ET:8 Rmac:00:20:00:20:00:20\n      Last update: Sat Aug  7 03:51:08 2021\n  Route [2]:[0]:[48]:[00:00:00:00:00:02]:[32]:[20.20.20.102] VNI 10020/10040\n  65550 64525\n    5.5.5.5 from Spine2(swp2) (22.22.22.22)\n      Origin IGP, valid, external\n      Extended Community: RT:5:10020 RT:5:10040 ET:8 Rmac:00:20:00:20:00:20\n      Last update: Sat Aug  7 03:51:08 2021\n</code></pre> <p>After the link down event (and route withdrawals), the traffic is simply routed via the peer-link. If the iBGP peering was missing, traffic would be blackholed on Leaf3.</p> <p>In the next post, we'll look at how Ethernet Segment based EVPN multi-homing acts as an alternative to MLAGs.</p>","tags":["cumulus","vxlan","evpn","mlag"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/","title":"Cumulus Part IX - Understanding VXLAN EVPN Route-Target control","text":"<p>In this post, we look at how route-targets extended communities can be used to control VXLAN BGP EVPN routes in Cumulus Linux.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/#introduction","title":"Introduction","text":"<p>This post assumes that the reader has a general understanding of L2/L3 VNIs and asymmetric/symmetric IRB.</p> <p>Cumulus, by default, uses auto RTs for L2 and L3 VNIs. This makes for a very easy experience (almost plug and play like) when building VXLAN BGP EVPN fabrics. But it also doesn't help you understand much of how route-targets are being imported across and how to completely control this. </p> <p>It's always good to learn to drive a stick, before moving to an automatic. So, the goal of this blog is to help understand how L2/L3 VNI RTs are exported/imported and how you can control what goes into your customer VRF.  This will also allow you to control whether you want asymmetric or symmetric IRB (see my previous Cumulus blog posts to understand what is asymmetric and symmetric IRB).</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/#topology","title":"Topology","text":"<p>For this post, we're going to use the following topology and incrementally add/delete/modify certain aspects of this network. </p> <p></p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/#rts-in-an-l2vni-environment","title":"RTs in an L2VNI environment","text":"<p>To begin with, this is a pure L2 VNI setup with two servers (Server1 and Server2) deployed in VLAN 10, mapped to VNI 10010. The network is a BGP unnumbered core, with loopbacks of each leaf acting as the VXLAN tunnel IPs. </p> <p>Let's review the BGP configuration:</p> <pre><code>router bgp 64521\n  bgp router-id 1.1.1.1\n  neighbor swp1 interface remote-as external\n  neighbor swp2 interface remote-as external\n\n  address-family ipv4 unicast\n    network 1.1.1.1/32 \n\n  address-family l2vpn evpn\n    neighbor swp1 activate\n    neighbor swp2 activate\n    advertise-all-vni\n</code></pre> <p>Typical BGP configuration - we're advertising all VNIs into EVPN and the BGP L2VPN EVPN peering is activated against both Spine1 and Spine2. By default, Cumulus Linux (FRR, really) uses a model of ASN:VNI to derive the VNI RTs. </p> <p>In our case, this will be 64521:10010 for VNI 10010. We can confirm using the following:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net show bgp l2vpn evpn vni 10010\nVNI: 10010 (known to the kernel)\n  Type: L2\n  Tenant-Vrf: default\n  RD: 1.1.1.1:2\n  Originator IP: 1.1.1.1\n  Mcast group: 0.0.0.0\n  Advertise-gw-macip : Disabled\n  Advertise-svi-macip : Disabled\n  Import Route Target:\n    64521:10010\n  Export Route Target:\n    64521:10010\n</code></pre> <p>PC1s mac address is advertised as a type-2 route using this export RT:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net show bgp l2vpn evpn route rd 1.1.1.1:2 mac 00:50:79:66:68:06 \nBGP routing table entry for 1.1.1.1:2:[2]:[00:50:79:66:68:06]/352\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:50:79:66:68:06] VNI 10010\n  Local\n    1.1.1.1 from 0.0.0.0 (1.1.1.1)\n      Origin IGP, weight 32768, valid, sourced, local, bestpath-from-AS Local, best (First path received)\n      Extended Community: ET:8 RT:64521:10010\n      Last update: Sat Jul 24 16:30:57 2021\n</code></pre> <p>This is correctly imported on Leaf2. Cumulus does not show the route imported into the local RD in the BGP table, however, bgpd informs zebra and zebra has installed it in the MAC address table.</p> <pre><code>cumulus@Leaf2:mgmt:~$ net show bgp l2vpn evpn route rd 1.1.1.1:2 mac 00:50:79:66:68:06\nBGP routing table entry for 1.1.1.1:2:[2]:[00:50:79:66:68:06]/352\nPaths: (2 available, best #1)\n  Advertised to non peer-group peers:\n  Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:50:79:66:68:06] VNI 10010\n  65550 64521\n    1.1.1.1 from Spine1(swp1) (11.11.11.11)\n      Origin IGP, valid, external, bestpath-from-AS 65550, best (Router ID)\n      Extended Community: RT:64521:10010 ET:8\n      Last update: Sat Jul 24 16:30:59 2021\n  Route [2]:[0]:[48]:[00:50:79:66:68:06] VNI 10010\n  65550 64521\n    1.1.1.1 from Spine2(swp2) (22.22.22.22)\n      Origin IGP, valid, external\n      Extended Community: RT:64521:10010 ET:8\n      Last update: Sat Jul 24 16:30:59 2021\n</code></pre> <p>The MAC address table also shows this entry, against a remote VTEP of 1.1.1.1, which is Leaf1. </p> <pre><code>cumulus@Leaf2:mgmt:~$ net show bridge macs 00:50:79:66:68:06\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags               LastSeen\n--------  ------  ---------  -----------------  ----------  -----  ------------------  --------\n10        bridge  vni10      00:50:79:66:68:06                     extern_learn        00:00:09\nuntagged          vni10      00:50:79:66:68:06  1.1.1.1            self, extern_learn  00:00:09\n</code></pre>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/#adding-manual-rts","title":"Adding manual RTs","text":"<p>Let's add a manual export RT for VNI 10010, on Leaf1:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net add bgp l2vpn evpn vni 10010 route-target export 1:10010\ncumulus@Leaf1:mgmt:~$ net commit\n</code></pre> <p>This is correctly added to the prefix that is being advertised via BGP EVPN:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net show bgp l2vpn evpn route type 2\nBGP table version is 9, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-1 prefix: [1]:[ESI]:[EthTag]:[IPlen]:[VTEP-IP]:[Frag-id]\nEVPN type-2 prefix: [2]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-4 prefix: [4]:[ESI]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\n                    Extended Community\nRoute Distinguisher: 1.1.1.1:2\n*&gt; [2]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                            32768 i\n                    ET:8 RT:1:10010\n\nDisplayed 1 prefixes (1 paths) (of requested type)\n</code></pre> <p>Leaf2 is still importing this though - why, and how?</p> <pre><code>// BGP EVPN table\n\ncumulus@Leaf2:mgmt:~$ net show bgp l2vpn evpn route type 2 \nBGP table version is 9, local router ID is 2.2.2.2\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-1 prefix: [1]:[ESI]:[EthTag]:[IPlen]:[VTEP-IP]:[Frag-id]\nEVPN type-2 prefix: [2]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-4 prefix: [4]:[ESI]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\n                    Extended Community\nRoute Distinguisher: 1.1.1.1:2\n*&gt; [2]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                                0 65550 64521 i\n                    RT:1:10010 ET:8\n*  [2]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                                0 65550 64521 i\n                    RT:1:10010 ET:8\n\nDisplayed 1 prefixes (2 paths) (of requested type) \n\n// MAC address table\n\ncumulus@Leaf2:mgmt:~$ net show bridge macs 00:50:79:66:68:06\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags               LastSeen\n--------  ------  ---------  -----------------  ----------  -----  ------------------  --------\n10        bridge  vni10      00:50:79:66:68:06                     extern_learn        00:03:14\nuntagged          vni10      00:50:79:66:68:06  1.1.1.1            self, extern_learn  00:03:14\n</code></pre> <p>This is the first important thing to remember with auto-derived RTs on Cumulus Linux - there is an implicit *:VNI import when using auto-RTs. This is necessary because when you follow an eBGP peering model, the AS numbers will naturally be different and a ASN:VNI import model will not work when using your own ASN for the import RT. </p> <p>Let's add a manual, incorrect RT now on Leaf2:</p> <pre><code>cumulus@Leaf2:mgmt:~$ net add bgp l2vpn evpn vni 10010 route-target import 1:10\ncumulus@Leaf2:mgmt:~$ net commit\n</code></pre> <p>We no longer see the entry in the MAC address table anymore, even though BGP EVPN has received it:</p> <pre><code>cumulus@Leaf2:mgmt:~$ net show bridge macs 00:50:79:66:68:06                   \n\nVLAN  Master  Interface  MAC  TunnelDest  State  Flags  LastSeen\n----  ------  ---------  ---  ----------  -----  -----  --------\n* no output *\n\ncumulus@Leaf2:mgmt:~$ net show bgp l2vpn evpn route type 2 \nBGP table version is 11, local router ID is 2.2.2.2\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-1 prefix: [1]:[ESI]:[EthTag]:[IPlen]:[VTEP-IP]:[Frag-id]\nEVPN type-2 prefix: [2]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-4 prefix: [4]:[ESI]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\n                    Extended Community\nRoute Distinguisher: 1.1.1.1:2\n*&gt; [2]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                                0 65550 64521 i\n                    RT:1:10010 ET:8\n*  [2]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                                0 65550 64521 i\n                    RT:1:10010 ET:8\n\nDisplayed 1 prefixes (2 paths) (of requested type) \n</code></pre> <p>Once we add the correct RT to be imported, we see it in the mac table again:</p> <pre><code>cumulus@Leaf2:mgmt:~$ net add bgp l2vpn evpn vni 10010 route-target import 1:10010\ncumulus@Leaf2:mgmt:~$ net commit\n\ncumulus@Leaf2:mgmt:~$ net show bridge macs 00:50:79:66:68:06\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags               LastSeen\n--------  ------  ---------  -----------------  ----------  -----  ------------------  --------\n10        bridge  vni10      00:50:79:66:68:06                     extern_learn        00:00:17\nuntagged          vni10      00:50:79:66:68:06  1.1.1.1            self, extern_learn  00:00:17 \n</code></pre> <p>Remember, this import RT also controls what is inserted into the EVPN ARP cache. Assuming corresponding SVIs were deployed as well, you should see the EVPN ARP cache populated with this entry if the correct import RTs are configured (via the type-2 MAC plus IP route).</p> <pre><code>cumulus@Leaf2:mgmt:~$ net show evpn arp-cache vni 10010\nNumber of ARPs (local and remote) known for this VNI: 2\nFlags: I=local-inactive, P=peer-active, X=peer-proxy\nNeighbor        Type   Flags State    MAC               Remote ES/VTEP                 Seq #'s\n10.10.10.101    remote       active   00:50:79:66:68:06 1.1.1.1                        0/0\n10.10.10.102    local        active   00:50:79:66:68:07                                0/0  \n</code></pre>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/#rts-with-symmetric-irb","title":"RTs with symmetric IRB","text":"<p>Let's now convert our network to a symmetric IRB topology. Server2 is moved to VLAN 20, with an IP address of 20.20.20.102. VLANs 10 and 20 are present on both Leaf1 and Leaf2, acting as anycast gateways for their respective subnets. </p> <p></p> <p>Both Leaf1 and Leaf2 have VNIs created for VLANs 10 and 20. Example below from Leaf1:</p> <pre><code>interface vni10\n  bridge-access 10\n  mstpctl-bpduguard yes\n  mstpctl-portbpdufilter yes\n  vxlan-id 10010\n  vxlan-local-tunnelip 1.1.1.1\n\ninterface vni20\n  bridge-access 20\n  mstpctl-bpduguard yes\n  mstpctl-portbpdufilter yes\n  vxlan-id 10020\n  vxlan-local-tunnelip 1.1.1.1\n</code></pre> <p>A L3VNI (VNI 10040) is created for symmetric routing and mapped to VLAN 40. Each of the servers are moved into a new VRF, called VRF1. The L3VNI is mapped to this VRF as well. </p> <pre><code>interface vlan10\n  address 10.10.10.1/24\n  hwaddress 00:10:00:10:00:10\n  vlan-id 10\n  vlan-raw-device bridge\n  vrf VRF1\n\ninterface vlan20\n  address 20.20.20.1/24\n  hwaddress 00:20:00:20:00:20\n  vlan-id 20\n  vlan-raw-device bridge\n  vrf VRF1\n\ninterface vlan40\n  vlan-id 40\n  vlan-raw-device bridge\n  vrf VRF1\n</code></pre> <p>The same import RT logic applies for L3VNIs also - if there's no manual import configured for the L3VNI, then the default *:VNI import is applied. It is crucial to understand how the import/export RTs for the L3VNI is controlled - this is done under the VRF specific address-family in BGP. </p> <p>For example, before setting any manual import/export RTs, the L3VNI has auto-derived it:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net show bgp l2vpn evpn vni 10040\nVNI: 10040 (known to the kernel)\n  Type: L3\n  Tenant VRF: VRF1\n  RD: 20.20.20.1:3\n  Originator IP: 1.1.1.1\n  Advertise-gw-macip : n/a\n  Advertise-svi-macip : n/a\n  Advertise-pip: Yes\n  System-IP: 1.1.1.1\n  System-MAC: 50:00:00:03:00:03\n  Router-MAC: 50:00:00:03:00:03\n  Import Route Target:\n    64521:10040\n  Export Route Target:\n    64521:10040 \n</code></pre> <p>We'll now configure this manually instead, as an example, on Leaf1. This goes under the BGP configuration itself:</p> <pre><code>net add bgp vrf VRF1 autonomous-system 64521\nnet add bgp vrf VRF1 l2vpn evpn route-target import 2:10040\nnet add bgp vrf VRF1 l2vpn evpn route-target export 1:10040\n</code></pre> <p>Notice how these are specific to the VRF. Now, Leaf1 should be adding a RT of 1:10040 to the prefixes.  The final BGP configuration in this case:</p> <pre><code>router bgp 64521\n  bgp router-id 1.1.1.1\n  neighbor swp1 interface remote-as external\n  neighbor swp2 interface remote-as external\n\n  address-family ipv4 unicast\n    network 1.1.1.1/32 \n\n  address-family l2vpn evpn\n    neighbor swp1 activate\n    neighbor swp2 activate\n    advertise-all-vni\n\n    vni 10020\n      route-target import 2:10\n      route-target export 2:10\n\n    vni 10010\n      route-target import 1:10\n      route-target export 1:10\n\nrouter bgp 64521 vrf VRF1\n\n  address-family l2vpn evpn\n    route-target export 1:10040\n    route-target import 2:10040\n</code></pre> <p>Looking at the BGP EVPN table, we can see the RTs are correctly added:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net show bgp l2vpn evpn route rd 1.1.1.1:3 type 2\nEVPN type-1 prefix: [1]:[ESI]:[EthTag]:[IPlen]:[VTEP-IP]:[Frag-id]\nEVPN type-2 prefix: [2]:[EthTag]:[MAClen]:[MAC]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-4 prefix: [4]:[ESI]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[EthTag]:[IPlen]:[IP]\n\nBGP routing table entry for 1.1.1.1:3:[2]:[00:50:79:66:68:06]/352\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:50:79:66:68:06] VNI 10010/10040\n  Local\n    1.1.1.1 from 0.0.0.0 (1.1.1.1)\n      Origin IGP, weight 32768, valid, sourced, local, bestpath-from-AS Local, best (First path received)\n      Extended Community: ET:8 RT:1:10 RT:1:10040 Rmac:50:00:00:03:00:03\n      Last update: Fri Jul 30 09:43:56 2021\nBGP routing table entry for 1.1.1.1:3:[2]:[00:50:79:66:68:06]:[10.10.10.101]/352\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.10.10.101] VNI 10010/10040\n  Local\n    1.1.1.1 from 0.0.0.0 (1.1.1.1)\n      Origin IGP, weight 32768, valid, sourced, local, bestpath-from-AS Local, best (First path received)\n      Extended Community: ET:8 RT:1:10 RT:1:10040 Rmac:50:00:00:03:00:03\n      Last update: Fri Jul 30 09:43:56 2021\n\nDisplayed 2 prefixes (2 paths) with this RD (of requested type) \n</code></pre> <p>There are two distinct RTs added here - one for the corresponding L2VNI and another for the L3VNI. </p> <p>It is important to understand the impact of importing each RT - on Leaf2, importing the RT for the L3VNI is what imports the /32 route (from the type-2 MAC plus IP route) into the VRF routing table, while importing the L2VNI will pull the MAC address into the MAC address table (this is done using the type-2 MAC only route) and create an entry in the EVPN ARP cache (using the type-2 MAC plus IP route).</p> <p>Let's confirm on Leaf2:</p> <pre><code>// MAC address table\n\ncumulus@Leaf2:mgmt:~$ net show bridge macs 00:50:79:66:68:06\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags               LastSeen\n--------  ------  ---------  -----------------  ----------  -----  ------------------  --------\n10        bridge  vni10      00:50:79:66:68:06                     extern_learn        00:00:43\nuntagged          vni10      00:50:79:66:68:06  1.1.1.1            self, extern_learn  00:00:43\n\n// EVPN ARP cache\n\ncumulus@Leaf2:mgmt:~$ net show evpn arp-cache vni 10010     \nNumber of ARPs (local and remote) known for this VNI: 1\nFlags: I=local-inactive, P=peer-active, X=peer-proxy\nNeighbor        Type   Flags State    MAC               Remote ES/VTEP                 Seq #'s\n10.10.10.101    remote       active   00:50:79:66:68:06 1.1.1.1                        0/0 \n\n// VRF1 route table\n\ncumulus@Leaf2:mgmt:~$ net show route vrf VRF1 ipv4          \nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR, f - OpenFabric,\n       &gt; - selected route, * - FIB route, q - queued, r - rejected, b - backup\n       t - trapped, o - offload failure\n\nVRF VRF1:\nK&gt;* 0.0.0.0/0 [255/8192] unreachable (ICMP unreachable), 01:46:41\nC&gt;* 10.10.10.0/24 is directly connected, vlan10, 01:46:41\nB&gt;* 10.10.10.101/32 [20/0] via 1.1.1.1, vlan40 onlink, weight 1, 00:21:36\nC&gt;* 20.20.20.0/24 is directly connected, vlan20, 01:46:41 \n</code></pre> <p>On Leaf2, as you can see, we have enough information to route both asymmetrically and symmetrically. Remember, the lookup is always a longest prefix match - which means the /32 route is hit and the packet is routed symmetrically. </p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/#controlling-rts","title":"Controlling RTs","text":"<p>Knowing what we know of RT import/export, we can fully control how we want our traffic to flow (asymmetric or symmetric).</p> <p>On Leaf1, let's import a different RT under the VRF address-family.</p> <pre><code>router bgp 64521\n  bgp router-id 1.1.1.1\n  neighbor swp1 interface remote-as external\n  neighbor swp2 interface remote-as external\n\n  address-family ipv4 unicast\n    network 1.1.1.1/32 \n\n  address-family l2vpn evpn\n    neighbor swp1 activate\n    neighbor swp2 activate\n    advertise-all-vni\n\n    vni 10020\n      route-target import 2:10\n      route-target export 2:10\n\n    vni 10010\n      route-target import 1:10\n      route-target export 1:10\n\nrouter bgp 64521 vrf VRF1\n\n  address-family l2vpn evpn\n    route-target export 1:10040\n    route-target import 2:10041 \n</code></pre> <p>This causes the type-2 MAC plus IP route to not be imported as a /32 route in the VRF table. Now, the longest prefix match is the subnet route itself:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net show route vrf VRF1 ipv4\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR, f - OpenFabric,\n       &gt; - selected route, * - FIB route, q - queued, r - rejected, b - backup\n       t - trapped, o - offload failure\n\nVRF VRF1:\nK&gt;* 0.0.0.0/0 [255/8192] unreachable (ICMP unreachable), 07:27:56\nC&gt;* 10.10.10.0/24 is directly connected, vlan10, 07:27:56\nC&gt;* 20.20.20.0/24 is directly connected, vlan20, 07:27:56 \n</code></pre> <p>This means that the destination is directly connected to Leaf1 and it can ARP for it. Using the EVPN ARP cache, Leaf1 already knows PC2s mac address, and there's no need to ARP for it again. Thus, PC1 to PC2, traffic will flow asymmetrically. A packet capture confirms that the VNI added to the VXLAN header is 10020.</p> <p></p> <p>The return path is symmetric because Leaf2 still has that /32 entry imported into the VRF table. A packet capture confirms that the return packet has the L3VNI (10040) added to the VXLAN header. </p> <p></p> <p>I hope this was informative, and I'll see you in the next one.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/archive/2021/","title":"December 2021","text":""},{"location":"blog/category/cumulus/","title":"cumulus","text":""},{"location":"blog/category/vxlan/","title":"vxlan","text":""},{"location":"blog/category/evpn/","title":"evpn","text":""},{"location":"blog/category/mlag/","title":"mlag","text":""},{"location":"blog/category/bgp/","title":"bgp","text":""}]}