{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to The ASCII Construct","text":"<p>My name is Aninda Chatterjee and I am a network engineer. I run this site (read: blog) to share my experiences across various networking domains that I have worked in, including a wide range of technologies and solutions such as Cisco SD-Access, EVPN VXLAN data centers, network infrastructure for AI clusters.</p> <p>I am a published author, having written the book Deploying Juniper Data Centers with EVPN VXLAN, which can be bought direct from Pearson here or from the US Amazon store here. I am in the process of writing a second book for Pearson on mastering advanved Juniper data center deployments.</p> <p>You can also support the work on this website and its upkeep with buy me a coffee:</p> <p></p> <p>Site Navigation</p> <p>While this site mostly functions as a peronal technical blog, I have also curated a list of reference learning material that I use most often, which, in general, should as excellent resources for other engineers. In addition to this, there are labs under construction as well, deployed using containerlab, predominatenly to help learn data center technologies and solutions. Blog Learning references Labs</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2021/12/05/cumulus-basics-part-i---navigating-the-os/","title":"Cumulus Basics Part I - navigating the OS","text":"<p>This first blog on Cumulus introduces the reader to the basics of the operating system and Cumulus' NCLU.</p>","tags":["cumulus"]},{"location":"blog/2021/12/05/cumulus-basics-part-i---navigating-the-os/#introduction","title":"Introduction","text":"<p>This is going to be a new mini series that we will do in preparation for the first open networking certification that Cumulus Networks introduced,see here. </p> <p>Cumulus Networks has a great page (both free and paid content) where you can spend some time and learn all things Cumulus and open networking related, see here.</p> <p>So, where do we begin? Every time I learn a new product, I start at the start. Things we learned way back when. Because Cumulus Linux is a native Linux distribution (and it's interface may be unfamiliar to many), we'll start with some very simple aspects of working with the box - basic port bring up/down, port configurations, gathering information about a port and finally an introduction to Cumulus' NCLU! </p>","tags":["cumulus"]},{"location":"blog/2021/12/05/cumulus-basics-part-i---navigating-the-os/#topology","title":"Topology","text":"<p>As a reference, we'll be working on the following topology:</p> <p></p>","tags":["cumulus"]},{"location":"blog/2021/12/05/cumulus-basics-part-i---navigating-the-os/#basic-interface-configuration","title":"Basic interface configuration","text":"<p>With Linux networking, your interface configurations would be found in /etc/network/interfaces (there are several helpful pages that you can google and find to understand the syntax in this file so we're not going to go over that in too much detail). You can quickly view this via the 'cat' option.</p> <p>On both boxes, with a blank configuration, we see:</p> <pre><code>cumulus@cumulus:~$ cat /etc/network/interfaces\n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*.intf\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto eth0\niface eth0 inet dhcp\n</code></pre> <p>This file can directly be modified to change the configuration of various interface or to introduce new logical interfaces. To demonstrate this, let's go ahead and configure swp1 on each box to be a L3 interface with an IP address in the subnet 10.0.0.0/24. </p> <p>SW1:</p> <pre><code>cumulus@cumulus:/$ cat /etc/network/interfaces\n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*.intf\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto eth0\niface eth0 inet dhcp\n\nauto swp1\niface swp1\n    address 10.0.0.1/24\n</code></pre> <p>SW2:</p> <pre><code>cumulus@cumulus:/$ cat /etc/network/interfaces\n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*.intf\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto eth0\niface eth0 inet dhcp\n\nauto swp1\niface swp1\n    address 10.0.0.2/24\n</code></pre> <p>Bring these interfaces up using the 'ifup' option of the ifupdown2 module on both the switches.</p> <pre><code>cumulus@cumulus:/$ sudo ifup swp1\n</code></pre> <p>To confirm the status of the interface, you can use 'ip link show' to list all interfaces or specify a particular interface to look at using 'ip link show swp1'.</p> <pre><code>cumulus@cumulus:~$ ip link show\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:00 brd ff:ff:ff:ff:ff:ff\n3: swp1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:01 brd ff:ff:ff:ff:ff:ff\n4: swp2: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:02 brd ff:ff:ff:ff:ff:ff\n5: swp3: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:03 brd ff:ff:ff:ff:ff:ff\n6: swp4: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:04 brd ff:ff:ff:ff:ff:ff\n7: swp5: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:05 brd ff:ff:ff:ff:ff:ff\n8: swp6: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:06 brd ff:ff:ff:ff:ff:ff\n</code></pre> <p>Looking at only swp1:</p> <pre><code>cumulus@cumulus:~$ ip link show swp1\n3: swp1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000\n    link/ether 0c:db:6b:cc:20:01 brd ff:ff:ff:ff:ff:ff\n</code></pre> <p>An interesting thing to note here - even if you shut down one side of the link (say, do a 'sudo ifdown swp1' on SPINE1), the other side would still show that the link is up. This behavior is specific to Cumulus VX in virtualized environments only and was confirmed by Cumulus folks (this is not just a Cumulus issue - this behavior is typical of virtual routers/switches and is seen across vendors).</p> <p>\"When using VBox there is a little switch in the middle of the link that holds it up.\" </p> <p>You can poke this middle switch to bring the links down if you're specifically testing link failures. But we're not going to get into that here. </p> <p>Clearly, manipulating these network configuration files can be a little tiresome and more importantly, prone to human error. You need to be aware of the kind of syntax that is used within these files and all of the different intricacies that go into the configuration here.</p>","tags":["cumulus"]},{"location":"blog/2021/12/05/cumulus-basics-part-i---navigating-the-os/#cumulus-nclu-introduction","title":"Cumulus NCLU introduction","text":"<p>This is where Cumulus' NCLU comes in. NCLU (Network Command Line Utility) is essentially a CLI that takes you away from manual manipulation of network files and provides a helpful CLI for the same instead. </p> <p>All NCLU commands start with 'net'. You can tab or use a question mark to get all the available options. </p> <pre><code>cumulus@SPINE1:~$ net \n    abort     :  abandon changes in the commit buffer\n    add       :  add/modify configuration\n    clear     :  clear counters, BGP neighbors, etc\n    commit    :  apply the commit buffer to the system\n    del       :  remove configuration\n    example   :  detailed examples of common workflows\n    help      :  context sensitive information; see section below\n    pending   :  show changes staged in the commit buffer\n    rollback  :  revert to a previous configuration state\n    show      :  show command output\n</code></pre> <p>To add configuration to an interface, you can use the 'net add interface' CLI. For example, instead of manipulating the /etc/network/interfaces file directly to add an IP address to swp1, I can do this instead:</p> <pre><code>cumulus@cumulus:~$ net add interface swp1 ip address 10.0.0.1/24\n</code></pre> <p>NCLU has three steps to it:</p> <ol> <li>Configure using 'net add [del]' commands.</li> <li>Confirm what is going to be configured using 'net pending'.</li> <li>Push this configuration using 'net commit'.</li> </ol> <p>A complete example of configuring an IP address on swp1 follows:</p> <pre><code>// net add\n\ncumulus@SPINE1:/$ net add interface swp1 ip address 10.0.0.1/24\n\n// net pending\n\ncumulus@SPINE1:~$ net pending\n--- /etc/network/interfaces     2019-03-05 16:40:58.268594116 +0000\n+++ /run/nclu/ifupdown2/interfaces.tmp  2019-03-05 17:23:54.585798862 +0000\n@@ -6,14 +6,12 @@\n # The loopback network interface\n auto lo\n iface lo inet loopback\n\n # The primary network interface\n auto eth0\n iface eth0 inet dhcp\n\n auto swp1\n iface swp1\n-       address 10.1.1.1/24\n-\n-\n-\n+    address 10.0.0.1/24\n+    address 10.1.1.1/24\n\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  ---------------------------------------------\ncumulus  2019-03-05 17:23:51.321383  net add interface swp1 ip address 10.0.0.1/24\n\n// net commit\n\ncumulus@SPINE1:~$ net commit\n--- /etc/network/interfaces     2019-03-05 16:40:58.268594116 +0000\n+++ /run/nclu/ifupdown2/interfaces.tmp  2019-03-05 17:24:00.025799294 +0000\n@@ -6,14 +6,12 @@\n # The loopback network interface\n auto lo\n iface lo inet loopback\n\n # The primary network interface\n auto eth0\n iface eth0 inet dhcp\n\n auto swp1\n iface swp1\n-       address 10.1.1.1/24\n-\n-\n-\n+    address 10.0.0.1/24\n+    address 10.1.1.1/24\n\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  ---------------------------------------------\ncumulus  2019-03-05 17:23:51.321383  net add interface swp1 ip address 10.0.0.1/24\n</code></pre> <p>Note</p> <p>From a link logging perspective, this is done via rsyslog. The defaults can be found in '/etc/rsyslog.d'. 22-linkstate.conf contains information on where link state changes are logged. By default, this goes to /var/log/linkstate. However, with Cumulus VX, you'd soon realize that there is no 'linkstate' file in /var/log. This is because switchd is responsible for this and switchd doesn't do much in Cumulus VX (as it is largely just an interface for the ASIC). Due to this, you cannot track link state changes in Cumulus VX. Shoutout to Eric Pulvino for clarifying this.  </p> <p>We will continue with bridging on Cumulus VX in part II. Each post in this series will show both NCLU configurations and manual changes needed to the relevant network files.</p>","tags":["cumulus"]},{"location":"blog/2021/12/07/cumulus-part-ii---bridging/","title":"Cumulus Part II - Bridging","text":"<p>This second blog on Cumulus looks at basic layer2 functionality in Cumulus Linux.</p> <p>This post is going to introduce you to basic Layer2 functionality on the Cumulus platform. Like before, we are going to be working with Cumulus VX. </p>","tags":["cumulus"]},{"location":"blog/2021/12/07/cumulus-part-ii---bridging/#topology","title":"Topology","text":"<p>We will be using the following network topology for this post:</p> <p></p> <p>PC1 and PC2 are two end clients in the same Layer2 domain (VLAN 10) that want to communicate with each other.</p>","tags":["cumulus"]},{"location":"blog/2021/12/07/cumulus-part-ii---bridging/#bridges-in-cumulus-linux","title":"Bridges in Cumulus Linux","text":"<p>Cumulus uses the traditional concept of a Linux bridge for bridging. It also allows you to configure a VLAN-aware bridge which is what we will be using today (more documentation about this can be found here. </p> <p>A 'bridge' interface will be created when the first VLAN is created on the system. So, let's start by creating a VLAN using Cumulus' NCLU and looking at the relevant changes that will be made to the /etc/network/interfaces file:</p> <pre><code>// create a VLAN\n\ncumulus@SPINE1:~$ net add vlan 10 \n\n// view changes that will be made and to what file using 'net pending'\n\ncumulus@SPINE1:~$ net pending\n--- /etc/network/interfaces     2018-11-13 18:46:52.000000000 +0000\n+++ /run/nclu/ifupdown2/interfaces.tmp  2019-03-25 15:05:05.712745615 +0000\n@@ -3,10 +3,15 @@\n\n source /etc/network/interfaces.d/*.intf\n\n # The loopback network interface\n auto lo\n iface lo inet loopback\n\n # The primary network interface\n auto eth0\n iface eth0 inet dhcp\n+\n+auto bridge\n+iface bridge\n+    bridge-vids 10\n+    bridge-vlan-aware yes\n\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  ---------------\ncumulus  2019-03-25 15:04:49.012290  net add vlan 10\n</code></pre> <p>From the above, you can see that a 'bridge' interface is created by adding 'iface bridge' and under this, you specify what VLANs are part of this bridge using 'bridge-vids &lt;&gt;'. The 'bridge-vlan-aware yes' option under the bridge makes this a VLAN-aware bridge as opposed to a traditional Linux bridge. </p> <p>Do the same thing on SPINE2 as well. </p> <p>Now, let's start mapping these VLANs to our interfaces. We want swp3 on SPINE1 and swp4 on SPINE2 to be untagged, host facing interfaces in VLAN 10 while the interconnection between SPINE1 and SPINE2 (via swp1) needs to allow multiple VLANs, making it a trunk. </p> <pre><code>// map bridge VLANs to interfaces\n\ncumulus@SPINE1:~$ net add interface swp3 bridge access 10 \ncumulus@SPINE1:~$ net add interface swp1 bridge trunk vlan 10-20\n\n// view changes that will be made and to what file using 'net pending'\n\ncumulus@SPINE1:~$ net pending\n--- /etc/network/interfaces     2018-11-13 18:46:52.000000000 +0000\n+++ /run/nclu/ifupdown2/interfaces.tmp  2019-03-25 15:08:04.817750296 +0000\n@@ -3,10 +3,24 @@\n\n source /etc/network/interfaces.d/*.intf\n\n # The loopback network interface\n auto lo\n iface lo inet loopback\n\n # The primary network interface\n auto eth0\n iface eth0 inet dhcp\n+\n+auto swp1\n+iface swp1\n+    bridge-vids 10-20\n+\n+auto swp3\n+iface swp3\n+    bridge-access 10\n+\n+auto bridge\n+iface bridge\n+    bridge-ports swp1 swp3\n+    bridge-vids 10-20\n+    bridge-vlan-aware yes\n\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  -----------------------------------------------\ncumulus  2019-03-25 15:04:49.012290  net add vlan 10\ncumulus  2019-03-25 15:07:10.463736  net add interface swp3 bridge access 10\ncumulus  2019-03-25 15:08:01.690646  net add interface swp1 bridge trunk vlans 10-20\n</code></pre> <p>Notice how under the bridge interface, there is an interface mapping that is created using 'bridge-ports'. This tells the bridge what interfaces are part of it.</p> <p>Similar configuration needs to be done on SPINE2. Finally, we need to do a 'net commit' to commit these changes. </p> <p>At this point, PC1 can successfully ping PC2:</p> <pre><code>PC-1&gt; ping 10.1.1.2\n84 bytes from 10.1.1.2 icmp_seq=1 ttl=64 time=3.108 ms\n84 bytes from 10.1.1.2 icmp_seq=2 ttl=64 time=2.412 ms\n84 bytes from 10.1.1.2 icmp_seq=3 ttl=64 time=1.585 ms\n\n*snip*\n</code></pre>","tags":["cumulus"]},{"location":"blog/2021/12/07/cumulus-part-ii---bridging/#final-configuration","title":"Final configuration","text":"<p>Let's take a final look at the configuration of both SPINE1 and SPINE2 that was needed to make this work:</p> <pre><code>SPINE1:\n\ncumulus@SPINE1:~$ cat /etc/network/interfaces\n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*.intf\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto eth0\niface eth0 inet dhcp\n\nauto swp1\niface swp1\n    bridge-vids 10-20\n\nauto swp3\niface swp3\n    bridge-access 10\n\nauto bridge\niface bridge\n    bridge-ports swp1 swp3\n    bridge-vids 10-20\n    bridge-vlan-aware yes\n\nSPINE2:\n\ncumulus@SPINE2:~$ cat /etc/network/interfaces\n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*.intf\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto eth0\niface eth0 inet dhcp\n\nauto swp1\niface swp1\n    bridge-vids 10-20\n\nauto swp4\niface swp4\n    bridge-access 10\n\nauto bridge\niface bridge\n    bridge-ports swp1 swp4\n    bridge-vids 10-20\n    bridge-vlan-aware yes\n</code></pre> <p>I hope this was informative. See y'all in the next post!</p>","tags":["cumulus"]},{"location":"blog/2021/12/09/cumulus-basics-part-iv---bgp-introduction/","title":"Cumulus Basics Part IV - BGP introduction","text":"<p>In this post, we introduce BGP on Cumulus Linux.</p>","tags":["cumulus","bgp"]},{"location":"blog/2021/12/09/cumulus-basics-part-iv---bgp-introduction/#introduction","title":"Introduction","text":"<p>The goal of this post is to introduce BGP on Cumulus Linux and then move towards a BGP unnumbered design, in the following post. </p>","tags":["cumulus","bgp"]},{"location":"blog/2021/12/09/cumulus-basics-part-iv---bgp-introduction/#topology","title":"Topology","text":"<p>We'll be using the following network topology for this post:</p> <p></p> <p>First, we will try to create a traditional BGP scenario with OSPF as an IGP. For now, OSPF is up and running and I have learnt the loopback of each LEAF switch.</p> <pre><code>// RIB lookup on LEAF1 for LEAF2s loopback\n\ncumulus@LEAF1:~$ net show route 2.2.2.2\nRIB entry for 2.2.2.2\n=====================\nRouting entry for 2.2.2.2/32\n  Known via \"ospf\", distance 110, metric 200, best\n  Last update 00:02:22 ago\n  * 172.16.11.11, via swp1\n  * 172.16.12.22, via swp2\n\n\nFIB entry for 2.2.2.2\n=====================\n2.2.2.2  proto ospf  metric 20 \n        nexthop via 172.16.11.11  dev swp1 weight 1\n        nexthop via 172.16.12.22  dev swp2 weight 1 \n\n// RIB lookup on LEAF1 for LEAF3s loopback\n\ncumulus@LEAF1:~$ net show route 3.3.3.3\nRIB entry for 3.3.3.3\n=====================\nRouting entry for 3.3.3.3/32\n  Known via \"ospf\", distance 110, metric 200, best\n  Last update 00:02:54 ago\n  * 172.16.11.11, via swp1\n  * 172.16.12.22, via swp2\n\n\nFIB entry for 3.3.3.3\n=====================\n3.3.3.3  proto ospf  metric 20 \n        nexthop via 172.16.11.11  dev swp1 weight 1\n        nexthop via 172.16.12.22  dev swp2 weight 1 \n</code></pre> <p>I can use these loopbacks now to form an iBGP session between LEAF1 and LEAF2 to provide connectivity between PC1 and PC2.</p> <pre><code>// LEAF1 configuration\n\ncumulus@LEAF1:~$ net add bgp autonomous-system 1\ncumulus@LEAF1:~$ net add bgp router-id 1.1.1.1  \ncumulus@LEAF1:~$ net add bgp neighbor 2.2.2.2 remote-as 1\ncumulus@LEAF1:~$ net add bgp neighbor 2.2.2.2 update-source lo\ncumulus@LEAF1:~$ net commit \n\n// LEAF2 configuration\n\ncumulus@LEAF2:~$ net add bgp autonomous-system 1\ncumulus@LEAF2:~$ net add bgp router-id 2.2.2.2\ncumulus@LEAF2:~$ net add bgp neighbor 1.1.1.1 remote-as 1\ncumulus@LEAF2:~$ net add bgp neighbor 1.1.1.1 update-source lo\ncumulus@LEAF2:~$ net commit  \n</code></pre> <p>An iBGP session is now up between the two:</p> <pre><code>cumulus@LEAF1:~$ net show bgp ipv4 unicast summary \nBGP router identifier 1.1.1.1, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 1, using 19 KiB of memory\n\nNeighbor        V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLEAF2(2.2.2.2)  4          1      21      21        0    0    0 00:00:56            0\n</code></pre> <p>Advertise the host subnets now into BGP:</p> <pre><code>cumulus@LEAF1:~$ net add bgp network 10.1.1.0/24\ncumulus@LEAF1:~$ net commit \n\ncumulus@LEAF2:~$ net add bgp network 20.1.1.0/24\ncumulus@LEAF2:~$ net commit    \n\n// IPv4 unicast BGP table on LEAF1\n\ncumulus@LEAF1:~$ net show bgp ipv4 unicast \nBGP table version is 2, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, = multipath,\n              i internal, r RIB-failure, S Stale, R Removed\nOrigin codes: i - IGP, e - EGP, ? - incomplete\n\n   Network          Next Hop            Metric LocPrf Weight Path\n*&gt; 10.1.1.0/24      0.0.0.0                  0         32768 i\n*&gt;i20.1.1.0/24      2.2.2.2                  0    100      0 i\n\nDisplayed  2 routes and 2 total paths \n\n// IPv4 unicast BGP table on LEAF2\n\ncumulus@LEAF2:~$ net show bgp ipv4 unicast                  \nBGP table version is 2, local router ID is 2.2.2.2\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, = multipath,\n              i internal, r RIB-failure, S Stale, R Removed\nOrigin codes: i - IGP, e - EGP, ? - incomplete\n\n   Network          Next Hop            Metric LocPrf Weight Path\n*&gt;i10.1.1.0/24      1.1.1.1                  0    100      0 i\n*&gt; 20.1.1.0/24      0.0.0.0                  0         32768 i\n</code></pre> <p>Let's try and ping from PC1 to PC2 now:</p> <pre><code>PC1&gt; ping 20.1.1.1\n\n20.1.1.1 icmp_seq=1 timeout\n20.1.1.1 icmp_seq=2 timeout\n20.1.1.1 icmp_seq=3 timeout\n20.1.1.1 icmp_seq=4 timeout\n20.1.1.1 icmp_seq=5 timeout\n</code></pre> <p>All pings timeout. This is a common problem that you may run into -  look at what is happening here with more thought. PC1 sources a packet with 10.1.1.1 with an IP destination of 20.1.1.1, with an ICMP header trailing it. This reaches the default gateway, LEAF1. LEAF1 does a mac lookup, realizes it owns the destination mac and thus moves into the IP header to do a RIB lookup and forwards it towards SPINE1 (assuming the packet hash is in such a way that it goes to SPINE1).</p> <pre><code>// RIB lookup on LEAF1 for 20.1.1.1\n\ncumulus@LEAF1:~$ net show route 20.1.1.1\nRIB entry for 20.1.1.1\n======================\nRouting entry for 20.1.1.0/24\n  Known via \"bgp\", distance 200, metric 0, best\n  Last update 00:03:17 ago\n    2.2.2.2 (recursive)\n  *   172.16.11.11, via swp1\n  *   172.16.12.22, via swp2\n\n\nFIB entry for 20.1.1.1\n======================\n20.1.1.0/24  proto bgp  metric 20 \n        nexthop via 172.16.11.11  dev swp1 weight 1\n        nexthop via 172.16.12.22  dev swp2 weight 1 \n\nThe packet can be sent towards SPINE1/SPINE2. Let's take SPINE1 as the next hop as an example here. Does SPINE1 know how to reach 20.1.1.1? \n\n// RIB lookup on SPINE1 for 20.1.1.1\n\ncumulus@SPINE1:~$ net show route 20.1.1.1\nRIB entry for 20.1.1.1\n======================\n% Network not in table \n</code></pre> <p>SPINE1 has no entry for this prefix and thus your packets get blackholed here. To get around this problem, you either redistribute your BGP table into your IGP (which doesn't make sense considering your BGP table might grow substantially) or you have some sort of a meshed iBGP peering to ensure all boxes receive this route. The cleanest way of doing this would be with route reflectors. So, let's go ahead and make SPINE1/SPINE2 as RRs and have LEAF1/2/3 peer with them as route reflector clients.</p> <p>I have now modified the configuration appropriately:</p> <pre><code>// IPv4 unicast BGP table on LEAF1\n\ncumulus@LEAF1:~$ net show bgp ipv4 unicast summary \nBGP router identifier 1.1.1.1, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 2, using 39 KiB of memory\n\nNeighbor            V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nSPINE1(11.11.11.11) 4          1      22      22        0    0    0 00:00:57            0\nSPINE2(22.22.22.22) 4          1       9       9        0    0    0 00:00:20            0\n\nTotal number of neighbors 2 \n\n// IPv4 unicast BGP table on LEAF2\n\ncumulus@LEAF2:~$ net show bgp ipv4 unicast summary \nBGP router identifier 2.2.2.2, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 2, using 39 KiB of memory\n\nNeighbor            V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nSPINE1(11.11.11.11) 4          1      13      13        0    0    0 00:00:30            0\nSPINE2(22.22.22.22) 4          1      23      23        0    0    0 00:01:01            0\n\nTotal number of neighbors 2 \n</code></pre> <p>Go back in and advertise the host subnets again and now we see the intermediate devices (SPINE1/SPINE2) also having the prefixes in RIB.</p> <pre><code>cumulus@SPINE1:~$ net show route 20.1.1.1\nRIB entry for 20.1.1.1\n======================\nRouting entry for 20.1.1.0/24\n  Known via \"bgp\", distance 200, metric 0, best\n  Last update 00:00:05 ago\n    2.2.2.2 (recursive)\n  *   172.16.21.2, via swp2\n\n\nFIB entry for 20.1.1.1\n======================\n20.1.1.0/24 via 172.16.21.2 dev swp2  proto bgp  metric 20\n</code></pre> <p>PC1 can now reach PC2:</p> <pre><code>PC1&gt; ping 20.1.1.1\n\n84 bytes from 20.1.1.1 icmp_seq=1 ttl=61 time=2.354 ms\n84 bytes from 20.1.1.1 icmp_seq=2 ttl=61 time=1.812 ms\n84 bytes from 20.1.1.1 icmp_seq=3 ttl=61 time=1.590 ms\n84 bytes from 20.1.1.1 icmp_seq=4 ttl=61 time=1.872 ms\n84 bytes from 20.1.1.1 icmp_seq=5 ttl=61 time=0.829 ms\n</code></pre> <p>In the next post, we'll take a look at the ingenious BGP unnumbered design and understand how it truly works.</p>","tags":["cumulus","bgp"]},{"location":"blog/2021/12/10/cumulus-basics-part-v---bgp-unnumbered/","title":"Cumulus Basics Part V - BGP unnumbered","text":"<p>In this post, we'll look at BGP unnumbered on Cumulus Linux.</p>","tags":["cumulus","bgp"]},{"location":"blog/2021/12/10/cumulus-basics-part-v---bgp-unnumbered/#introduction","title":"Introduction","text":"<p>The last post introduced basic BGP bringup on a Cumulus box with OSPF as the IGP. Let's now move towards a BGP unnumbered design and understand how that works.</p>","tags":["cumulus","bgp"]},{"location":"blog/2021/12/10/cumulus-basics-part-v---bgp-unnumbered/#topology","title":"Topology","text":"<p>We will use the same network topology as before:</p> <p></p> <p>The idea behind BGP unnumbered is to use the IPv6 link local addressing on hop by hop basis. When you're building a L3 fabric, what is the goal of the underlay? Outside of any multicast replication that may be required, the main goal (from a unicast perspective) is to provide connectivity from one tunnel end point to another. Typically, you would use something like OSPF or IS-IS to advertise the loopbacks of the tunnel endpoints and thus, provide connectivity from one loopback to another. </p> <p>Now, with that premise in mind, let's break it down some more. What is really done on a per hop basis? Each node is simply doing a L3 lookup, resolving the next hop's address, rewriting the L2 header and forwarding it on towards the next hop. This entire process can be lifted away from an IGP and done via BGP itself, by utilizing  link local IPv6 addressing and RFC 5549, which allows you to advertise an IPv4 NLRI with an IPv6 next hop. And how do you resolve the IPv6 next hop? Using the IPv6 neighbor discovery process.</p> <p>Let's start putting some of these pieces together now. First, we enable IPv6 ND on the point to point links and disable RA suppression (which appears to be enabled by default). </p> <pre><code>cumulus@LEAF1:~$ net add interface swp1-2 ipv6 nd ra-interval 10 \ncumulus@LEAF1:~$ net del interface swp1-2 ipv6 nd suppress-ra \ncumulus@LEAF1:~$ net commit \n</code></pre> <p>This adds the following to '/etc/frr/frr.conf' file:</p> <pre><code>cumulus@LEAF1:~$ sudo cat /etc/frr/frr.conf\n[sudo] password for cumulus: \nSorry, try again.\n[sudo] password for cumulus: \nfrr version 4.0+cl3u10\nfrr defaults datacenter\nhostname LEAF1\nusername cumulus nopassword\n!\nservice integrated-vtysh-config\n!\nlog syslog informational\n!\ninterface swp1\n ipv6 nd ra-interval 10\n no ipv6 nd suppress-ra\n!\ninterface swp2\n ipv6 nd ra-interval 10\n no ipv6 nd suppress-ra\n!\nline vty\n!\n</code></pre> <p>From 'net show interface &lt;&gt;' you can confirm the link local IPv6 address, the mac address associated with this interface, Router Advertisement (abbreviated to 'RA' going forward) interval and so on:</p> <pre><code>cumulus@LEAF1:~$ net show interface swp1\n    Name  MAC                Speed  MTU   Mode\n--  ----  -----------------  -----  ----  -------------\nUP  swp1  50:00:00:03:00:01  1G     1500  NotConfigured\n\ncl-netstat counters\n-------------------\nRX_OK  RX_ERR  RX_DRP  RX_OVR  TX_OK  TX_ERR  TX_DRP  TX_OVR\n-----  ------  ------  ------  -----  ------  ------  ------\n54708       0      18       0  57169       0       0       0\n\nRouting\n-------\n  Interface swp1 is up, line protocol is up\n  Link ups:       1    last: 2019/04/29 03:51:10.27\n  Link downs:     1    last: 2019/04/29 03:01:38.14\n  PTM status: disabled\n  vrf: default\n  index 3 metric 0 mtu 1500 speed 1000\n  flags: &lt;UP,BROADCAST,RUNNING,MULTICAST&gt;\n  Type: Ethernet\n  HWaddr: 50:00:00:03:00:01\n  inet6 fe80::5200:ff:fe03:1/64\n  Interface Type Other\n  ND advertised reachable time is 0 milliseconds\n  ND advertised retransmit interval is 0 milliseconds\n  ND router advertisements sent: 51 rcvd: 2\n  ND router advertisements are sent every 10 seconds\n  ND router advertisements lifetime tracks ra-interval\n  ND router advertisement default router preference is medium\n  Hosts use stateless autoconfig for addresses.\n  Neighbor address(s):\n  inet6 fe80::5200:ff:fe01:1/128\n</code></pre> <p>Let's mimic the configuration on SPINE1 as well now.</p> <pre><code>cumulus@SPINE1:~$ net add interface swp1 ipv6 nd ra-interval 10\ncumulus@SPINE1:~$ net del interface swp1 ipv6 nd suppress-ra \ncumulus@SPINE1:~$ net commit\n</code></pre> <p>Again, look at 'net show interface swp1' to confirm the mac address and the IPv6 link local address:</p> <pre><code>cumulus@SPINE1:~$ net show interface swp1\n    Name  MAC                Speed  MTU   Mode\n--  ----  -----------------  -----  ----  -------------\nUP  swp1  50:00:00:01:00:01  1G     1500  NotConfigured\n\ncl-netstat counters\n-------------------\nRX_OK  RX_ERR  RX_DRP  RX_OVR  TX_OK  TX_ERR  TX_DRP  TX_OVR\n-----  ------  ------  ------  -----  ------  ------  ------\n57216       0      44       0  54847       0       0       0\n\nLLDP Details\n------------\nLocalPort  RemotePort(RemoteHost)\n---------  ----------------------\nswp1       swp1(LEAF1)\n\nRouting\n-------\n  Interface swp1 is up, line protocol is up\n  Link ups:       1    last: 2019/04/29 04:01:52.99\n  Link downs:     1    last: 2019/04/29 03:04:24.45\n  PTM status: disabled\n  vrf: default\n  index 3 metric 0 mtu 1500 speed 1000\n  flags: &lt;UP,BROADCAST,RUNNING,MULTICAST&gt;\n  Type: Ethernet\n  HWaddr: 50:00:00:01:00:01\n  inet6 fe80::5200:ff:fe01:1/64\n  Interface Type Other\n  ND advertised reachable time is 0 milliseconds\n  ND advertised retransmit interval is 0 milliseconds\n  ND router advertisements sent: 81 rcvd: 80\n  ND router advertisements are sent every 10 seconds\n  ND router advertisements lifetime tracks ra-interval\n  ND router advertisement default router preference is medium\n  Hosts use stateless autoconfig for addresses.\n  Neighbor address(s):\n  inet6 fe80::5200:ff:fe03:1/128\n</code></pre> <p>Take a look at the neighbor discovery process between LEAF1 and SPINE1 now. </p> <p>First, SPINE1 sends out a neighbor solicitation (abbreviated to 'NS' going forward) message with a target address of itself. This is sent to a well known multicast address:</p> <p></p> <p>After a back and forth RA, another NS is sent by SPINE1 but this time, with a target address of 'fe80::5200:ff:fe03:1', which corresponds to the link local IPv6 address assigned to swp1 of LEAF1. Notice how the ICMPv6 option also specifies the link-layer address, which corresponds to SPINE1, swp1's mac address. </p> <p></p> <p>LEAF1 responds to this with a Neighbor Advertisement (abbreviated to NA going forward) message.</p> <p></p> <p>Notice that the link-layer address in the NA sent by LEAF1 is the mac address of its port, swp1. SPINE1 can now use this information to build its IP neighbor table. The same process happens the other way around, with LEAF1 sending a NS and SPINE1 responding back with a NA. At the end of this, both should have their IP neighbor tables correctly populated. </p> <p>You can confirm this using:</p> <pre><code>cumulus@LEAF1:~$ ip -6 neighbor show\nfe80::5200:ff:fe01:1 dev swp1 lladdr 50:00:00:01:00:01 router REACHABLE\n\ncumulus@SPINE1:~$ ip -6 neighbor show\nfe80::5200:ff:fe03:1 dev swp1 lladdr 50:00:00:03:00:01 router REACHABLE\n</code></pre> <p>Let's bring up BGP over this link now. The configuration needs to be modified a little bit since these links do not have any IPv4 address anymore (apart from their default link local IPv4 addresses). Instead of specifying an IP address in the BGP neighbor statement, Cumulus allows you to specify the port number.</p> <pre><code>// LEAF1 configuration\n\ncumulus@LEAF1:~$ net add bgp autonomous-system 1\ncumulus@LEAF1:~$ net add bgp router-id 1.1.1.1  \ncumulus@LEAF1:~$ net add bgp neighbor swp1 remote-as internal\n\n// SPINE1 configuration\n\ncumulus@LEAF1:~$ net add bgp autonomous-system 1\ncumulus@LEAF1:~$ net add bgp router-id 11.11.11.11 \ncumulus@LEAF1:~$ net add bgp neighbor swp1 remote-as internal\n</code></pre> <p>A packet capture shows us the bringup sequence for BGP between these two boxes:</p> <p></p> <p>Let's break this down quickly - initially we see several TCP resets. Why is that? Because BGP port is not open yet on SPINE1 (it was not configured at that point in time), hence any TCP SYN coming for a destination port of 179 (BGP) would be rejected by SPINE1. Once the configuration is complete on both sides, we see the 3-way TCP handshake complete and the OPEN messages being sent. </p> <p>Among other capabilities exchanged in the OPEN message, an important one is highlighted in the capture - the extended next hop encoding. This allows for an IPv4 NLRI to have an IPv6 next hop. You need to make sure this capability is exchanged. To force this, you can use this 'net add bgp neighbor  capability extended-nexthop' command on a Cumulus box. <p>Using a similar approach, we can complete our BGP peerings for this entire infrastructure. Remember to make each LEAF switch a route reflector client of the SPINE switches (otherwise an update from a LEAF switch will not be sent to the other LEAF switches by the SPINE because of iBGP peering rules). At the end of this, each SPINE should see three peerings - one to each of the LEAF switches:</p> <pre><code>//SPINE1\n\ncumulus@SPINE1:~$ net show bgp ipv4 unicast summary \nBGP router identifier 11.11.11.11, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 3, using 58 KiB of memory\n\nNeighbor        V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLEAF1(swp1)     4          1     374     376        0    0    0 00:04:19            0\nLEAF2(swp2)     4          1      31      31        0    0    0 00:01:26            0\nLEAF3(swp3)     4          1       6       6        0    0    0 00:00:11            0\n\nTotal number of neighbors 3 \n\n// SPINE2\n\ncumulus@SPINE2:~$ net show bgp ipv4 unicast summary\nBGP router identifier 22.22.22.22, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 3, using 58 KiB of memory\n\nNeighbor        V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLEAF2(swp1)     4          1      34      34        0    0    0 00:01:33            0\nLEAF1(swp2)     4          1     125     127        0    0    0 00:04:52            0\nLEAF3(swp3)     4          1       9       9        0    0    0 00:00:18            0\n\nTotal number of neighbors 3 \n</code></pre> <p>Each SPINE has three BGP neighbors, as expected. We have not advertised the host subnets yet so let's do that and take a packet capture to analyze how this is advertised.</p> <pre><code>cumulus@LEAF1:~$ net add bgp network 10.1.1.0/24\ncumulus@LEAF1:~$ net commit\n</code></pre> <p>Take a look at the following capture taken on LEAF2 as it receives a BGP update from SPINE1:</p> <p></p> <p>The NLRI describes an IPv4 subnet but the next hop is an IPv6 address. How cool is that? Look at the RIB/FIB on LEAF2 to confirm how this is installed:</p> <pre><code>cumulus@LEAF2:~$ net show route 10.1.1.1\nRIB entry for 10.1.1.1\n======================\nRouting entry for 10.1.1.0/24\n  Known via \"bgp\", distance 200, metric 0, best\n  Last update 00:12:21 ago\n  * fe80::5200:ff:fe01:2, via swp2\n  * fe80::5200:ff:fe02:1, via swp1\n\n\nFIB entry for 10.1.1.1\n======================\n10.1.1.0/24  proto bgp  metric 20 \n        nexthop via 169.254.0.1  dev swp2 weight 1 onlink\n        nexthop via 169.254.0.1  dev swp1 weight 1 onlink \n</code></pre> <p>The RIB installs the prefix against the link local IPv6 address while the FIB installs them against the link local IPv4 address. </p> <p>After advertising all host subnets, LEAF1s RIB looks like this:</p> <pre><code>cumulus@LEAF1:~$ net show route ipv4\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR,\n       &gt; - selected route, * - FIB route\n\nC&gt;* 1.1.1.1/32 is directly connected, lo, 16:51:39\nC&gt;* 10.1.1.0/24 is directly connected, swp3, 16:28:27\nB&gt;* 20.1.1.0/24 [200/0] via fe80::5200:ff:fe01:1, swp1, 00:00:24\n  *                     via fe80::5200:ff:fe02:2, swp2, 00:00:24\nB&gt;* 30.1.1.0/24 [200/0] via fe80::5200:ff:fe01:1, swp1, 00:00:14\n  *                     via fe80::5200:ff:fe02:2, swp2, 00:00:14 \n</code></pre> <p>PC1 should be able to ping PC2 and PC3 now:</p> <pre><code>PC1&gt; ping 20.1.1.1\n\n84 bytes from 20.1.1.1 icmp_seq=1 ttl=61 time=2.743 ms\n84 bytes from 20.1.1.1 icmp_seq=2 ttl=61 time=1.295 ms\n84 bytes from 20.1.1.1 icmp_seq=3 ttl=61 time=1.648 ms\n84 bytes from 20.1.1.1 icmp_seq=4 ttl=61 time=1.637 ms\n84 bytes from 20.1.1.1 icmp_seq=5 ttl=61 time=1.543 ms\n\nPC1&gt; ping 30.1.1.1\n\n84 bytes from 30.1.1.1 icmp_seq=1 ttl=61 time=2.585 ms\n84 bytes from 30.1.1.1 icmp_seq=2 ttl=61 time=1.402 ms\n84 bytes from 30.1.1.1 icmp_seq=3 ttl=61 time=1.528 ms\n84 bytes from 30.1.1.1 icmp_seq=4 ttl=61 time=1.975 ms\n84 bytes from 30.1.1.1 icmp_seq=5 ttl=61 time=1.638 ms\n</code></pre> <p>And there it is. A thing of beauty!</p>","tags":["cumulus","bgp"]},{"location":"blog/2021/12/11/cumulus-basics-part-vi---vxlan-l2vnis-with-bgp-evpn/","title":"Cumulus Basics Part VI - VXLAN L2VNIs with BGP EVPN","text":"<p>In this post, we introduce BGP EVPN and a VXLAN fabric in Cumulus Linux, with L2VNIs.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/11/cumulus-basics-part-vi---vxlan-l2vnis-with-bgp-evpn/#introduction","title":"Introduction","text":"<p>Now that we've covered the basics of BGP unnumbered in the last post, we'll start building a VXLAN based fabric with BGP EVPN. </p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/11/cumulus-basics-part-vi---vxlan-l2vnis-with-bgp-evpn/#topology","title":"Topology","text":"<p>Our topology remains the same, with a minor change - PC2 has now been moved to the same subnet as PC1, 10.1.1.0/24:</p> <p></p> <p>The PCs are lightweight emulations here. Take note of the mac addresses assigned to them as they will be important and constantly referenced through the post:</p> <pre><code>PC1&gt; show ip\n\nNAME        : PC1[1]\nIP/MASK     : 10.1.1.1/24\nGATEWAY     : 10.1.1.254\nDNS         : \nMAC         : 00:50:79:66:68:06\nLPORT       : 20000\nRHOST:PORT  : 127.0.0.1:30000\nMTU         : 1500\n\nPC2&gt; show ip\n\nNAME        : PC2[1]\nIP/MASK     : 10.1.1.2/24\nGATEWAY     : 10.1.1.254\nDNS         : \nMAC         : 00:50:79:66:68:07\nLPORT       : 20000\nRHOST:PORT  : 127.0.0.1:30000\nMTU         : 1500\n</code></pre> <p>Start by activating the neighbors against the L2VPN EVPN AFI/SAFI. Outside of this, I am re-doing some of the configurations from the previous post for completeness sake. We are adding a loopback per switch, adding BGP neighbors and marking each LEAF switch as a route reflector client on the SPINEs</p> <pre><code>// SPINE1\n\ncumulus@SPINE1:~$ net add loopback lo ip address 11.11.11.11/32\ncumulus@SPINE1:~$ net add bgp autonomous-system 1\ncumulus@SPINE1:~$ net add bgp neighbor swp1-3 remote-as internal\ncumulus@SPINE1:~$ net add bgp neighbor swp1-3 route-reflector-client\ncumulus@SPINE1:~$ net add bgp l2vpn evpn neighbor swp1-3 activate\ncumulus@SPINE1:~$ net add bgp l2vpn evpn neighbor swp1-3 route-reflector-client\n\n// SPINE2 \n\ncumulus@SPINE2:~$ net add loopback lo ip address 22.22.22.22/32\ncumulus@SPINE2:~$ net add bgp autonomous-system 1\ncumulus@SPINE2:~$ net add bgp neighbor swp1-3 remote-as internal\ncumulus@SPINE2:~$ net add bgp neighbor swp1-3 route-reflector-client\ncumulus@SPINE2:~$ net add bgp l2vpn evpn neighbor swp1-3 activate\ncumulus@SPINE2:~$ net add bgp l2vpn evpn neighbor swp1-3 route-reflector-client\n\n// LEAF1\n\ncumulus@LEAF1:~$ net add loopback lo ip add 1.1.1.1/32\ncumulus@LEAF1:~$ net add bgp autonomous-system 1\ncumulus@LEAF1:~$ net add bgp neighbor swp1-2 remote-as internal \ncumulus@LEAF1:~$ net add bgp l2vpn evpn neighbor swp1-2 activate \ncumulus@LEAF1:~$ net commit \n</code></pre> <p>The LEAF1 configuration must be appropriately replicated on the other two LEAF switches (modify interfaces as needed and add unique loopback IPs).</p> <p>Post this, confirm that BGP peerings for L2VPN EVPN is up. Each SPINE should have three peerings, one to each LEAF switch:</p> <pre><code>// SPINE1 peerings:\n\ncumulus@SPINE1:~$ net show bgp l2vpn evpn summary \nBGP router identifier 11.11.11.11, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 3, using 58 KiB of memory\n\nNeighbor        V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLEAF1(swp1)     4          1      63      64        0    0    0 00:02:57            0\nLEAF2(swp2)     4          1      42      43        0    0    0 00:01:56            0\nLEAF3(swp3)     4          1      29      30        0    0    0 00:01:16            0\n\nTotal number of neighbors 3 \n\n// SPINE2 peerings:\n\ncumulus@SPINE2:~$ net show bgp l2vpn evpn summary \nBGP router identifier 22.22.22.22, local AS number 1 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 3, using 58 KiB of memory\n\nNeighbor        V         AS MsgRcvd MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLEAF2(swp1)     4          1      47      48        0    0    0 00:02:10            0\nLEAF1(swp2)     4          1      70      71        0    0    0 00:03:19            0\nLEAF3(swp3)     4          1      36      36        0    0    0 00:01:38            0\n\nTotal number of neighbors 3 \n</code></pre> <p>Remember that the underlay provides reachability from one tunnel endpoint to another. Since we are using BGP unnumbered as the underlay itself, we need to advertise the loopbacks of each LEAF switch into the IPv4 address family.</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net add bgp network 1.1.1.1/32\ncumulus@LEAF1:~$ net commit\n\n// LEAF2\n\ncumulus@LEAF2:~$ net add bgp network 2.2.2.2/32\ncumulus@LEAF2:~$ net commit\n\n// LEAF3\n\ncumulus@LEAF3:~$ net add bgp network 3.3.3.3/32\ncumulus@LEAF3:~$ net commit\n</code></pre> <p>Each LEAF switch should now be aware of the loopbacks of the other to LEAF switches.</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net show bgp ipv4 unicast \nBGP table version is 7, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, = multipath,\n              i internal, r RIB-failure, S Stale, R Removed\nOrigin codes: i - IGP, e - EGP, ? - incomplete\n\n   Network          Next Hop            Metric LocPrf Weight Path\n*&gt; 1.1.1.1/32       0.0.0.0                  0         32768 i\n*=i2.2.2.2/32       swp2                     0    100      0 i\n*&gt;i                 swp1                     0    100      0 i\n*=i3.3.3.3/32       swp2                     0    100      0 i\n*&gt;i                 swp1                     0    100      0 i\n\nDisplayed  3 routes and 5 total paths \n\n// LEAF2\n\ncumulus@LEAF2:~$ net show bgp ipv4 unicast \nBGP table version is 6, local router ID is 2.2.2.2\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, = multipath,\n              i internal, r RIB-failure, S Stale, R Removed\nOrigin codes: i - IGP, e - EGP, ? - incomplete\n\n   Network          Next Hop            Metric LocPrf Weight Path\n*&gt;i1.1.1.1/32       swp2                     0    100      0 i\n*=i                 swp1                     0    100      0 i\n*&gt; 2.2.2.2/32       0.0.0.0                  0         32768 i\n*&gt;i3.3.3.3/32       swp2                     0    100      0 i\n*=i                 swp1                     0    100      0 i\n\nDisplayed  3 routes and 5 total paths \n\n// LEAF3\n\ncumulus@LEAF3:~$ net show bgp ipv4 unicast \nBGP table version is 6, local router ID is 3.3.3.3\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, = multipath,\n              i internal, r RIB-failure, S Stale, R Removed\nOrigin codes: i - IGP, e - EGP, ? - incomplete\n\n   Network          Next Hop            Metric LocPrf Weight Path\n*&gt;i1.1.1.1/32       swp3                     0    100      0 i\n*=i                 swp1                     0    100      0 i\n*&gt;i2.2.2.2/32       swp3                     0    100      0 i\n*=i                 swp1                     0    100      0 i\n*&gt; 3.3.3.3/32       0.0.0.0                  0         32768 i\n\nDisplayed  3 routes and 5 total paths \n</code></pre> <p>Confirm reachability from loopback to loopback. Running this test from one of the LEAF switches should be enough:</p> <pre><code>// LEAF1 to LEAF2 reachability\n\ncumulus@LEAF1:~$ ping 2.2.2.2 -I 1.1.1.1\nPING 2.2.2.2 (2.2.2.2) from 1.1.1.1 : 56(84) bytes of data.\n64 bytes from 2.2.2.2: icmp_seq=1 ttl=63 time=1.94 ms\n64 bytes from 2.2.2.2: icmp_seq=2 ttl=63 time=2.05 ms\n64 bytes from 2.2.2.2: icmp_seq=3 ttl=63 time=2.73 ms\n64 bytes from 2.2.2.2: icmp_seq=4 ttl=63 time=1.91 ms\n64 bytes from 2.2.2.2: icmp_seq=5 ttl=63 time=1.01 ms\n^C\n--- 2.2.2.2 ping statistics ---\n5 packets transmitted, 5 received, 0% packet loss, time 4008ms\nrtt min/avg/max/mdev = 1.012/1.931/2.730/0.550 ms\n\n// LEAF1 to LEAF3 reachability\n\ncumulus@LEAF1:~$ ping 3.3.3.3 -I 1.1.1.1\nPING 3.3.3.3 (3.3.3.3) from 1.1.1.1 : 56(84) bytes of data.\n64 bytes from 3.3.3.3: icmp_seq=1 ttl=63 time=18.2 ms\n64 bytes from 3.3.3.3: icmp_seq=2 ttl=63 time=1.78 ms\n64 bytes from 3.3.3.3: icmp_seq=3 ttl=63 time=2.41 ms\n64 bytes from 3.3.3.3: icmp_seq=4 ttl=63 time=1.85 ms\n64 bytes from 3.3.3.3: icmp_seq=5 ttl=63 time=2.28 ms\n^C\n--- 3.3.3.3 ping statistics ---\n5 packets transmitted, 5 received, 0% packet loss, time 4008ms\nrtt min/avg/max/mdev = 1.788/5.314/18.230/6.462 ms\n</code></pre> <p>Now, we start with a simple premise - PC1 cannot ping PC2:</p> <pre><code>PC1&gt; ping 10.1.1.2\n\nhost (10.1.1.2) not reachable\n</code></pre> <p>Our goal is to allow PC1 to talk to PC2 via a VXLAN fabric. The idea is straightforward - you map your VLAN to a VNID (VXLAN network ID) and associate this to a virtual interface that acts as your entry/exit point for VXLAN packets. </p> <p>The configuration on a Cumulus box for this is like so (scroll right to understand what each configuration is doing):</p> <pre><code>cumulus@LEAF1:~$ net add vxlan vni10 vxlan id 10010             // adds VNID to virtual interface, vni10\ncumulus@LEAF1:~$ net add vxlan vni10 bridge access 10           // adds VLAN 10 to vni10\ncumulus@LEAF1:~$ net add vxlan vni10 bridge learning off        // disables mac learning on vni10\ncumulus@LEAF1:~$ net add vxlan vni10 vxlan local-tunnelip 1.1.1.1   // specifies local source IP to be 1.1.1.1 for VXLAN packets\n</code></pre> <p>It is also important to understand what changes the above commands translate to, so take a look at that when you commit them:</p> <pre><code>cumulus@LEAF1:~$ net commit \n--- /etc/network/interfaces     2019-05-13 05:23:36.667000000 +0000\n+++ /run/nclu/ifupdown2/interfaces.tmp  2019-05-13 05:29:13.613000000 +0000\n@@ -18,20 +18,29 @@\n\n auto swp2\n iface swp2\n\n auto swp3\n iface swp3\n     bridge-access 10\n\n auto bridge\n iface bridge\n-    bridge-ports swp3\n+    bridge-ports swp3 vni10\n     bridge-vids 10\n     bridge-vlan-aware yes\n\n auto vlan10\n iface vlan10\n     address 10.1.1.254/24\n     vlan-id 10\n     vlan-raw-device bridge\n\n+auto vni10\n+iface vni10\n+    bridge-access 10\n+    bridge-learning off\n+    mstpctl-bpduguard yes\n+    mstpctl-portbpdufilter yes\n+    vxlan-id 10010\n+    vxlan-local-tunnelip 1.1.1.1\n+\n\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  ------------------------------------------------\ncumulus  2019-05-13 05:27:26.613573  net add vxlan vni10 vxlan id 10010\ncumulus  2019-05-13 05:28:16.336833  net add vxlan vni10 bridge access 10\ncumulus  2019-05-13 05:28:27.454434  net add vxlan vni10 bridge learning off\ncumulus  2019-05-13 05:29:08.227404  net add vxlan vni10 vxlan local-tunnelip 1.1.1.1\n</code></pre> <p>The changes we made creates a new virtual interface with the name 'vni10' (we gave this name in the first command), associates VLAN 10 and a VXLAN network identifier (VNID) of 10010 to it, disables mac learning on this, associates a source IP of 1.1.1.1 to this and adds this interface to the VLAN-aware bridge. </p> <p>Mimic this configuration on LEAF2 - the only change you need to make there is the local tunnel-ip that is used (configure that as LEAF2s loopback, 2.2.2.2).</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/11/cumulus-basics-part-vi---vxlan-l2vnis-with-bgp-evpn/#control-plane-flow","title":"Control-plane flow","text":"<p>Let's try and understand the flow of control-plane learning now. As a baseline, we have not learnt PC1/PC2 macs nor are there any type-2 routes in the BGP table:</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net show bridge macs vlan 10\n\nVLAN  Master  Interface  MAC                TunnelDest  State      Flags  LastSeen\n----  ------  ---------  -----------------  ----------  ---------  -----  --------\n  10  bridge  bridge     50:00:00:04:00:03              permanent         02:40:27\n\ncumulus@LEAF1:~$ net show bgp l2vpn evpn route type macip \nNo EVPN prefixes (of requested type) exist \n\n// LEAF2\n\ncumulus@LEAF2:~$ net show bridge macs vlan 10\n\nVLAN  Master  Interface  MAC                TunnelDest  State      Flags  LastSeen\n----  ------  ---------  -----------------  ----------  ---------  -----  --------\n  10  bridge  bridge     50:00:00:05:00:03              permanent         02:40:41\n\ncumulus@LEAF2:~$ net show bgp l2vpn evpn route type macip\nNo EVPN prefixes (of requested type) exist \n</code></pre> <p>I am going to restart PC1 now. This forces a GARP to be sent out when it comes up and this GARP causes PC1s mac to be learnt on swp3 of LEAF1:</p> <pre><code>cumulus@LEAF1:~$ net show bridge macs vlan 10\n\nVLAN  Master  Interface  MAC                TunnelDest  State      Flags  LastSeen\n----  ------  ---------  -----------------  ----------  ---------  -----  --------\n  10  bridge  bridge     50:00:00:04:00:03              permanent         02:49:50\n  10  bridge  swp3       00:50:79:66:68:06                                00:00:17\n</code></pre> <p>Once the mac is learnt, it gets pushed into the BGP table as well:</p> <pre><code>cumulus@LEAF1:~$ net show bgp l2vpn evpn route type macip \nBGP table version is 13, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:2\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                            32768 i\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                            32768 i\n\nDisplayed 2 prefixes (2 paths) (of requested type) \n</code></pre> <p>This will now be sent as an update to the SPINEs and the SPINEs reflect it to LEAF2. A packet capture on LEAF2 shows this BGP UPDATE message:</p> <p></p> <p>There is a lot of interesting information in this update. The NLRI is an EVPN NLRI, describing a mac advertisement route (type-2). The RD is 1.1.1.1:2 and you can see the mac address inside this NLRI with no IP address included. Also, look at the final attribute which is for extended communities - this includes the RT (which is a combination of the AS and the VNID itself). The extended community also describes the encapsulation which is of type VXLAN. </p> <p>Immediately after this update, another update is sent with the IP address included this time:</p> <p></p> <p>LEAF2 gets this update and installs it in its BGP table as well against the RD that was in the update (1.1.1.1:2):</p> <pre><code>cumulus@LEAF2:~$ net show bgp l2vpn evpn route type macip\nBGP table version is 16, local router ID is 2.2.2.2\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:2\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                  0    100      0 i\n\nDisplayed 2 prefixes (2 paths) (of requested type) \n</code></pre> <p>From here, the type-2 route gets pushed into the mac address table and associated to vni10, with the 'offload' flag set and the VXLAN destination IP set to 1.1.1.1:</p> <pre><code>cumulus@LEAF2:~$ net show bridge macs | grep 00:50:79:66:68:06\nVLAN      Master  Interface  MAC                TunnelDest  State      Flags          LastSeen\n--------  ------  ---------  -----------------  ----------  ---------  -------------  --------\n10        bridge  vni10      00:50:79:66:68:06                         offload        00:16:43\nuntagged          vni10      00:50:79:66:68:06  1.1.1.1                self, offload  00:16:43\n</code></pre> <p>You can see that with the BGP EVPN control-plane, the data-plane is already built and ready without any conversation between the hosts.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/11/cumulus-basics-part-vi---vxlan-l2vnis-with-bgp-evpn/#data-plane-flow","title":"Data-plane flow","text":"<p>From a data-plane perspective, when PC1 pings PC2, it tries to resolve the IP address first. This is achieved via an ARP request that goes through the fabric. This is typically done in two ways - either with headend replication on the ingress tunnel endpoint (meaning the ARP is packaged into a unicast packet sent over the VXLAN fabric) or via multicast (the underlay needs to be multicast aware). We are not going to go into BUM (broadcast/unknown unicast/multicast) traffic handling right now since that needs to be a separate post in itself.</p> <p>Assuming that ARP is resolved, PC1 generates a unicast ICMP request message that hits LEAF1. LEAF1 does a mac table lookup and finds that this needs to be sent over vni10, with a VXLAN encapsulation, a VNID of 10010 and a destination IP of 2.2.2.2:</p> <pre><code>cumulus@LEAF1:~$ net show bridge macs 00:50:79:66:68:07\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags          LastSeen\n--------  ------  ---------  -----------------  ----------  -----  -------------  --------\n10        bridge  vni10      00:50:79:66:68:07                     offload        00:59:05\nuntagged          vni10      00:50:79:66:68:07  2.2.2.2            self, offload  00:59:05\n</code></pre> <p>On the wire, the packets look like this:</p> <p></p> <p>It is an ICMP header inside an IP header inside a VXLAN header inside a UDP header inside an IP header. The logic is that the top most IP header carries the packet from one VXLAN tunnel endpoint to the other. At the other end, the top most IP header is stripped off (since it owns the destination IP address), revealing the UDP and the VXLAN headers which are subsequently stripped off as well. Another mac lookup is done on the packet and a forwarding decision is made based on that:</p> <pre><code>cumulus@LEAF2:~$ net show bridge macs 00:50:79:66:68:07\n\nVLAN  Master  Interface  MAC                TunnelDest  State  Flags  LastSeen\n----  ------  ---------  -----------------  ----------  -----  -----  --------\n  10  bridge  swp3       00:50:79:66:68:07                            00:00:03\n</code></pre> <p>In this case, the mac table says that the packet needs to be forwarded out of swp3. This gets the packet to PC2 which responds back with an ICMP reply. The entire data-plane process happens again, in the reverse direction with LEAF2 now being the source of the VXLAN encapsulation:</p> <p></p> <p>Notice how the top most IP header now has a source IP address of 2.2.2.2. Effectively, you can visualize this as a bi-directional tunnel carrying traffic over a routed infrastructure between two hosts in the same subnet.</p> <p></p> <p>I hope this was informative. I'll see y'all in the next post!</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/12/cumulus-basics-part-vii---vxlan-routing---asymmetric-irb/","title":"Cumulus Basics Part VII - VXLAN routing - asymmetric IRB","text":"<p>In this post, we look at VXLAN routing with asymmetric IRB on Cumulus Linux.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/12/cumulus-basics-part-vii---vxlan-routing---asymmetric-irb/#topology","title":"Topology","text":"<p>We continue on with the same topology as the last post:</p> <p></p> <p>Our goal is to have PC1 talk to PC3 over the VXLAN fabric that we have built out. VXLAN routing can broadly be done using two methods - asymmetric and symmetric IRB. In this post, we will discuss the asymmetric methodology.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/12/cumulus-basics-part-vii---vxlan-routing---asymmetric-irb/#understanding-asymmetric-irb","title":"Understanding Asymmetric IRB","text":"<p>The logic behind asymmetric IRB is that the routing between VXLANs/VNIs is done at each LEAF. This is analogous to inter-VLAN routing where the routing from one VLAN to another happens on the first routed hop itself. To facilitate said inter-VLAN routing, you are required to have the L3 interfaces for both VLANs defined on the first hop (LEAF1 here, for example), so that the box is capable of taking in the packet from one VLAN, routing it into the other VLAN where it can then ARP for the end host directly.</p> <p>Following the same logic, for inter-VXLAN routing, you are required to have the L3 interface for both VNIs on the first routed hop. Let's break this down into chunks and start our configuration. </p> <p>First, we need to have L3 interfaces for the VLANs themselves that will act as gateways for the end hosts:</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net add vlan 10 ip address 10.1.1.254/24\ncumulus@LEAF1:~$ net add vlan 30 ip address 30.1.1.254/24\n\n// LEAF3\n\ncumulus@LEAF3:~$ net add vlan 10 ip address 10.1.1.254/24\ncumulus@LEAF3:~$ net add vlan 30 ip address 30.1.1.254/24\n</code></pre> <p>Notice how the same IP address is defined at both LEAF switches for the same VLAN. Typically, what you should do here is define another virtual address with a virtual mac address common to all LEAF switches sharing this VLAN (VRR, in Cumulus terms). However, for simplicity sake, we are not doing that here.  </p> <p>Next, the routed interfaces for the corresponding VXLANs (or rather VNIs) are also created on both LEAF1 and LEAF3:</p> <pre><code>// LEAF1\n\n    // for VNI 10010, mapped to VLAN 10\n\n    cumulus@LEAF1:~$ net add vxlan vni10 vxlan id 10010\n    cumulus@LEAF1:~$ net add vxlan vni10 bridge access 10\n    cumulus@LEAF1:~$ net add vxlan vni10 vxlan local-tunnelip 1.1.1.1\n\n    // for VNI 10030, mapped to VLAN 30\n\n    cumulus@LEAF1:~$ net add vxlan vni30 vxlan id 10030\n    cumulus@LEAF1:~$ net add vxlan vni30 bridge access 30\n    cumulus@LEAF1:~$ net add vxlan vni30 vxlan local-tunnelip 1.1.1.1\n\n// LEAF3\n\n    // for VNI 10010 mapped to VLAN 10\n\n    cumulus@LEAF3:~$ net add vxlan vni10 vxlan id 10010\n    cumulus@LEAF3:~$ net add vxlan vni10 bridge access 10\n    cumulus@LEAF3:~$ net add vxlan vni10 vxlan local-tunnelip 3.3.3.3\n\n    // for VNI 10030 mapped to VLAN 30\n\n    cumulus@LEAF3:~$ net add vxlan vni30 vxlan id 10030\n    cumulus@LEAF3:~$ net add vxlan vni30 bridge access 30\n    cumulus@LEAF3:~$ net add vxlan vni30 vxlan local-tunnelip 3.3.3.3\n</code></pre> <p>You can confirm the RD/RT values that are auto-generated for these VNIs using:</p> <pre><code>cumulus@LEAF1:~$ net show bgp evpn vni \nAdvertise Gateway Macip: Disabled\nAdvertise SVI Macip: Disabled\nAdvertise All VNI flag: Enabled\nBUM flooding: Head-end replication\nNumber of L2 VNIs: 2\nNumber of L3 VNIs: 0\nFlags: * - Kernel\n  VNI        Type RD                    Import RT                 Export RT                 Tenant VRF                           \n* 10030      L2   1.1.1.1:3             1:10030                   1:10030                  default                              \n* 10010      L2   1.1.1.1:2             1:10010                   1:10010                  default\n</code></pre> <p>Remember, the default gateway of the hosts is the corresponding SVI IP address of the LEAF switch:</p> <pre><code>PC1&gt; show ip\n\nNAME        : PC1[1]\nIP/MASK     : 10.1.1.1/24\nGATEWAY     : 10.1.1.254\nDNS         : \nMAC         : 00:50:79:66:68:06\nLPORT       : 20000\nRHOST:PORT  : 127.0.0.1:30000\nMTU         : 1500\n\nPC3&gt; show ip\n\nNAME        : PC3[1]\nIP/MASK     : 30.1.1.1/24\nGATEWAY     : 30.1.1.254\nDNS         : \nMAC         : 00:50:79:66:68:08\nLPORT       : 20000\nRHOST:PORT  : 127.0.0.1:30000\nMTU         : 1500\n</code></pre> <p>So, how does this all work?</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/12/cumulus-basics-part-vii---vxlan-routing---asymmetric-irb/#the-control-plane","title":"The control-plane","text":"<p>PC1 initiates a ping. It first does an 'and' operation to determine if the destination is in the same subnet or not. In this case, it finds out that it is not, so it forwards the packet to its default gateway. In order to do this, it must first ARP for the default gateway, resolve that and then build the ICMP request packet. </p> <p>This ARP request causes PC1s mac address to be learnt on swp3 of LEAF1. 'Zebra' notifies this to BGP; BGP adds this into its EVPN table and sends an update regarding this to its peers. This BGP update is carried to LEAF3, where BGP populates its EVPN table and then informs 'Zebra', which pushes this into the L2 table. This entire control-plane learning process can be visualized like so:</p> <p></p> <p>Debugs on the box can confirm this process. This is the first time we are introducing debugging on Cumulus boxes, so it's good to make a note of this. Debugs are typically redirected to the '/var/log/frr/frr.log' file but you need to set your logging level correctly first:</p> <pre><code>cumulus@LEAF3:~$ sudo vtysh\n\nHello, this is FRRouting (version 4.0+cl3u10).\nCopyright 1996-2005 Kunihiro Ishiguro, et al.\n\nLEAF3# conf t\nLEAF3(config)# log syslog debug\nLEAF3(config)# end\n</code></pre> <p>'sudo vtysh' allows you to enter a Cisco IOS type prompt. From here, you set the syslog level to 'debug'. This allows for debug logs to be logged in the /var/log/frr/frr.log file. Enable the relevant debugs now:</p> <pre><code>LEAF3# debug bgp updates \nLEAF3# debug bgp zebra\nLEAF3# debug zebra vxlan\nLEAF3# debug zebra rib\nLEAF3# debug zebra events \n</code></pre> <p>From the log file, I have taken relevant snippets of the debugs. On LEAF1, when PC1s mac address is first learnt:</p> <pre><code>// zebra processes mac and sends update to bgpd\n\n2019-05-19T13:03:40.248233+00:00 LEAF1 zebra[865]: UPD MAC 00:50:79:66:68:06 intf swp3(5) VID 10 -&gt; VNI 10010 curFlags 0x4\n2019-05-19T13:03:40.249155+00:00 LEAF1 zebra[865]: Send MACIP Add flags 0x0 MAC 00:50:79:66:68:06 IP  seq 0 L2-VNI 10010 to bgp\n2019-05-19T13:03:40.250227+00:00 LEAF1 zebra[865]: Processing neighbors on local MAC 00:50:79:66:68:06 ADD, VNI 10010\n2019-05-19T13:03:40.250601+00:00 LEAF1 zebra[865]: Send MACIP Add flags 0x0 MAC 00:50:79:66:68:06 IP 10.1.1.1 seq 0 L2-VNI 10010 to bgp\n\n// bgpd receives this update and adds to its table\n\n2019-05-19T13:03:40.250845+00:00 LEAF1 bgpd[874]: 0:Recv MACIP Add flags 0x0 MAC 00:50:79:66:68:06 IP  VNI 10010 seq 0 state 0\n2019-05-19T13:03:40.251222+00:00 LEAF1 bgpd[874]: 0:Recv MACIP Add flags 0x0 MAC 00:50:79:66:68:06 IP 10.1.1.1 VNI 10010 seq 0 state 0\n2019-05-19T13:03:40.301259+00:00 LEAF1 bgpd[874]: group_announce_route_walkcb: afi=l2vpn, safi=evpn, p=[2]:[00:50:79:66:68:06]/224\n2019-05-19T13:03:40.301930+00:00 LEAF1 bgpd[874]: subgroup_process_announce_selected: p=[2]:[00:50:79:66:68:06]/224, selected=0xaf8cac41e0\n2019-05-19T13:03:40.302449+00:00 LEAF1 bgpd[874]: group_announce_route_walkcb: afi=l2vpn, safi=evpn, p=[2]:[00:50:79:66:68:06]:[10.1.1.1]/224\n2019-05-19T13:03:40.302762+00:00 LEAF1 bgpd[874]: subgroup_process_announce_selected: p=[2]:[00:50:79:66:68:06]:[10.1.1.1]/224, selected=0xaf8cac3860\n\n// bgp sends an update to its peers\n\n2019-05-19T13:03:40.303018+00:00 LEAF1 bgpd[874]: u2:s2 send UPDATE w/ attr: nexthop 1.1.1.1, localpref 100, extcommunity ET:8 RT:1:10010, path\n2019-05-19T13:03:40.303266+00:00 LEAF1 bgpd[874]: u2:s2 send MP_REACH for afi/safi 25/70\n2019-05-19T13:03:40.303782+00:00 LEAF1 bgpd[874]: u2:s2 send UPDATE RD 1.1.1.1:2 [2]:[00:50:79:66:68:06]/224 label 10010 l2vpn evpn\n2019-05-19T13:03:40.304096+00:00 LEAF1 bgpd[874]: u2:s2 send UPDATE RD 1.1.1.1:2 [2]:[00:50:79:66:68:06]:[10.1.1.1]/224 label 10010 l2vpn evpn\n2019-05-19T13:03:40.304332+00:00 LEAF1 bgpd[874]: u2:s2 send UPDATE len 144 numpfx 2\n2019-05-19T13:03:40.304744+00:00 LEAF1 bgpd[874]: u2:s2 swp1 send UPDATE w/ nexthop 1.1.1.1\n2019-05-19T13:03:40.304961+00:00 LEAF1 bgpd[874]: u2:s2 swp2 send UPDATE w/ nexthop 1.1.1.1\n</code></pre> <p>Similar debugs from LEAF3, run in parallel:</p> <pre><code>// bgp on LEAF3 receives update from SPINE1\n\n2019-05-19T13:03:40.017491+00:00 LEAF3 bgpd[887]: swp3 rcvd UPDATE w/ attr: nexthop 1.1.1.1, localpref 100, metric 0, extcommunity RT:1:10010 ET:8, originator 1.1.1.1, clusterlist 11.11.11.11, path\n2019-05-19T13:03:40.017949+00:00 LEAF3 bgpd[887]: swp3 rcvd UPDATE wlen 0 attrlen 142 alen 0\n2019-05-19T13:03:40.018244+00:00 LEAF3 bgpd[887]: swp3 rcvd RD 1.1.1.1:2 [2]:[00:50:79:66:68:06]/224 label 10010 l2vpn evpn\n2019-05-19T13:03:40.018402+00:00 LEAF3 bgpd[887]: Tx ADD MACIP, VNI 10010 MAC 00:50:79:66:68:06 IP  flags 0x0 seq 0 remote VTEP 1.1.1.1\n2019-05-19T13:03:40.018523+00:00 LEAF3 bgpd[887]: swp3 rcvd RD 1.1.1.1:2 [2]:[00:50:79:66:68:06]:[10.1.1.1]/224 label 10010 l2vpn evpn\n2019-05-19T13:03:40.018650+00:00 LEAF3 bgpd[887]: Tx ADD MACIP, VNI 10010 MAC 00:50:79:66:68:06 IP 10.1.1.1 flags 0x0 seq 0 remote VTEP 1.1.1.1\n\n// bgpd informs zebra, which installs the mac address\n\n2019-05-19T13:03:40.021415+00:00 LEAF3 zebra[880]: zebra message comes from socket [16]\n2019-05-19T13:03:40.021681+00:00 LEAF3 zebra[880]: Recv MACIP ADD VNI 10010 MAC 00:50:79:66:68:06 flags 0x0 seq 0 VTEP 1.1.1.1 from bgp\n2019-05-19T13:03:40.021893+00:00 LEAF3 zebra[880]: Processing neighbors on remote MAC 00:50:79:66:68:06 ADD, VNI 10010\n2019-05-19T13:03:40.022072+00:00 LEAF3 zebra[880]: zebra message comes from socket [16]\n2019-05-19T13:03:40.022363+00:00 LEAF3 zebra[880]: Recv MACIP ADD VNI 10010 MAC 00:50:79:66:68:06 IP 10.1.1.1 flags 0x0 seq 0 VTEP 1.1.1.1 from bgp\n</code></pre> <p>You can now look at the BGP EVPN table to confirm what was installed:</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net show bgp evpn route\nBGP table version is 1, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:2\n*&gt; [3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                            32768 i\nRoute Distinguisher: 1.1.1.1:3\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                            32768 i\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                            32768 i\n*&gt; [3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                            32768 i\nRoute Distinguisher: 2.2.2.2:2\n* i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\nRoute Distinguisher: 3.3.3.3:2\n* i[2]:[0]:[0]:[48]:[00:50:79:66:68:08]\n                    3.3.3.3                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:08]\n                    3.3.3.3                  0    100      0 i\n* i[2]:[0]:[0]:[48]:[00:50:79:66:68:08]:[32]:[30.1.1.1]\n                    3.3.3.3                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:08]:[32]:[30.1.1.1]\n                    3.3.3.3                  0    100      0 i\n* i[3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                  0    100      0 i\nRoute Distinguisher: 3.3.3.3:3\n* i[3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                  0    100      0 i\n\nDisplayed 9 prefixes (14 paths) \n\n// LEAF3\n\ncumulus@LEAF3:~$ net show bgp evpn route\nBGP table version is 1, local router ID is 3.3.3.3\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:2\n* i[3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                  0    100      0 i\nRoute Distinguisher: 1.1.1.1:3\n* i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                  0    100      0 i\n* i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                  0    100      0 i\n* i[3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                  0    100      0 i\nRoute Distinguisher: 2.2.2.2:2\n* i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\nRoute Distinguisher: 3.3.3.3:2\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:08]\n                    3.3.3.3                            32768 i\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:08]:[32]:[30.1.1.1]\n                    3.3.3.3                            32768 i\n*&gt; [3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                            32768 i\nRoute Distinguisher: 3.3.3.3:3\n*&gt; [3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                            32768 i\n\nDisplayed 9 prefixes (14 paths) \n</code></pre> <p>Also confirm the entries in the mac address table, which should now include the mac address of the the host learnt via BGP EVPN:</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net show bridge macs 00:50:79:66:68:08\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags          LastSeen\n--------  ------  ---------  -----------------  ----------  -----  -------------  --------\n30        bridge  vni30      00:50:79:66:68:08                     offload        00:00:26\nuntagged          vni30      00:50:79:66:68:08  3.3.3.3            self, offload  00:10:48\n\n// LEAF3\n\ncumulus@LEAF3:~$ net show bridge macs 00:50:79:66:68:06\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags          LastSeen\n--------  ------  ---------  -----------------  ----------  -----  -------------  --------\n10        bridge  vni10      00:50:79:66:68:06                     offload        00:11:02\nuntagged          vni10      00:50:79:66:68:06  1.1.1.1            self, offload  00:11:02\n</code></pre> <p>Remember that the presence of the mac address in the BGP EVPN table is purely controlled by the presence of the same mac address in the L2 table. When the mac is added to the L2 table, a notification is sent to BGP to add it to its EVPN table and advertise the NLRI. The mac address is subjected to its normal mac ageing timers in the L2 table - if/when the mac expires and it gets deleted from the L2 table, again, BGP is notified of the same and it sends a withdraw message for that mac address.</p> <p>This concludes the workings of the control-plane. </p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/12/cumulus-basics-part-vii---vxlan-routing---asymmetric-irb/#the-data-plane","title":"The data-plane","text":"<p>Once PC1 resolves its default gateway, it generates the native ICMP packet. LEAF1 gets this:</p> <p></p> <p>Since the destination mac address is of LEAF1, it strips off the Ethernet header and does a route lookup on the destination IP address in the IP header.</p> <pre><code>cumulus@LEAF1:~$ net show route 30.1.1.1\nRIB entry for 30.1.1.1\n======================\nRouting entry for 30.1.1.0/24\n  Known via \"connected\", distance 0, metric 0, best\n  Last update 00:10:17 ago\n  * directly connected, vlan30\n\n\nFIB entry for 30.1.1.1\n======================\n30.1.1.0/24 dev vlan30  proto kernel  scope link  src 30.1.1.254\n</code></pre> <p>The destination is directly connected, so LEAF1 can ARP for it. Here's where things get a bit interesting - if you take a packet capture, you would see no ARPs generated. This is because of an enhancement that reduces flooding within the VXLAN fabric (called ARP suppression). These VTEPs (VXLAN tunnel endpoints) maintain an arp-cache that can be populated via a type-2 mac+ip route. This local ARP cache can be used to proxy ARP (note - this is supposed to be disabled by default and enabled using the command 'net add vxlan  bridge arp-nd-suppress on' but the behavior on 3.7.5 appears to indicate that it is enabled by default).  <p>You can confirm what is there in the arp-cache using the following command:</p> <pre><code>// LEAF1\n\ncumulus@LEAF1:~$ net show evpn arp-cache vni all\nVNI 10030 #ARP (IPv4 and IPv6, local and remote) 3\n\nIP                   Type   State    MAC               Remote VTEP          \n30.1.1.1             remote active   00:50:79:66:68:08 3.3.3.3              \nfe80::5200:ff:fe04:3 local  active   50:00:00:04:00:03\n30.1.1.254           local  active   50:00:00:04:00:03\n\nVNI 10010 #ARP (IPv4 and IPv6, local and remote) 3\n\nIP                   Type   State    MAC               Remote VTEP          \n10.1.1.254           local  active   50:00:00:04:00:03\n10.1.1.1             local  active   00:50:79:66:68:06\nfe80::5200:ff:fe04:3 local  active   50:00:00:04:00:03 \n\n// LEAF3\n\ncumulus@LEAF3:~$ net show evpn arp-cache vni all\nVNI 10030 #ARP (IPv4 and IPv6, local and remote) 3\n\nIP                   Type   State    MAC               Remote VTEP          \nfe80::5200:ff:fe03:2 local  active   50:00:00:03:00:02\n30.1.1.1             local  active   00:50:79:66:68:08\n30.1.1.254           local  active   50:00:00:03:00:02\n\nVNI 10010 #ARP (IPv4 and IPv6, local and remote) 3\n\nIP                   Type   State    MAC               Remote VTEP          \n10.1.1.254           local  active   50:00:00:03:00:02\nfe80::5200:ff:fe03:2 local  active   50:00:00:03:00:02\n10.1.1.1             remote active   00:50:79:66:68:06 1.1.1.1 \n</code></pre> <p>LEAF1 has all the required information to forward this packet. It encapsulates this with the relevant headers and forwards the packet. Assuming that the link to SPINE1 is chosen as the hash, the following packet is sent:</p> <p></p> <p>The VXLAN header contains the egress VNI - 10030 in this case. The outer IP header has a source IP address of the VTEP encapsulating the packet (1.1.1.1, in this case) and a destination IP address of the peer VTEP (3.3.3.3, in this case).</p> <p>Assuming SPINE1 gets it, it simply does a route lookup for the destination (since the destination mac address belongs to itself):</p> <pre><code>cumulus@SPINE1:~$ net show route 3.3.3.3\nRIB entry for 3.3.3.3\n=====================\nRouting entry for 3.3.3.3/32\n  Known via \"bgp\", distance 200, metric 0, best\n  Last update 02:28:05 ago\n  * fe80::5200:ff:fe03:3, via swp3\n\n\nFIB entry for 3.3.3.3\n=====================\n3.3.3.3 via 169.254.0.1 dev swp3  proto bgp  metric 20 onlink\n</code></pre> <p>It rewrites the layer2 header and forwards the packet to LEAF3. LEAF3 decapsulates it and forwards the native ICMP packet to PC3. The data-plane process can be visualized like so:</p> <p></p> <p>The same process happens backwards from PC3 to PC1 now. The egress VNI (once LEAF3 encapsulates the ICMP reply) would be 10010 in this case. The return packet from LEAF3 is this:</p> <p></p> <p>This has been fun, hasn't it? We looked at the basic workflows for both data-plane and control-plane of an asymmetric IRB based VXLAN fabric and successfully routed traffic between VNIs 10010 and 10030.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/08/cumulus-basics-part-iii---static-routing-and-ospf/","title":"Cumulus Basics Part III - static routing and OSPF","text":"<p>In this post, we will look at an introduction to routing on Cumulus Linux, with static routing and OSPF.</p>","tags":["cumulus"]},{"location":"blog/2021/12/08/cumulus-basics-part-iii---static-routing-and-ospf/#introduction","title":"Introduction","text":"<p>Initially, Cumulus OS used the Quagga suite for routing capability. However, more recently, there has been a general adoption of a fork of Quagga called FRRouting (FRR) - Cumulus now includes FRR instead of Quagga. Like always, you can either edit the files directly or using Cumulus' NCLU to enable the respective routing features as well. </p>","tags":["cumulus"]},{"location":"blog/2021/12/08/cumulus-basics-part-iii---static-routing-and-ospf/#topology","title":"Topology","text":"<p>We'll be using the following network topology for this post:</p> <p></p> <p>Routing configuration is stored in /etc/frr/frr.conf (like how interface configuration is stored in /etc/network/interfaces). It is important to note that the protocols under FRR run as deamons on the OS and are not enabled by default. To see the list of daemons and their status, you can view the /etc/frr/daemons file:</p> <pre><code>cumulus@SW1:~$ cat /etc/frr/daemons\nzebra=yes\nbgpd=no\nospfd=no\nospf6d=no\nripd=no\nripngd=no\nisisd=no\npimd=no\nldpd=no\nnhrpd=no\neigrpd=no\nbabeld=no\nsharpd=no\npbrd=no\n</code></pre> <p>'zebra' is the IP routing manager and controls things like static routes. It provides kernel routing table updates, interface lookups, and redistribution of routes between different routing protocols (quoted from 'http://docs.frrouting.org/en/latest/zebra.html').</p> <p>Let's configure some static routes to provide connectivity between PC1 and PC2. The option to add static routes comes under 'net add routing':</p> <pre><code>cumulus@SW1:~$ net add routing \n    agentx                :  Enable SNMP support for OSPF, OSPFV3, and BGP4 MIBS\n    as-path               :  AS_PATH attribute\n    community-list        :  Add a community list entry\n    defaults              :  Set of configuration defaults used\n    enable                :  To make able\n    extcommunity-list     :  An extended community list\n    import-table          :  Import routes from non-main kernel table\n    large-community-list  :  BGP large community-list\n    line                  :  A terminal line\n    log                   :  Logging control\n    mroute                :  Static unicast routes in MRIB for multicast RPF lookup\n    password              :  Set a password\n    prefix-list           :  Filter updates to/from this neighbor\n    protocol              :  Filter routing info exchanged between zebra and protocol\n    ptm-enable            :  Enable neighbor check with specified topology\n    route                 :  Static routes\n    route-map             :  Route-map\n    service               :  Service\n    zebra                 :  Zebra information\n</code></pre> <p>Among other things, you can debug zebra, create route-maps from here as well. To serve the purpose of this topology, we simply need to create a static route for 20.1.1.0/24 on SW1 with a next hop of SW2 and a static route for 10.1.1.0/24 on SW2 with a next hop of SW1.</p> <pre><code>cumulus@SW1:~$ net add routing route 20.1.1.0/24 172.16.12.2\ncumulus@SW1:~$ net commit\n\n--- /run/nclu/frr/frr.conf.scratchpad.baseline  2019-04-29 14:28:53.585648028 +0000\n+++ /run/nclu/frr/frr.conf.scratchpad   2019-04-29 14:28:53.585648028 +0000\n@@ -1,9 +1,11 @@\n frr version 4.0+cl3u8\n frr defaults datacenter\n hostname SW1\n username cumulus nopassword\n service integrated-vtysh-config\n log syslog informational\n line vty\n\n end\n+ip route 20.1.1.0/24 172.16.12.2\n+end\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  ---------------------------------------------\ncumulus  2019-04-29 14:28:53.587527  net add routing route 20.1.1.0/24 172.16.12.2\n</code></pre> <pre><code>cumulus@SW2:~$ net add routing route 10.1.1.0/24 172.16.12.1\ncumulus@SW2:~$ net commit \n--- /run/nclu/frr/frr.conf.scratchpad.baseline  2019-04-29 14:29:37.915465608 +0000\n+++ /run/nclu/frr/frr.conf.scratchpad   2019-04-29 14:29:37.915465608 +0000\n@@ -1,9 +1,11 @@\n frr version 4.0+cl3u8\n frr defaults datacenter\n hostname SW2\n username cumulus nopassword\n service integrated-vtysh-config\n log syslog informational\n line vty\n\n end\n+ip route 10.1.1.0/24 172.16.12.1\n+end\n\n\nnet add/del commands since the last \"net commit\"\n================================================\n\nUser     Timestamp                   Command\n-------  --------------------------  ---------------------------------------------\ncumulus  2019-04-29 14:29:37.918018  net add routing route 10.1.1.0/24 172.16.12.1\n</code></pre> <p>The changes made to /etc/frr/frr.conf are highlighted in the previous outputs (in blue). PC1 can ping PC2 now: </p> <pre><code>PC-1&gt; ping 20.1.1.1\n84 bytes from 20.1.1.1 icmp_seq=1 ttl=62 time=8.933 ms\n84 bytes from 20.1.1.1 icmp_seq=2 ttl=62 time=1.999 ms\n84 bytes from 20.1.1.1 icmp_seq=3 ttl=62 time=2.573 ms\n84 bytes from 20.1.1.1 icmp_seq=4 ttl=62 time=2.043 ms\n84 bytes from 20.1.1.1 icmp_seq=5 ttl=62 time=2.237 ms\n</code></pre> <p>Pretty straightforward, wasn't it?</p> <p>I am going to undo these changes now and move towards a routing protocol like OSPF instead.</p> <pre><code>cumulus@SW1:~$ net del routing route 20.1.1.0/24 172.16.12.2\ncumulus@SW1:~$ net commit\n\ncumulus@SW2:~$ net del routing route 10.1.1.0/24 172.16.12.1\ncumulus@SW2:~$ net commit \n</code></pre> <p>Let's bring up OSPF now.</p> <pre><code>// SW1 configuration\n\ncumulus@SW1:~$ net add ospf router-id 1.1.1.1\ncumulus@SW1:~$ net add interface swp2 ospf area 0     \ncumulus@SW1:~$ net add interface swp2 ospf network point-to-point \ncumulus@SW1:~$ net add interface swp1 ospf area 0\ncumulus@SW1:~$ net add interface swp1 ospf passive\ncumulus@SW1:~$ net commit\n\n// SW2 configuration\n\ncumulus@SW2:~$ net add ospf router-id 2.2.2.2\ncumulus@SW2:~$ net add interface swp2 ospf area 0\ncumulus@SW2:~$ net add interface swp2 ospf network point-to-point\ncumulus@SW2:~$ net add interface swp1 ospf area 0\ncumulus@SW2:~$ net add interface swp1 ospf passive \ncumulus@SW2:~$ net commit \n</code></pre> <p>Verify that OSPF is up and that the LSDB has the correct information:</p> <pre><code>cumulus@SW1:~$ net show ospf neighbor \n\nNeighbor ID     Pri State           Dead Time Address         Interface            RXmtL RqstL DBsmL\n2.2.2.2           1 Full/DROther      36.571s 172.16.12.2     swp2:172.16.12.1         0     0     0\n\n\ncumulus@SW2:~$ net show ospf neighbor \n\nNeighbor ID     Pri State           Dead Time Address         Interface            RXmtL RqstL DBsmL\n1.1.1.1           1 Full/DROther      35.519s 172.16.12.1     swp2:172.16.12.2         0     0     0\n</code></pre> <pre><code>cumulus@SW1:~$ net show ospf database \n\n       OSPF Router with ID (1.1.1.1)\n\n                Router Link States (Area 0.0.0.0)\n\nLink ID         ADV Router      Age  Seq#       CkSum  Link count\n1.1.1.1         1.1.1.1           79 0x80000006 0x470d 3\n2.2.2.2         2.2.2.2          100 0x80000006 0x1e27 3\n\ncumulus@SW1:~$ net show ospf database router 1.1.1.1\n\n       OSPF Router with ID (1.1.1.1)\n\n\n                Router Link States (Area 0.0.0.0)\n\n  LS age: 89\n  Options: 0x2  : *|-|-|-|-|-|E|-\n  LS Flags: 0x3  \n  Flags: 0x0\n  LS Type: router-LSA\n  Link State ID: 1.1.1.1 \n  Advertising Router: 1.1.1.1\n  LS Seq Number: 80000006\n  Checksum: 0x470d\n  Length: 60\n\n   Number of Links: 3\n\n    Link connected to: another Router (point-to-point)\n     (Link ID) Neighboring Router ID: 2.2.2.2\n     (Link Data) Router Interface address: 172.16.12.1\n      Number of TOS metrics: 0\n       TOS 0 Metric: 100\n\n    Link connected to: Stub Network\n     (Link ID) Net: 172.16.12.0\n     (Link Data) Network Mask: 255.255.255.0\n      Number of TOS metrics: 0\n       TOS 0 Metric: 100\n\n    Link connected to: Stub Network\n     (Link ID) Net: 10.1.1.0\n     (Link Data) Network Mask: 255.255.255.0\n      Number of TOS metrics: 0\n       TOS 0 Metric: 100\n\n\ncumulus@SW1:~$ net show ospf database router 2.2.2.2\n\n       OSPF Router with ID (1.1.1.1)\n\n\n                Router Link States (Area 0.0.0.0)\n\n  LS age: 114\n  Options: 0x2  : *|-|-|-|-|-|E|-\n  LS Flags: 0x6  \n  Flags: 0x0\n  LS Type: router-LSA\n  Link State ID: 2.2.2.2 \n  Advertising Router: 2.2.2.2\n  LS Seq Number: 80000006\n  Checksum: 0x1e27\n  Length: 60\n\n   Number of Links: 3\n\n    Link connected to: another Router (point-to-point)\n     (Link ID) Neighboring Router ID: 1.1.1.1\n     (Link Data) Router Interface address: 172.16.12.2\n      Number of TOS metrics: 0\n       TOS 0 Metric: 100\n\n    Link connected to: Stub Network\n     (Link ID) Net: 172.16.12.0\n     (Link Data) Network Mask: 255.255.255.0\n      Number of TOS metrics: 0\n       TOS 0 Metric: 100\n\n    Link connected to: Stub Network\n     (Link ID) Net: 20.1.1.0\n     (Link Data) Network Mask: 255.255.255.0\n      Number of TOS metrics: 0\n       TOS 0 Metric: 100\n</code></pre> <p>SW1 and SW2 should have installed 10.1.1.0/24 and 20.1.1.0/24 respectively into RIB/FIB as well.</p> <pre><code>cumulus@SW1:~$ net show route ospf\nRIB entry for ospf\n==================\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR,\n       &gt; - selected route, * - FIB route\n\nO   10.1.1.0/24 [110/100] is directly connected, swp1, 00:02:52\nO&gt;* 20.1.1.0/24 [110/200] via 172.16.12.2, swp2, 00:02:42\nO   172.16.12.0/24 [110/100] is directly connected, swp2, 00:02:52\n\ncumulus@SW2:~$ net show route ospf\nRIB entry for ospf\n==================\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR,\n       &gt; - selected route, * - FIB route\n\nO&gt;* 10.1.1.0/24 [110/200] via 172.16.12.1, swp2, 00:03:51\nO   20.1.1.0/24 [110/100] is directly connected, swp1, 00:04:21\nO   172.16.12.0/24 [110/100] is directly connected, swp2, 00:04:21\n</code></pre> <p>The only thing left to verify is the connectivity from PC1 to PC2. </p> <pre><code>PC-1&gt; ping 20.1.1.1\n84 bytes from 20.1.1.1 icmp_seq=1 ttl=62 time=3.140 ms\n84 bytes from 20.1.1.1 icmp_seq=2 ttl=62 time=2.152 ms\n84 bytes from 20.1.1.1 icmp_seq=3 ttl=62 time=2.563 ms\n84 bytes from 20.1.1.1 icmp_seq=4 ttl=62 time=2.020 ms\n84 bytes from 20.1.1.1 icmp_seq=5 ttl=62 time=3.031 ms\n</code></pre> <p>Works like a charm! In our next post, we'll take a look at implementing BGP with OSPF as an IGP on Cumulus VX. </p>","tags":["cumulus"]},{"location":"blog/2021/12/13/cumulus-basics-part-viii---vxlan-symmetric-routing-and-multi-tenancy/","title":"Cumulus Basics Part VIII - VXLAN symmetric routing and multi-tenancy","text":"<p>In this post, we look at VXLAN routing with symmetric IRB and multi-tenancy on Cumulus Linux.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/13/cumulus-basics-part-viii---vxlan-symmetric-routing-and-multi-tenancy/#introduction","title":"Introduction","text":"<p>Now that we've configured and verified a working asymmetric VXLAN routing solution in the previous post, let's take a look at the greener side of the grass (well, it depends on where you stand) - symmetric IRB. This post is going to introduce VRFs into the picture that pave the way for multi-tenancy in VXLAN solutions. </p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/13/cumulus-basics-part-viii---vxlan-symmetric-routing-and-multi-tenancy/#topology","title":"Topology","text":"<p>We continue to use the same topology as our previous post:</p> <p></p> <p>Any configuration we added for asymmetric routing has been removed. We simply have our L2VNIs created on LEAF1 and LEAF3 and the BGP peerings are up. </p> <p>Before we begin with the actual configuration, let's understand the logic behind symmetric IRB. This functions as a bridge-route-route-bridge model where the packet is bridged to your source VTEP from the host and is then routed into a new type of VNI called the L3VNI. This takes the packet through your fabric core to the destination VTEP, where it is routed from the L3VNI into the local L2VNI and then bridged across to the destination host. </p> <p>In addition to the L3VNIs, we also introduce VRFs here. VRFs allow for multi-tenancy. Imagine this - instead of having a dedicated core infrastructure for every customer, you could have this core infrastructure common to multiple customers with the kind of segregation that VRFs can provide. The L3VNIs are tied to the customer VRFs directly and in that sense, the L3VNIs should be unique per VRF.</p> <p>The configuration for symmetric IRB is not very complicated. Let's take LEAF1 as an example and start to configure this:</p> <pre><code>// first, create a VLAN for your L3VNI \n\ncumulus@LEAF1:~$ net add vlan 40 alias VLAN for L3VNI 10040\n\n// next, create the L3VNI, map it to the VLAN and configure the tunnel source \n\ncumulus@LEAF1:~$ net add vxlan vni40 vxlan id 10040\ncumulus@LEAF1:~$ net add vxlan vni40 vxlan local-tunnelip 1.1.1.1\ncumulus@LEAF1:~$ net add vxlan vni40 bridge access 40\n\n// now create the tenant VRF and map it to the L3VNI\n\ncumulus@LEAF1:~$ net add vrf TENANT1 vni 10040\n</code></pre> <p>Now that we have some of these pieces configured, we need to start making sense of this and putting it all together. </p> <p>Naturally, to segregate your customers, they need to be put in their respective tenant VRFs. This means that the customer VLAN (particularly, the first L3 hop, which is the corresponding SVI in this case) needs to be in the tenant VRF. Additionally, you add the corresponding VLAN for the L3VNI in the same tenant VRF. </p> <pre><code>// customer VLAN goes into the tenant VRF\n\ncumulus@LEAF1:~$ net add vlan 10 vrf TENANT1\n\n// VLAN corresponding to the L3VNI goes into tenant VRF as well\n\ncumulus@LEAF1:~$ net add vlan 40 vrf TENANT1\n</code></pre> <p>Confirm that the customer facing SVI and the SVI corresponding to the L3VNI are up and in the correct VRF:</p> <pre><code>// customer facing SVI \n\ncumulus@LEAF1:~$ net show interface vlan10\n    Name    MAC                Speed  MTU   Mode\n--  ------  -----------------  -----  ----  ------------\nUP  vlan10  50:00:00:04:00:03  N/A    1500  Interface/L3\n\nIP Details\n-------------------------  -------------\nIP:                        10.1.1.254/24\nIP Neighbor(ARP) Entries:  1\n\ncl-netstat counters\n-------------------\nRX_OK  RX_ERR  RX_DRP  RX_OVR  TX_OK  TX_ERR  TX_DRP  TX_OVR\n-----  ------  ------  ------  -----  ------  ------  ------\n   22       0       0       0     36       0       0       0\n\nRouting\n-------\n  Interface vlan10 is up, line protocol is up\n  Link ups:       1    last: 2019/06/06 03:30:36.79\n  Link downs:     1    last: 2019/06/06 03:30:36.79\n  PTM status: disabled\n  vrf: TENANT1\n  index 10 metric 0 mtu 1500 speed 0\n  flags: &lt;UP,BROADCAST,RUNNING&gt;,MULTICAST&gt;\n  Type: Ethernet\n  HWaddr: 50:00:00:04:00:03\n  inet 10.1.1.254/24\n  inet6 fe80::5200:ff:fe04:3/64\n  Interface Type Vlan\n  VLAN Id 10\n  Link ifindex 9(bridge)\n\n// SVI corresponding to the L3VNI  \n\ncumulus@LEAF1:~$ net show interface vlan40\n    Name    MAC                Speed  MTU   Mode\n--  ------  -----------------  -----  ----  -------------\nUP  vlan40  50:00:00:04:00:03  N/A    1500  NotConfigured\n\nAlias\n-----\nVLAN for L3VNI 10040\n\ncl-netstat counters\n-------------------\nRX_OK  RX_ERR  RX_DRP  RX_OVR  TX_OK  TX_ERR  TX_DRP  TX_OVR\n-----  ------  ------  ------  -----  ------  ------  ------\n   13       0       0       0     20       0       0       0\n\nRouting\n-------\n  Interface vlan40 is up, line protocol is up\n  Link ups:       3    last: 2019/06/06 03:32:58.69\n  Link downs:     2    last: 2019/06/06 03:32:58.69\n  PTM status: disabled\n  vrf: TENANT1\n  Description: VLAN for L3VNI 10040\n  index 12 metric 0 mtu 1500 speed 0\n  flags: &lt;UP,BROADCAST,RUNNING&gt;,MULTICAST&gt;\n  Type: Unknown\n  HWaddr: 50:00:00:04:00:03\n  inet6 fe80::5200:ff:fe04:3/64\n  Interface Type Vlan\n  VLAN Id 40\n  Link ifindex 9(bridge)\n</code></pre> <p>Both the L2VNI and the L3VNI is going to be assigned a RD/RT value. You can confirm this using 'net show bgp evpn vni\":</p> <pre><code>cumulus@LEAF1:~$ net show bgp evpn vni \nAdvertise Gateway Macip: Disabled\nAdvertise SVI Macip: Disabled\nAdvertise All VNI flag: Enabled\nBUM flooding: Head-end replication\nNumber of L2 VNIs: 1\nNumber of L3 VNIs: 1\nFlags: * - Kernel\n  VNI        Type RD                    Import RT                 Export RT                 Tenant VRF                           \n* 10010      L2   1.1.1.1:3             1:10010                   1:10010                  TENANT1                              \n* 10040      L3   10.1.1.254:2          1:10040                   1:10040                  TENANT1\n</code></pre> <p>Similar configuration is done on LEAF3:</p> <pre><code>cumulus@LEAF3:~$ net add vlan 40\ncumulus@LEAF3:~$ net add vxlan vni40 vxlan id 10040\ncumulus@LEAF3:~$ net add vxlan vni40 vxlan local-tunnelip 3.3.3.3\ncumulus@LEAF3:~$ net add vxlan vni40 bridge access 40\ncumulus@LEAF3:~$ net add vrf TENANT1 vni 10040\ncumulus@LEAF3:~$ net add vlan 30 vrf TENANT1\ncumulus@LEAF3:~$ net add vlan 40 vrf TENANT1\n</code></pre> <p>Let's take a quick peek at the BGP EVPN table and see what is in there:</p> <pre><code>// LEAF1s BGP EVPN table\n\ncumulus@LEAF1:~$ net show bgp l2vpn evpn route\nBGP table version is 10, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:3\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                            32768 i\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                            32768 i\n*&gt; [3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                            32768 i\nRoute Distinguisher: 2.2.2.2:2\n* i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\n\n\n// LEAF3s BGP EVPN table\n\ncumulus@LEAF3:~$ net show bgp l2vpn evpn route\nBGP table version is 3, local router ID is 3.3.3.3\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:3\n* i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                  0    100      0 i\n* i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                  0    100      0 i\n* i[3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                  0    100      0 i\nRoute Distinguisher: 2.2.2.2:2\n* i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\nRoute Distinguisher: 3.3.3.3:2\n*&gt; [3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                            32768 i\n</code></pre> <p>It appears that PC1s mac and IP address have already been learnt and installed in the BGP EVPN table. However, there is no information about PC3 in here (most likely because PC3 has not sent any traffic so far). So, let's take this opportunity to generate some traffic from PC3 and understand how the control-plane is built with L3VNIs.</p> <p>We will enable some debugs to understand what is going on. As explained in the previous post, you must enable logging of syslogs at the debugging level and then enable the debugs. The following debugs were enabled:</p> <pre><code>LEAF3# show debug\nZebra debugging status:\n  Zebra event debugging is on\n  Zebra packet detail debugging is on\n  Zebra kernel debugging is on\n  Zebra RIB debugging is on\n  Zebra VXLAN debugging is on\n\nBGP debugging status:\n  BGP updates debugging is on (inbound)\n  BGP updates debugging is on (outbound)\n  BGP zebra debugging is on\n  BGP vpn label event debugging is on\n</code></pre> <p>The same debugs were enabled on LEAF1 as well so as to capture simultaneous debugging information from both. </p> <p>We generate some traffic from PC3 now (by pinging its default gateway, which is SVI30 on LEAF3). Remember, all debugs are redirected to /var/log/frr/frr.log. Snippets of relevant debug logs are below:</p> <pre><code>// LEAF3\n\n    // LEAF3 zebra learns PC3s mac address and notifies BGP\n\n    2019-06-06T10:01:51.630081+00:00 LEAF3 zebra[936]: ADD MAC 00:50:79:66:68:08 intf swp2(4) VID 30 -&gt; VNI 10030\n    2019-06-06T10:01:51.630191+00:00 LEAF3 zebra[936]: Send MACIP Add flags 0x0 MAC 00:50:79:66:68:08 IP  seq 0 L2-VNI 10030 to bgp\n    2019-06-06T10:01:51.630301+00:00 LEAF3 zebra[936]: netlink_parse_info: netlink-listen (NS 0) type RTM_NEWROUTE(24), len=116, seq=0, pid=0\n    2019-06-06T10:01:51.630570+00:00 LEAF3 zebra[936]: RTM_NEWROUTE ipv6 unicast proto  NS 0\n\n    // bgpd receives the notification from zebra\n\n    2019-06-06T10:01:51.632750+00:00 LEAF3 bgpd[948]: 0:Recv MACIP Add flags 0x0 MAC 00:50:79:66:68:08 IP  VNI 10030 seq 0 state 0\n    2019-06-06T10:01:51.683158+00:00 LEAF3 bgpd[948]: group_announce_route_walkcb: afi=l2vpn, safi=evpn, p=[2]:[00:50:79:66:68:08]/224\n    2019-06-06T10:01:51.683739+00:00 LEAF3 bgpd[948]: subgroup_process_announce_selected: p=[2]:[00:50:79:66:68:08]/224, selected=0x338517eed0\n\n    // bgpd sends an update to its peer about this new learn\n\n    2019-06-06T10:01:51.684041+00:00 LEAF3 bgpd[948]: u2:s2 send UPDATE w/ attr: nexthop 3.3.3.3, localpref 100, extcommunity ET:8 RT:1:10030 RT:1:10040    Rmac:50:00:00:03:00:02, path\n    2019-06-06T10:01:51.684266+00:00 LEAF3 bgpd[948]: u2:s2 send MP_REACH for afi/safi 25/70\n    2019-06-06T10:01:51.684441+00:00 LEAF3 bgpd[948]: u2:s2 send UPDATE RD 3.3.3.3:2 [2]:[00:50:79:66:68:08]/224 label 10030/10040 l2vpn evpn\n    2019-06-06T10:01:51.684808+00:00 LEAF3 bgpd[948]: u2:s2 send UPDATE len 121 numpfx 1\n    2019-06-06T10:01:51.685071+00:00 LEAF3 bgpd[948]: u2:s2 swp1 send UPDATE w/ nexthop 3.3.3.3\n    2019-06-06T10:01:51.685410+00:00 LEAF3 bgpd[948]: u2:s2 swp3 send UPDATE w/ nexthop 3.3.3.3\n\n// LEAF1\n\n    // LEAF1s bgpd receives the update from SPINE1\n\n    2019-06-06T10:01:51.929153+00:00 LEAF1 bgpd[890]: swp1 rcvd UPDATE w/ attr: nexthop 3.3.3.3, localpref 100, metric 0, extcommunity RT:1:10030 RT:1:10040 ET:8   Rmac:50:00:00:03:00:02, originator 3.3.3.3, clusterlist 11.11.11.11, path\n    2019-06-06T10:01:51.929462+00:00 LEAF1 bgpd[890]: swp1 rcvd UPDATE wlen 0 attrlen 126 alen 0\n    2019-06-06T10:01:51.929665+00:00 LEAF1 bgpd[890]: swp1 rcvd RD 3.3.3.3:2 [2]:[00:50:79:66:68:08]:[30.1.1.1]/224 label 10030/10040 l2vpn evpn\n\n    // LEAF1s bgpd installs the prefix in the BGP EVPN table\n\n    2019-06-06T10:01:51.929940+00:00 LEAF1 bgpd[890]: installing evpn prefix [2]:[00:50:79:66:68:08]:[30.1.1.1]/224 as ip prefix 30.1.1.1/32 in vrf TENANT1\n\n    // LEAF1s bgpd receives the update from SPINE2\n\n    2019-06-06T10:01:51.930173+00:00 LEAF1 bgpd[890]: swp2 rcvd UPDATE w/ attr: nexthop 3.3.3.3, localpref 100, metric 0, extcommunity RT:1:10030 RT:1:10040 ET:8   Rmac:50:00:00:03:00:02, originator 3.3.3.3, clusterlist 22.22.22.22, path\n    2019-06-06T10:01:51.930381+00:00 LEAF1 bgpd[890]: swp2 rcvd UPDATE wlen 0 attrlen 126 alen 0\n    2019-06-06T10:01:51.930550+00:00 LEAF1 bgpd[890]: swp2 rcvd RD 3.3.3.3:2 [2]:[00:50:79:66:68:08]:[30.1.1.1]/224 label 10030/10040 l2vpn evpn\n\n    // LEAF1s bgpd installs the prefix in the BGP EVPN table\n\n    2019-06-06T10:01:51.930799+00:00 LEAF1 bgpd[890]: installing evpn prefix [2]:[00:50:79:66:68:08]:[30.1.1.1]/224 as ip prefix 30.1.1.1/32 in vrf TENANT1\n\n    // LEAF1s bgpd notifies zebra\n\n    2019-06-06T10:01:51.981956+00:00 LEAF1 bgpd[890]: bgp_zebra_announce: p=30.1.1.1/32, bgp_is_valid_label: 2\n    2019-06-06T10:01:51.982328+00:00 LEAF1 bgpd[890]: Tx route add VRF 14 30.1.1.1/32 metric 0 tag 0 flags 0x1409 nhnum 1\n    2019-06-06T10:01:51.982524+00:00 LEAF1 bgpd[890]:   nhop [1]: 303:303:: if 12 VRF 14\n    2019-06-06T10:01:51.982848+00:00 LEAF1 bgpd[890]: bgp_zebra_announce: 30.1.1.1/32: announcing to zebra (recursion set)\n\n\n    // LEAF1s zebra gets this message from bgpd and processes it\n\n    2019-06-06T10:01:51.983503+00:00 LEAF1 zebra[881]: zebra message comes from socket [16]\n    2019-06-06T10:01:51.983768+00:00 LEAF1 zebra[881]: Tx RTM_NEWNEIGH family ipv4 IF vlan40(12) Neigh 3.3.3.3 MAC 50:00:00:03:00:02 flags 0x10 state 0x40\n    2019-06-06T10:01:51.983959+00:00 LEAF1 zebra[881]: netlink_talk: netlink-cmd (NS 0) type RTM_NEWNEIGH(28), len=48 seq=46 flags 0x505\n    2019-06-06T10:01:51.984200+00:00 LEAF1 zebra[881]: netlink_parse_info: netlink-cmd (NS 0) ACK: type=RTM_NEWNEIGH(28), seq=46, pid=4294963174\n    2019-06-06T10:01:51.984498+00:00 LEAF1 zebra[881]: Tx RTM_NEWNEIGH family bridge IF vni40(13) VLAN 40 MAC 50:00:00:03:00:02 dst 3.3.3.3\n    2019-06-06T10:01:51.984770+00:00 LEAF1 zebra[881]: netlink_talk: netlink-cmd (NS 0) type RTM_NEWNEIGH(28), len=64 seq=47 flags 0x505\n    2019-06-06T10:01:51.985005+00:00 LEAF1 zebra[881]: netlink_parse_info: netlink-cmd (NS 0) ACK: type=RTM_NEWNEIGH(28), seq=47, pid=4294963174\n    2019-06-06T10:01:51.985202+00:00 LEAF1 zebra[881]: rib_add_multipath: 14:30.1.1.1/32: Inserting route rn 0x653ba3c640, re 0x653b8a7e60 (type 9) existing (nil)\n</code></pre> <p>As you can see, this gets learnt in the BGP EVPN table on LEAF1 and pushed to RIB/FIB as well (for simplicity sake, I have shut down all links to SPINE2):</p> <pre><code>// LEAF1s BGP EVPN table\n\ncumulus@LEAF1:~$ net show bgp l2vpn evpn route\nBGP table version is 4, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-2 prefix: [2]:[ESI]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[ESI]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\nRoute Distinguisher: 1.1.1.1:3\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                            32768 i\n*&gt; [2]:[0]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.1.1.1]\n                    1.1.1.1                            32768 i\n*&gt; [3]:[0]:[32]:[1.1.1.1]\n                    1.1.1.1                            32768 i\nRoute Distinguisher: 2.2.2.2:2\n*&gt;i[3]:[0]:[32]:[2.2.2.2]\n                    2.2.2.2                  0    100      0 i\nRoute Distinguisher: 3.3.3.3:2\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:08]\n                    3.3.3.3                  0    100      0 i\n*&gt;i[2]:[0]:[0]:[48]:[00:50:79:66:68:08]:[32]:[30.1.1.1]\n                    3.3.3.3                  0    100      0 i\n*&gt;i[3]:[0]:[32]:[3.3.3.3]\n                    3.3.3.3                  0    100      0 i\n\n// LEAF1s RIB/FIB                    \n\ncumulus@LEAF1:~$ net show route vrf TENANT1 30.1.1.1\nRIB entry for 30.1.1.1 in vrf TENANT1\n=====================================\nRouting entry for 30.1.1.1/32\n  Known via \"bgp\", distance 200, metric 0, vrf TENANT1, best\n  Last update 00:04:26 ago\n  * 3.3.3.3, via vlan40 onlink\n\n\nFIB entry for 30.1.1.1 in vrf TENANT1\n=====================================\n30.1.1.1 via 3.3.3.3 dev vlan40  proto bgp  metric 20 onlink\n</code></pre> <p>The BGP update itself looks like this:</p> <p></p> <p>The control-plane exchange can be visualized as following:</p> <p></p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/13/cumulus-basics-part-viii---vxlan-symmetric-routing-and-multi-tenancy/#understanding-the-data-plane","title":"Understanding the data-plane","text":"<p>The ICMP request from PC1 hits LEAF1. Since the destination mac address is owned by LEAF1, it strips off the Layer2 header and does a lookup against the destination IP address in the IP header.</p> <pre><code>cumulus@LEAF1:~$ net show route vrf TENANT1 30.1.1.1\nRIB entry for 30.1.1.1 in vrf TENANT1\n=====================================\nRouting entry for 30.1.1.1/32\n  Known via \"bgp\", distance 200, metric 0, vrf TENANT1, best\n  Last update 00:04:26 ago\n  * 3.3.3.3, via vlan40 onlink\n\n\nFIB entry for 30.1.1.1 in vrf TENANT1\n=====================================\n30.1.1.1 via 3.3.3.3 dev vlan40  proto bgp  metric 20 onlink\n</code></pre> <p>This gets encapsulated with the appropriate headers and the outer destination IP address is set to 3.3.3.3. The VNI inside the VXLAN header is set to 10040 which is what VLAN 40 is associated to. </p> <p></p> <p>The data-plane packet, post encapsulation:</p> <p></p> <p>As you can see, the source and destination IP addresses in the outer header belong to the source VTEP and the destination VTEP respectively. A UDP header follows, wherein the destination port signifies the following header as VXLAN. Inside the VXLAN header, the VNI is set to 10040, which is the L3VNI shared between VTEPs, uniquely identifying the VRF. </p> <p>This encapsulated packet is sent towards the SPINE1/SPINE2. </p> <p></p> <p>Assuming it hits SPINE1, a traditional route lookup is done against the destination IP address in the outer header (which is 3.3.3.3 in this case). This is advertised in the underlay and SPINE1 knows that the next-hop for this is LEAF3. </p> <pre><code>cumulus@SPINE1:~$ net show route 3.3.3.3\nRIB entry for 3.3.3.3\n=====================\nRouting entry for 3.3.3.3/32\n  Known via \"bgp\", distance 200, metric 0, best\n  Last update 02:35:42 ago\n  * fe80::5200:ff:fe03:3, via swp3\n\n\nFIB entry for 3.3.3.3\n=====================\n3.3.3.3 via 169.254.0.1 dev swp3  proto bgp  metric 20 onlink \n</code></pre> <p>SPINE1 now forwards this to LEAF3 (remember, the packet is still encapsulated and is simply being routed in the underlay till it reaches the destination VTEP):</p> <p></p> <p>The packet reaches LEAF3 now. The destination mac address in the outer Ethernet header is owned by it so this gets stripped. The destination IP address in the outer IP header is also owned by it, so this gets stripped as well. </p> <p>LEAF3 parses the UDP header and understands that a VXLAN header follows. It then parses the VXLAN header - the most relevant information here is the embedded VNI. </p> <p>Why is this VNI (the L3VNI) so important? LEAF3 (the destination VTEP) uses this VNI to determine which VRF table should be consulted to do the inner destination IP address lookup in. This is how the separation of customers is achieved end to end. </p> <pre><code>cumulus@LEAF3:~$ net show vrf vni \nVRF                                   VNI        VxLAN IF             L3-SVI               State Rmac              \nTENANT1                               10040      vni40                vlan40               Up    50:00:00:03:00:02 \nTENANT2                               10400      vni400               vlan400              Up    50:00:00:03:00:02\n</code></pre> <p>LEAF3 can now look at the VRF table for TENANT1 to determine how to get to 30.1.1.1:</p> <pre><code>cumulus@LEAF3:~$ net show route vrf TENANT1 30.1.1.1\nRIB entry for 30.1.1.1 in vrf TENANT1\n=====================================\nRouting entry for 30.1.1.0/24\n  Known via \"connected\", distance 0, metric 0, vrf TENANT1, best\n  Last update 00:10:47 ago\n  * directly connected, vlan30\n\n\nFIB entry for 30.1.1.1 in vrf TENANT1\n=====================================\n30.1.1.0/24 dev vlan30  proto kernel  scope link  src 30.1.1.254 \n</code></pre> <p>Remember, the EVPN arp-cache table should also be populated with information about 30.1.1.1:</p> <pre><code>cumulus@LEAF3:~$ net show evpn arp-cache vni all\nVNI 10030 #ARP (IPv4 and IPv6, local and remote) 3\n\nIP                   Type   State    MAC               Remote VTEP          \nfe80::5200:ff:fe03:2 local  active   50:00:00:03:00:02\n30.1.1.1             local  active   00:50:79:66:68:08\n30.1.1.254           local  active   50:00:00:03:00:02\n\nVNI 10300 #ARP (IPv4 and IPv6, local and remote) 2\n\nIP                   Type   State    MAC               Remote VTEP          \nfe80::5200:ff:fe03:2 local  active   50:00:00:03:00:02\n30.1.1.254           local  active   50:00:00:03:00:02 \n</code></pre> <p>The packet is now forwarded to PC3 and it can respond back. The same process occurs in the reverse direction as well. </p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/15/cumulus-part-x---vxlan-evpn-and-mlag/","title":"Cumulus Part X - VXLAN EVPN and MLAG","text":"<p>In this post, we take a look at the interaction of MLAG with an EVPN based VXLAN fabric on Cumulus Linux.</p>","tags":["cumulus","vxlan","evpn","mlag"]},{"location":"blog/2021/12/15/cumulus-part-x---vxlan-evpn-and-mlag/#introduction","title":"Introduction","text":"<p>MLAG or MC-LAG (multi-chassis link aggregation) is a fairly common deployment model at the access/leaf layer of both Enterprise and Data Center networks, typically offered by most leading vendors (with different terminologies - vPC, VSS, stackwise-virtual and so on).</p> <p>The general idea is to offer redundancy at the access layer by pairing together two access/leaf switches into a common logical switch, from the perspective of any devices downstream.  Details of Cumulus' implementation can be found here. </p>","tags":["cumulus","vxlan","evpn","mlag"]},{"location":"blog/2021/12/15/cumulus-part-x---vxlan-evpn-and-mlag/#topology","title":"Topology","text":"<p>For this post, we're going to be using the following topology (tested with Cumulus VX 4.2):</p> <p></p> <p>We have three servers, Server5, Server6 and Server3 in VLAN 10, with another server, Server2, in VLAN 20. Server5 is uplinked to both MLAG peers, while Server6 is an orphan device, off of Leaf4 only. </p> <p>We also have external connectivity via PE1, again, connected only to one of the MLAG peers - Leaf3, in this case. PE1 is advertising 99.99.99.99/32, an external network, to Leaf3. </p> <p>Logically, for BGP peering, the spines share a common AS, while each leaf is it's own unique AS. This is a standard eBGP type design, meant to avoid BGP path hunting issues. </p> <p></p>","tags":["cumulus","vxlan","evpn","mlag"]},{"location":"blog/2021/12/15/cumulus-part-x---vxlan-evpn-and-mlag/#basic-configuration","title":"Basic configuration","text":"<p>Each of these devices have a loopback configured. For the leaf's, these loopbacks are the VTEP IPs. The MLAG pair have unique loopacks, but also an anycast CLAG VTEP IP that is configured (similar to a secondary IP):</p> <p></p> <p>This CLAG anycast IP is configured under the loopback itself, for both Leaf3 and Leaf4:</p> <pre><code>// Leaf3\n\ncumulus@Leaf3:mgmt:~$ net show configuration interface lo\ninterface lo\n  # The primary network interface\n  address 3.3.3.3/32\n  clagd-vxlan-anycast-ip 34.34.34.34\n  vxlan-local-tunnelip 3.3.3.3\n\n// Leaf4\n\ncumulus@Leaf4:mgmt:~$ net show configuration interface lo\ninterface lo\n  # The primary network interface\n  address 4.4.4.4/32\n  clagd-vxlan-anycast-ip 34.34.34.34\n  vxlan-local-tunnelip 4.4.4.4\n</code></pre> <p>Each of the MLAG peers form an eBGP peering with the spines, and an iBGP peering with each other. This iBGP peering is important for failure conditions (we'll look at this in a little bit).</p> <pre><code>// IPv4 unicast peering\n\ncumulus@Leaf3:mgmt:~$ net show bgp ipv4 unicast summary \nBGP router identifier 3.3.3.3, local AS number 64523 vrf-id 0\nBGP table version 26\nRIB entries 11, using 2112 bytes of memory\nPeers 3, using 64 KiB of memory\n\nNeighbor             V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLeaf4(peerlink.4094) 4      64523     87384     87976        0    0    0 07:26:27            5\nSpine1(swp1)         4      65550     87974     87971        0    0    0 07:34:50            3\nSpine2(swp2)         4      65550     87973     87968        0    0    0 07:34:50            3\n\nTotal number of neighbors 3 \n\n// L2VPN EVPN unicast peering\n\ncumulus@Leaf3:mgmt:~$ net show bgp l2vpn evpn summary \nBGP router identifier 3.3.3.3, local AS number 64523 vrf-id 0\nBGP table version 0\nRIB entries 25, using 4800 bytes of memory\nPeers 3, using 64 KiB of memory\n\nNeighbor             V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd\nLeaf4(peerlink.4094) 4      64523     87387     87979        0    0    0 07:26:37           30\nSpine1(swp1)         4      65550     87977     87974        0    0    0 07:35:00           30\nSpine2(swp2)         4      65550     87976     87971        0    0    0 07:35:00           30\n\nTotal number of neighbors 3\n</code></pre>","tags":["cumulus","vxlan","evpn","mlag"]},{"location":"blog/2021/12/15/cumulus-part-x---vxlan-evpn-and-mlag/#control-plane-learning-with-mlag","title":"Control-plane learning with MLAG","text":"<p>When a MAC address is learnt over the MLAG, it is synced to the MLAG peer as well. Both the peers would insert the entry in their BGP EVPN tables and advertise it out. As an example, Server5s MAC address is learnt by both Leaf3 and Leaf4 and advertised via BGP EVPN to the spines and to each other, over the iBGP peering.</p> <pre><code>// Leaf3\n\ncumulus@Leaf3:mgmt:~$ net show bridge macs 00:00:00:00:00:05\n\nVLAN  Master  Interface     MAC                TunnelDest  State  Flags  LastSeen\n----  ------  ------------  -----------------  ----------  -----  -----  --------\n  10  bridge  bond-server5  00:00:00:00:00:05                            00:01:03\n\ncumulus@Leaf3:mgmt:~$ net show bgp l2vpn evpn route rd 3.3.3.3:2 mac 00:00:00:00:00:05 \nBGP routing table entry for 3.3.3.3:2:[2]:[0]:[48]:[00:00:00:00:00:05]\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Leaf4(peerlink.4094) Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:00:00:00:00:05] VNI 10010/10040\n  Local\n    34.34.34.34 from 0.0.0.0 (3.3.3.3)\n      Origin IGP, weight 32768, valid, sourced, local, bestpath-from-AS Local, best (First path received)\n      Extended Community: ET:8 RT:3:10010 RT:3:10040 Rmac:44:38:39:ff:00:05 MM:1\n      Last update: Wed Aug  4 06:28:11 2021\n\n// Leaf4\n\ncumulus@Leaf4:mgmt:~$ net show bridge macs 00:00:00:00:00:05\n\nVLAN  Master  Interface     MAC                TunnelDest  State  Flags  LastSeen\n----  ------  ------------  -----------------  ----------  -----  -----  --------\n  10  bridge  bond-server5  00:00:00:00:00:05                            00:00:11\n\ncumulus@Leaf4:mgmt:~$ net show bgp l2vpn evpn route rd 4.4.4.4:3 mac 00:00:00:00:00:05\nBGP routing table entry for 4.4.4.4:3:[2]:[0]:[48]:[00:00:00:00:00:05]\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Leaf3(peerlink.4094) Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:00:00:00:00:05] VNI 10010/10040\n  Local\n    34.34.34.34 from 0.0.0.0 (4.4.4.4)\n      Origin IGP, weight 32768, valid, sourced, local, bestpath-from-AS Local, best (First path received)\n      Extended Community: ET:8 RT:4:10010 RT:4:10040 Rmac:44:38:39:ff:00:05\n      Last update: Wed Aug  4 12:17:01 2021\n</code></pre> <p>There are two big things to remember with MLAG and BGP EVPN advertisements:</p> <ol> <li> <p>Type-2 EVPN prefixes are sent using the anycast VTEP IP address as the next-hop. </p> </li> <li> <p>Type-5 EVPN prefixes are sent using the local VTEP IP address (default behavior in Cumulus Linux, other vendors provide a knob to optionally enable it).</p> </li> </ol> <p>The first big why - why do we need an anycast VTEP IP address for the MLAG peers? This just allows for easy BGP filtering - remember, when Leaf3 advertises a prefix into BGP EVPN, it adds it's own AS number, since the update is being sent to eBGP peers (Spine1/Spine2). When Leaf4 gets this, the update is denied because it sees it's own AS - basic BGP loop prevention. </p> <p>However, this doesn't apply to the iBGP peering that is created over the peer-link. This is where the anycast VTEP IP is useful - because the next-hop is owned by the peers, they will drop any BGP NLRI which has this next-hop (due to self next-hop/martian check). This is important because we wouldn't want the MLAG peers to see each other as next hops (over VXLAN) for locally attached hosts. </p> <p>With a simple BGP updates debug, you can confirm that the peers drop this because of the self next-hop check:</p> <pre><code>Leaf4 bgpd[895]: peerlink.4094 rcvd UPDATE w/ attr: nexthop 34.34.34.34, \nlocalpref 100, extcommunity RT:3:10010 RT:3:10040 ET:8 MM:1 Rmac:44:38:39:ff:00:05, path\n\nLeaf4 bgpd[895]: peerlink.4094 rcvd UPDATE about RD 3.3.3.3:2 \n[2]:[00:00:00:00:00:05]/320 label 10010 \nl2vpn evpn -- DENIED due to: martian or self next-hop;\n\nLeaf4 bgpd[895]: peerlink.4094 rcvd UPDATE about RD 3.3.3.3:2 \n[2]:[00:00:00:00:00:05]:[10.10.10.105]/320 label 10010/10040 \nl2vpn evpn -- DENIED due to: martian or self next-hop;\n</code></pre> <p>Remember, even orphan devices are sent with this anycast VTEP address. For example, in our case, Server6 is an orphan device. Leaf4 sends the BGP EVPN update with the anycast VTEP address:</p> <pre><code>cumulus@Leaf4:mgmt:~$ net show bgp l2vpn evpn route rd 4.4.4.4:3 mac 00:00:00:00:00:06 ip 10.10.10.106\nBGP routing table entry for 4.4.4.4:3:[2]:[0]:[48]:[00:00:00:00:00:06]:[32]:[10.10.10.106]\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Leaf3(peerlink.4094) Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:00:00:00:00:06]:[32]:[10.10.10.106] VNI 10010/10040\n  Local\n    34.34.34.34 from 0.0.0.0 (4.4.4.4)\n      Origin IGP, weight 32768, valid, sourced, local, bestpath-from-AS Local, best (First path received)\n      Extended Community: ET:8 RT:4:10010 RT:4:10040 Rmac:44:38:39:ff:00:05\n      Last update: Wed Aug  4 12:17:01 2021\n</code></pre> <p>On remote VTEPs (taking Leaf5, as an example), this is installed with 34.34.34.34 as the next-hop:</p> <pre><code>// BGP table\n\ncumulus@Leaf5:mgmt:~$ net show bgp l2vpn evpn route rd 4.4.4.4:3 mac 00:00:00:00:00:06 ip 10.10.10.106\nBGP routing table entry for 4.4.4.4:3:[2]:[00:00:00:00:00:06]:[10.10.10.106]/352\nPaths: (2 available, best #2)\n  Advertised to non peer-group peers:\n  Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:00:00:00:00:06]:[32]:[10.10.10.106] VNI 10010/10040\n  65550 64523\n    34.34.34.34 from Spine2(swp2) (22.22.22.22)\n      Origin IGP, valid, external\n      Extended Community: RT:4:10010 RT:4:10040 ET:8 Rmac:44:38:39:ff:00:05\n      Last update: Wed Aug  4 12:17:02 2021\n  Route [2]:[0]:[48]:[00:00:00:00:00:06]:[32]:[10.10.10.106] VNI 10010/10040\n  65550 64523\n    34.34.34.34 from Spine1(swp1) (11.11.11.11)\n      Origin IGP, valid, external, bestpath-from-AS 65550, best (Older Path)\n      Extended Community: RT:4:10010 RT:4:10040 ET:8 Rmac:44:38:39:ff:00:05\n      Last update: Wed Aug  4 12:17:02 2021\n\n// RIB table\n\ncumulus@Leaf5:mgmt:~$ net show route vrf VRF1 ipv4 \nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR, f - OpenFabric,\n       &gt; - selected route, * - FIB route, q - queued, r - rejected, b - backup\n       t - trapped, o - offload failure\n\nVRF VRF1:\nK&gt;* 0.0.0.0/0 [255/8192] unreachable (ICMP unreachable), 2d04h24m\nC&gt;* 10.10.10.0/24 is directly connected, vlan10, 2d04h21m\nB&gt;* 10.10.10.105/32 [20/0] via 34.34.34.34, vlan40 onlink, weight 1, 05:58:40\nB&gt;* 10.10.10.106/32 [20/0] via 34.34.34.34, vlan40 onlink, weight 1, 1d07h40m\nC&gt;* 20.20.20.0/24 is directly connected, vlan20, 2d04h21m\n</code></pre> <p>This can cause traffic for this prefix to hash towards Leaf3 (which does not have a direct connection to Server6). Let's take an example of Server3 pinging Server6. </p> <p>Because this is a same subnet ping, Server3 tries to ARP for the destination directly. Leaf5 responds back because it already has an entry for Server6 in it's EVPN ARP cache:</p> <pre><code>cumulus@Leaf5:mgmt:~$ net show evpn arp vni 10010 ip 10.10.10.106\nIP: 10.10.10.106\n Type: remote\n State: active\n MAC: 00:00:00:00:00:06\n Sync-info: -\n Remote VTEP: 34.34.34.34\n Local Seq: 0 Remote Seq: 0 \n</code></pre> <p>Server3 can now send the ICMP request to Leaf5. The destination MAC address is 00:00:00:00:00:06. Leaf5 does a lookup in it's MAC address table, and sends the packet out with VNI 10010, towards a destination of 34.34.34.34 (the anycast VTEP address, owned by both Leaf3 and Leaf4):</p> <pre><code>cumulus@Leaf5:mgmt:~$ net show bridge macs 00:00:00:00:00:06\n\nVLAN      Master  Interface  MAC                TunnelDest   State  Flags               LastSeen\n--------  ------  ---------  -----------------  -----------  -----  ------------------  --------\n10        bridge  vni10      00:00:00:00:00:06                      extern_learn        01:20:27\nuntagged          vni10      00:00:00:00:00:06  34.34.34.34         self, extern_learn  01:20:27\n</code></pre> <p>A packet capture confirms the L2VNI:</p> <p></p> <p>This can be hashed towards Leaf3. Leaf3 simply does a MAC address lookup (for 00:00:00:00:00:06) now, and find's that is it reachable via the peer-link:</p> <pre><code>cumulus@Leaf3:mgmt:~$ net show bridge macs 00:00:00:00:00:06\n\nVLAN  Master  Interface  MAC                TunnelDest  State  Flags  LastSeen\n----  ------  ---------  -----------------  ----------  -----  -----  --------\n  10  bridge  peerlink   00:00:00:00:00:06                            00:00:53\n</code></pre> <p>Thus, visually, the path of the packet in this case would be:</p> <p></p> <p>The next big why - why are type-5 EVPN routes sent with the local VTEP address, instead of the anycast VTEP address? It is quite common to see external prefixes advertised via one of the MLAG peers only, and not both. In such cases, you can potentially black hole traffic by advertising these type-5 prefixes with the anycast VTEP address (because traffic may be ECMP'd to the peer which does not have a route to these external prefixes). Of course, this can be fixed by having per VRF iBGP peering between the MLAG peers but it doesn't scale well and is a lot of administrative overhead.</p> <p>In general, the 'advertise-pip' BGP EVPN option is used for this - the local VTEP IP address is the 'Primary IP'. Cumulus Linux introduced this in their 4.0 release and it is enabled by default. </p> <p>However, it needs to be configured in a particular way for it to work. Prior to 4.0, you could use the 'hwaddress' option to specify a MAC address for an interface. From 4.0 onward, you need to use the 'address-virtual' option to create the common router MAC that is shared between the two MLAG peers. This allows for each MLAG peer to retain it's unique system MAC and share this common router MAC. </p> <p>This change is done under the SVI that maps to the L3VNI:</p> <pre><code>// Leaf3\n\ncumulus@Leaf3:mgmt:~$ net show configuration interface vlan40\ninterface vlan40\n  address-virtual 44:38:39:FF:00:05\n  vlan-id 40\n  vlan-raw-device bridge\n  vrf VRF1\n\n// Leaf4\n\ncumulus@Leaf4:mgmt:~$ net show configuration interface vlan40\ninterface vlan40\n  address-virtual 44:38:39:FF:00:05\n  vlan-id 40\n  vlan-raw-device bridge\n  vrf VRF1\n</code></pre> <p>You should now see the router MAC changed to this common anycast MAC address (general practice is to just set it as the CLAG MAC address), while the system MAC is retained. </p> <pre><code>// Leaf3\n\ncumulus@Leaf3:mgmt:~$ net show bgp l2vpn evpn vni 10040\nVNI: 10040 (known to the kernel)\n  Type: L3\n  Tenant VRF: VRF1\n  RD: 12.12.12.1:3\n  Originator IP: 34.34.34.34\n  Advertise-gw-macip : n/a\n  Advertise-svi-macip : n/a\n  Advertise-pip: Yes\n  System-IP: 3.3.3.3\n  System-MAC: 50:00:00:05:00:05\n  Router-MAC: 44:38:39:ff:00:05\n  Import Route Target:\n    1:10040\n    2:10040\n    4:10040\n    5:10040\n  Export Route Target:\n    3:10040 \n\n// Leaf4\n\ncumulus@Leaf4:mgmt:~$ net show bgp l2vpn evpn vni 10040\nVNI: 10040 (known to the kernel)\n  Type: L3\n  Tenant VRF: VRF1\n  RD: 50.50.50.1:2\n  Originator IP: 34.34.34.34\n  Advertise-gw-macip : n/a\n  Advertise-svi-macip : n/a\n  Advertise-pip: Yes\n  System-IP: 4.4.4.4\n  System-MAC: 50:00:00:06:00:05\n  Router-MAC: 44:38:39:ff:00:05\n  Import Route Target:\n    1:10040\n    2:10040\n    3:10040\n    5:10040\n  Export Route Target:\n    4:10040 \n</code></pre> <p>This gives a lot of good information - it tells you that 'advertise-pip' is enabled, what the system IP is, what the system MAC and the router MAC is. Thus, for type-5 prefixes, the system IP and the system MAC are used, and for type-2 prefix (regardless of the host being an orphan host), the router MAC and the anycast VTEP IP address is used. </p> <p>The type-5 routes are now generated using the system IP and MAC itself. </p> <pre><code>cumulus@Leaf3:mgmt:~$ net show bgp l2vpn evpn route rd 12.12.12.1:3 prefix 99.99.99.99/32\nBGP routing table entry for 12.12.12.1:3:[5]:[0]:[32]:[99.99.99.99]\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Leaf4(peerlink.4094) Spine1(swp1) Spine2(swp2)\n  Route [5]:[0]:[32]:[99.99.99.99] VNI 10040\n  12\n    3.3.3.3 from 0.0.0.0 (3.3.3.3)\n      Origin IGP, metric 0, valid, sourced, local, bestpath-from-AS 12, best (First path received)\n      Extended Community: ET:8 RT:3:10040 Rmac:50:00:00:05:00:05\n      Last update: Tue Aug  3 04:46:12 2021\n</code></pre> <p>This is advertised only with the L3VNI. On other VTEPs, this should be installed in the VRF table:</p> <pre><code>cumulus@Leaf5:mgmt:~$ net show route vrf VRF1 ipv4\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR, f - OpenFabric,\n       &gt; - selected route, * - FIB route, q - queued, r - rejected, b - backup\n       t - trapped, o - offload failure\n\nVRF VRF1:\nK&gt;* 0.0.0.0/0 [255/8192] unreachable (ICMP unreachable), 5d00h05m\nC&gt;* 10.10.10.0/24 is directly connected, vlan10, 5d00h02m\nB&gt;* 10.10.10.105/32 [20/0] via 34.34.34.34, vlan40 onlink, weight 1, 04:16:51\nB&gt;* 10.10.10.106/32 [20/0] via 34.34.34.34, vlan40 onlink, weight 1, 04:16:51\nC&gt;* 20.20.20.0/24 is directly connected, vlan20, 5d00h02m\nB&gt;* 99.99.99.99/32 [20/0] via 3.3.3.3, vlan40 onlink, weight 1, 00:00:01\n</code></pre> <p>Before we wrap this up, it is important to talk about some failure scenarios with MLAG. An important design consideration (and we'll see this more prominently when we talk about Ethernet Segments in EVPN), is that losing a downlink (towards the server itself) has no control-plane implications. There is absolutely no control-plane convergence because of this - it is purely a data plane forwarding change. </p> <p>For example, say Leaf3s interface going to Server5 goes down.  Traffic can still be hashed towards Leaf3, destined for Server5. It would just be forwarded over the peer-link. This is why capacity planning of the peer-link is equally important. </p> <p>A second failure scenario to consider is what would happen if all fabric links go down. So, for example, Leaf3 loses all its spine facing links and the packet from Server5 (destined to Server2) is hashed to Leaf3. </p> <p>This is where the iBGP peering is useful. Prior to this event, the route is received via the spines, and via the iBGP peering to the MLAG peer. </p> <pre><code>cumulus@Leaf3:mgmt:~$ net show bgp l2vpn evpn route rd 5.5.5.5:2 mac 00:00:00:00:00:02 ip 20.20.20.102\nBGP routing table entry for 5.5.5.5:2:[2]:[0]:[48]:[00:00:00:00:00:02]:[32]:[20.20.20.102]\nPaths: (3 available, best #2)\n  Advertised to non peer-group peers:\n  Leaf4(peerlink.4094) Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:00:00:00:00:02]:[32]:[20.20.20.102] VNI 10020/10040\n  65550 64525\n    5.5.5.5 from Leaf4(peerlink.4094) (4.4.4.4)\n      Origin IGP, localpref 100, valid, internal\n      Extended Community: RT:5:10020 RT:5:10040 ET:8 Rmac:00:20:00:20:00:20\n      Last update: Sat Aug  7 08:18:29 2021\n  Route [2]:[0]:[48]:[00:00:00:00:00:02]:[32]:[20.20.20.102] VNI 10020/10040\n  65550 64525\n    5.5.5.5 from Spine1(swp1) (11.11.11.11)\n      Origin IGP, valid, external, bestpath-from-AS 65550, best (Older Path)\n      Extended Community: RT:5:10020 RT:5:10040 ET:8 Rmac:00:20:00:20:00:20\n      Last update: Sat Aug  7 03:51:08 2021\n  Route [2]:[0]:[48]:[00:00:00:00:00:02]:[32]:[20.20.20.102] VNI 10020/10040\n  65550 64525\n    5.5.5.5 from Spine2(swp2) (22.22.22.22)\n      Origin IGP, valid, external\n      Extended Community: RT:5:10020 RT:5:10040 ET:8 Rmac:00:20:00:20:00:20\n      Last update: Sat Aug  7 03:51:08 2021\n</code></pre> <p>After the link down event (and route withdrawals), the traffic is simply routed via the peer-link. If the iBGP peering was missing, traffic would be blackholed on Leaf3.</p> <p>In the next post, we'll look at how Ethernet Segment based EVPN multi-homing acts as an alternative to MLAGs.</p>","tags":["cumulus","vxlan","evpn","mlag"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/","title":"Cumulus Part IX - Understanding VXLAN EVPN Route-Target control","text":"<p>In this post, we look at how route-targets extended communities can be used to control VXLAN BGP EVPN routes in Cumulus Linux.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/#introduction","title":"Introduction","text":"<p>This post assumes that the reader has a general understanding of L2/L3 VNIs and asymmetric/symmetric IRB.</p> <p>Cumulus, by default, uses auto RTs for L2 and L3 VNIs. This makes for a very easy experience (almost plug and play like) when building VXLAN BGP EVPN fabrics. But it also doesn't help you understand much of how route-targets are being imported across and how to completely control this. </p> <p>It's always good to learn to drive a stick, before moving to an automatic. So, the goal of this blog is to help understand how L2/L3 VNI RTs are exported/imported and how you can control what goes into your customer VRF.  This will also allow you to control whether you want asymmetric or symmetric IRB (see my previous Cumulus blog posts to understand what is asymmetric and symmetric IRB).</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/#topology","title":"Topology","text":"<p>For this post, we're going to use the following topology and incrementally add/delete/modify certain aspects of this network. </p> <p></p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/#rts-in-an-l2vni-environment","title":"RTs in an L2VNI environment","text":"<p>To begin with, this is a pure L2 VNI setup with two servers (Server1 and Server2) deployed in VLAN 10, mapped to VNI 10010. The network is a BGP unnumbered core, with loopbacks of each leaf acting as the VXLAN tunnel IPs. </p> <p>Let's review the BGP configuration:</p> <pre><code>router bgp 64521\n  bgp router-id 1.1.1.1\n  neighbor swp1 interface remote-as external\n  neighbor swp2 interface remote-as external\n\n  address-family ipv4 unicast\n    network 1.1.1.1/32 \n\n  address-family l2vpn evpn\n    neighbor swp1 activate\n    neighbor swp2 activate\n    advertise-all-vni\n</code></pre> <p>Typical BGP configuration - we're advertising all VNIs into EVPN and the BGP L2VPN EVPN peering is activated against both Spine1 and Spine2. By default, Cumulus Linux (FRR, really) uses a model of ASN:VNI to derive the VNI RTs. </p> <p>In our case, this will be 64521:10010 for VNI 10010. We can confirm using the following:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net show bgp l2vpn evpn vni 10010\nVNI: 10010 (known to the kernel)\n  Type: L2\n  Tenant-Vrf: default\n  RD: 1.1.1.1:2\n  Originator IP: 1.1.1.1\n  Mcast group: 0.0.0.0\n  Advertise-gw-macip : Disabled\n  Advertise-svi-macip : Disabled\n  Import Route Target:\n    64521:10010\n  Export Route Target:\n    64521:10010\n</code></pre> <p>PC1s mac address is advertised as a type-2 route using this export RT:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net show bgp l2vpn evpn route rd 1.1.1.1:2 mac 00:50:79:66:68:06 \nBGP routing table entry for 1.1.1.1:2:[2]:[00:50:79:66:68:06]/352\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:50:79:66:68:06] VNI 10010\n  Local\n    1.1.1.1 from 0.0.0.0 (1.1.1.1)\n      Origin IGP, weight 32768, valid, sourced, local, bestpath-from-AS Local, best (First path received)\n      Extended Community: ET:8 RT:64521:10010\n      Last update: Sat Jul 24 16:30:57 2021\n</code></pre> <p>This is correctly imported on Leaf2. Cumulus does not show the route imported into the local RD in the BGP table, however, bgpd informs zebra and zebra has installed it in the MAC address table.</p> <pre><code>cumulus@Leaf2:mgmt:~$ net show bgp l2vpn evpn route rd 1.1.1.1:2 mac 00:50:79:66:68:06\nBGP routing table entry for 1.1.1.1:2:[2]:[00:50:79:66:68:06]/352\nPaths: (2 available, best #1)\n  Advertised to non peer-group peers:\n  Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:50:79:66:68:06] VNI 10010\n  65550 64521\n    1.1.1.1 from Spine1(swp1) (11.11.11.11)\n      Origin IGP, valid, external, bestpath-from-AS 65550, best (Router ID)\n      Extended Community: RT:64521:10010 ET:8\n      Last update: Sat Jul 24 16:30:59 2021\n  Route [2]:[0]:[48]:[00:50:79:66:68:06] VNI 10010\n  65550 64521\n    1.1.1.1 from Spine2(swp2) (22.22.22.22)\n      Origin IGP, valid, external\n      Extended Community: RT:64521:10010 ET:8\n      Last update: Sat Jul 24 16:30:59 2021\n</code></pre> <p>The MAC address table also shows this entry, against a remote VTEP of 1.1.1.1, which is Leaf1. </p> <pre><code>cumulus@Leaf2:mgmt:~$ net show bridge macs 00:50:79:66:68:06\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags               LastSeen\n--------  ------  ---------  -----------------  ----------  -----  ------------------  --------\n10        bridge  vni10      00:50:79:66:68:06                     extern_learn        00:00:09\nuntagged          vni10      00:50:79:66:68:06  1.1.1.1            self, extern_learn  00:00:09\n</code></pre>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/#adding-manual-rts","title":"Adding manual RTs","text":"<p>Let's add a manual export RT for VNI 10010, on Leaf1:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net add bgp l2vpn evpn vni 10010 route-target export 1:10010\ncumulus@Leaf1:mgmt:~$ net commit\n</code></pre> <p>This is correctly added to the prefix that is being advertised via BGP EVPN:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net show bgp l2vpn evpn route type 2\nBGP table version is 9, local router ID is 1.1.1.1\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-1 prefix: [1]:[ESI]:[EthTag]:[IPlen]:[VTEP-IP]:[Frag-id]\nEVPN type-2 prefix: [2]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-4 prefix: [4]:[ESI]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\n                    Extended Community\nRoute Distinguisher: 1.1.1.1:2\n*&gt; [2]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                            32768 i\n                    ET:8 RT:1:10010\n\nDisplayed 1 prefixes (1 paths) (of requested type)\n</code></pre> <p>Leaf2 is still importing this though - why, and how?</p> <pre><code>// BGP EVPN table\n\ncumulus@Leaf2:mgmt:~$ net show bgp l2vpn evpn route type 2 \nBGP table version is 9, local router ID is 2.2.2.2\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-1 prefix: [1]:[ESI]:[EthTag]:[IPlen]:[VTEP-IP]:[Frag-id]\nEVPN type-2 prefix: [2]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-4 prefix: [4]:[ESI]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\n                    Extended Community\nRoute Distinguisher: 1.1.1.1:2\n*&gt; [2]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                                0 65550 64521 i\n                    RT:1:10010 ET:8\n*  [2]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                                0 65550 64521 i\n                    RT:1:10010 ET:8\n\nDisplayed 1 prefixes (2 paths) (of requested type) \n\n// MAC address table\n\ncumulus@Leaf2:mgmt:~$ net show bridge macs 00:50:79:66:68:06\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags               LastSeen\n--------  ------  ---------  -----------------  ----------  -----  ------------------  --------\n10        bridge  vni10      00:50:79:66:68:06                     extern_learn        00:03:14\nuntagged          vni10      00:50:79:66:68:06  1.1.1.1            self, extern_learn  00:03:14\n</code></pre> <p>This is the first important thing to remember with auto-derived RTs on Cumulus Linux - there is an implicit *:VNI import when using auto-RTs. This is necessary because when you follow an eBGP peering model, the AS numbers will naturally be different and a ASN:VNI import model will not work when using your own ASN for the import RT. </p> <p>Let's add a manual, incorrect RT now on Leaf2:</p> <pre><code>cumulus@Leaf2:mgmt:~$ net add bgp l2vpn evpn vni 10010 route-target import 1:10\ncumulus@Leaf2:mgmt:~$ net commit\n</code></pre> <p>We no longer see the entry in the MAC address table anymore, even though BGP EVPN has received it:</p> <pre><code>cumulus@Leaf2:mgmt:~$ net show bridge macs 00:50:79:66:68:06                   \n\nVLAN  Master  Interface  MAC  TunnelDest  State  Flags  LastSeen\n----  ------  ---------  ---  ----------  -----  -----  --------\n* no output *\n\ncumulus@Leaf2:mgmt:~$ net show bgp l2vpn evpn route type 2 \nBGP table version is 11, local router ID is 2.2.2.2\nStatus codes: s suppressed, d damped, h history, * valid, &gt; best, i - internal\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nEVPN type-1 prefix: [1]:[ESI]:[EthTag]:[IPlen]:[VTEP-IP]:[Frag-id]\nEVPN type-2 prefix: [2]:[EthTag]:[MAClen]:[MAC]:[IPlen]:[IP]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-4 prefix: [4]:[ESI]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[EthTag]:[IPlen]:[IP]\n\n   Network          Next Hop            Metric LocPrf Weight Path\n                    Extended Community\nRoute Distinguisher: 1.1.1.1:2\n*&gt; [2]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                                0 65550 64521 i\n                    RT:1:10010 ET:8\n*  [2]:[0]:[48]:[00:50:79:66:68:06]\n                    1.1.1.1                                0 65550 64521 i\n                    RT:1:10010 ET:8\n\nDisplayed 1 prefixes (2 paths) (of requested type) \n</code></pre> <p>Once we add the correct RT to be imported, we see it in the mac table again:</p> <pre><code>cumulus@Leaf2:mgmt:~$ net add bgp l2vpn evpn vni 10010 route-target import 1:10010\ncumulus@Leaf2:mgmt:~$ net commit\n\ncumulus@Leaf2:mgmt:~$ net show bridge macs 00:50:79:66:68:06\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags               LastSeen\n--------  ------  ---------  -----------------  ----------  -----  ------------------  --------\n10        bridge  vni10      00:50:79:66:68:06                     extern_learn        00:00:17\nuntagged          vni10      00:50:79:66:68:06  1.1.1.1            self, extern_learn  00:00:17 \n</code></pre> <p>Remember, this import RT also controls what is inserted into the EVPN ARP cache. Assuming corresponding SVIs were deployed as well, you should see the EVPN ARP cache populated with this entry if the correct import RTs are configured (via the type-2 MAC plus IP route).</p> <pre><code>cumulus@Leaf2:mgmt:~$ net show evpn arp-cache vni 10010\nNumber of ARPs (local and remote) known for this VNI: 2\nFlags: I=local-inactive, P=peer-active, X=peer-proxy\nNeighbor        Type   Flags State    MAC               Remote ES/VTEP                 Seq #'s\n10.10.10.101    remote       active   00:50:79:66:68:06 1.1.1.1                        0/0\n10.10.10.102    local        active   00:50:79:66:68:07                                0/0  \n</code></pre>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/#rts-with-symmetric-irb","title":"RTs with symmetric IRB","text":"<p>Let's now convert our network to a symmetric IRB topology. Server2 is moved to VLAN 20, with an IP address of 20.20.20.102. VLANs 10 and 20 are present on both Leaf1 and Leaf2, acting as anycast gateways for their respective subnets. </p> <p></p> <p>Both Leaf1 and Leaf2 have VNIs created for VLANs 10 and 20. Example below from Leaf1:</p> <pre><code>interface vni10\n  bridge-access 10\n  mstpctl-bpduguard yes\n  mstpctl-portbpdufilter yes\n  vxlan-id 10010\n  vxlan-local-tunnelip 1.1.1.1\n\ninterface vni20\n  bridge-access 20\n  mstpctl-bpduguard yes\n  mstpctl-portbpdufilter yes\n  vxlan-id 10020\n  vxlan-local-tunnelip 1.1.1.1\n</code></pre> <p>A L3VNI (VNI 10040) is created for symmetric routing and mapped to VLAN 40. Each of the servers are moved into a new VRF, called VRF1. The L3VNI is mapped to this VRF as well. </p> <pre><code>interface vlan10\n  address 10.10.10.1/24\n  hwaddress 00:10:00:10:00:10\n  vlan-id 10\n  vlan-raw-device bridge\n  vrf VRF1\n\ninterface vlan20\n  address 20.20.20.1/24\n  hwaddress 00:20:00:20:00:20\n  vlan-id 20\n  vlan-raw-device bridge\n  vrf VRF1\n\ninterface vlan40\n  vlan-id 40\n  vlan-raw-device bridge\n  vrf VRF1\n</code></pre> <p>The same import RT logic applies for L3VNIs also - if there's no manual import configured for the L3VNI, then the default *:VNI import is applied. It is crucial to understand how the import/export RTs for the L3VNI is controlled - this is done under the VRF specific address-family in BGP. </p> <p>For example, before setting any manual import/export RTs, the L3VNI has auto-derived it:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net show bgp l2vpn evpn vni 10040\nVNI: 10040 (known to the kernel)\n  Type: L3\n  Tenant VRF: VRF1\n  RD: 20.20.20.1:3\n  Originator IP: 1.1.1.1\n  Advertise-gw-macip : n/a\n  Advertise-svi-macip : n/a\n  Advertise-pip: Yes\n  System-IP: 1.1.1.1\n  System-MAC: 50:00:00:03:00:03\n  Router-MAC: 50:00:00:03:00:03\n  Import Route Target:\n    64521:10040\n  Export Route Target:\n    64521:10040 \n</code></pre> <p>We'll now configure this manually instead, as an example, on Leaf1. This goes under the BGP configuration itself:</p> <pre><code>net add bgp vrf VRF1 autonomous-system 64521\nnet add bgp vrf VRF1 l2vpn evpn route-target import 2:10040\nnet add bgp vrf VRF1 l2vpn evpn route-target export 1:10040\n</code></pre> <p>Notice how these are specific to the VRF. Now, Leaf1 should be adding a RT of 1:10040 to the prefixes.  The final BGP configuration in this case:</p> <pre><code>router bgp 64521\n  bgp router-id 1.1.1.1\n  neighbor swp1 interface remote-as external\n  neighbor swp2 interface remote-as external\n\n  address-family ipv4 unicast\n    network 1.1.1.1/32 \n\n  address-family l2vpn evpn\n    neighbor swp1 activate\n    neighbor swp2 activate\n    advertise-all-vni\n\n    vni 10020\n      route-target import 2:10\n      route-target export 2:10\n\n    vni 10010\n      route-target import 1:10\n      route-target export 1:10\n\nrouter bgp 64521 vrf VRF1\n\n  address-family l2vpn evpn\n    route-target export 1:10040\n    route-target import 2:10040\n</code></pre> <p>Looking at the BGP EVPN table, we can see the RTs are correctly added:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net show bgp l2vpn evpn route rd 1.1.1.1:3 type 2\nEVPN type-1 prefix: [1]:[ESI]:[EthTag]:[IPlen]:[VTEP-IP]:[Frag-id]\nEVPN type-2 prefix: [2]:[EthTag]:[MAClen]:[MAC]\nEVPN type-3 prefix: [3]:[EthTag]:[IPlen]:[OrigIP]\nEVPN type-4 prefix: [4]:[ESI]:[IPlen]:[OrigIP]\nEVPN type-5 prefix: [5]:[EthTag]:[IPlen]:[IP]\n\nBGP routing table entry for 1.1.1.1:3:[2]:[00:50:79:66:68:06]/352\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:50:79:66:68:06] VNI 10010/10040\n  Local\n    1.1.1.1 from 0.0.0.0 (1.1.1.1)\n      Origin IGP, weight 32768, valid, sourced, local, bestpath-from-AS Local, best (First path received)\n      Extended Community: ET:8 RT:1:10 RT:1:10040 Rmac:50:00:00:03:00:03\n      Last update: Fri Jul 30 09:43:56 2021\nBGP routing table entry for 1.1.1.1:3:[2]:[00:50:79:66:68:06]:[10.10.10.101]/352\nPaths: (1 available, best #1)\n  Advertised to non peer-group peers:\n  Spine1(swp1) Spine2(swp2)\n  Route [2]:[0]:[48]:[00:50:79:66:68:06]:[32]:[10.10.10.101] VNI 10010/10040\n  Local\n    1.1.1.1 from 0.0.0.0 (1.1.1.1)\n      Origin IGP, weight 32768, valid, sourced, local, bestpath-from-AS Local, best (First path received)\n      Extended Community: ET:8 RT:1:10 RT:1:10040 Rmac:50:00:00:03:00:03\n      Last update: Fri Jul 30 09:43:56 2021\n\nDisplayed 2 prefixes (2 paths) with this RD (of requested type) \n</code></pre> <p>There are two distinct RTs added here - one for the corresponding L2VNI and another for the L3VNI. </p> <p>It is important to understand the impact of importing each RT - on Leaf2, importing the RT for the L3VNI is what imports the /32 route (from the type-2 MAC plus IP route) into the VRF routing table, while importing the L2VNI will pull the MAC address into the MAC address table (this is done using the type-2 MAC only route) and create an entry in the EVPN ARP cache (using the type-2 MAC plus IP route).</p> <p>Let's confirm on Leaf2:</p> <pre><code>// MAC address table\n\ncumulus@Leaf2:mgmt:~$ net show bridge macs 00:50:79:66:68:06\n\nVLAN      Master  Interface  MAC                TunnelDest  State  Flags               LastSeen\n--------  ------  ---------  -----------------  ----------  -----  ------------------  --------\n10        bridge  vni10      00:50:79:66:68:06                     extern_learn        00:00:43\nuntagged          vni10      00:50:79:66:68:06  1.1.1.1            self, extern_learn  00:00:43\n\n// EVPN ARP cache\n\ncumulus@Leaf2:mgmt:~$ net show evpn arp-cache vni 10010     \nNumber of ARPs (local and remote) known for this VNI: 1\nFlags: I=local-inactive, P=peer-active, X=peer-proxy\nNeighbor        Type   Flags State    MAC               Remote ES/VTEP                 Seq #'s\n10.10.10.101    remote       active   00:50:79:66:68:06 1.1.1.1                        0/0 \n\n// VRF1 route table\n\ncumulus@Leaf2:mgmt:~$ net show route vrf VRF1 ipv4          \nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR, f - OpenFabric,\n       &gt; - selected route, * - FIB route, q - queued, r - rejected, b - backup\n       t - trapped, o - offload failure\n\nVRF VRF1:\nK&gt;* 0.0.0.0/0 [255/8192] unreachable (ICMP unreachable), 01:46:41\nC&gt;* 10.10.10.0/24 is directly connected, vlan10, 01:46:41\nB&gt;* 10.10.10.101/32 [20/0] via 1.1.1.1, vlan40 onlink, weight 1, 00:21:36\nC&gt;* 20.20.20.0/24 is directly connected, vlan20, 01:46:41 \n</code></pre> <p>On Leaf2, as you can see, we have enough information to route both asymmetrically and symmetrically. Remember, the lookup is always a longest prefix match - which means the /32 route is hit and the packet is routed symmetrically. </p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2021/12/14/cumulus-part-ix---understanding-vxlan-evpn-route-target-control/#controlling-rts","title":"Controlling RTs","text":"<p>Knowing what we know of RT import/export, we can fully control how we want our traffic to flow (asymmetric or symmetric).</p> <p>On Leaf1, let's import a different RT under the VRF address-family.</p> <pre><code>router bgp 64521\n  bgp router-id 1.1.1.1\n  neighbor swp1 interface remote-as external\n  neighbor swp2 interface remote-as external\n\n  address-family ipv4 unicast\n    network 1.1.1.1/32 \n\n  address-family l2vpn evpn\n    neighbor swp1 activate\n    neighbor swp2 activate\n    advertise-all-vni\n\n    vni 10020\n      route-target import 2:10\n      route-target export 2:10\n\n    vni 10010\n      route-target import 1:10\n      route-target export 1:10\n\nrouter bgp 64521 vrf VRF1\n\n  address-family l2vpn evpn\n    route-target export 1:10040\n    route-target import 2:10041 \n</code></pre> <p>This causes the type-2 MAC plus IP route to not be imported as a /32 route in the VRF table. Now, the longest prefix match is the subnet route itself:</p> <pre><code>cumulus@Leaf1:mgmt:~$ net show route vrf VRF1 ipv4\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n       F - PBR, f - OpenFabric,\n       &gt; - selected route, * - FIB route, q - queued, r - rejected, b - backup\n       t - trapped, o - offload failure\n\nVRF VRF1:\nK&gt;* 0.0.0.0/0 [255/8192] unreachable (ICMP unreachable), 07:27:56\nC&gt;* 10.10.10.0/24 is directly connected, vlan10, 07:27:56\nC&gt;* 20.20.20.0/24 is directly connected, vlan20, 07:27:56 \n</code></pre> <p>This means that the destination is directly connected to Leaf1 and it can ARP for it. Using the EVPN ARP cache, Leaf1 already knows PC2s mac address, and there's no need to ARP for it again. Thus, PC1 to PC2, traffic will flow asymmetrically. A packet capture confirms that the VNI added to the VXLAN header is 10020.</p> <p></p> <p>The return path is symmetric because Leaf2 still has that /32 entry imported into the VRF table. A packet capture confirms that the return packet has the L3VNI (10040) added to the VXLAN header. </p> <p></p> <p>I hope this was informative, and I'll see you in the next one.</p>","tags":["cumulus","vxlan","evpn"]},{"location":"blog/2022/06/30/juniper-apstra-part-i---introducing-a-true-ibns/","title":"Juniper Apstra Part I - Introducing a true IBNS","text":"<p>With this post, we kick off a new series based on Juniper Apstra. This post serves as an introduction to Apstra - we'll look at what a true IBN system is, how relational and graph databases are different (and why graph databases are ideal for network infrastructure), concluding with some general workflows in Apstra.</p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/06/30/juniper-apstra-part-i---introducing-a-true-ibns/#introduction","title":"Introduction","text":"<p>It's that time again - time for a new series! And I am really excited for this one. I am going to be kicking off a Juniper Apstra series with this first post. This post will look at what Intent Based Networking truly is, and what an Intent Based Networking System (IBNS) looks like.</p> <p>We'll then take a bit of a detour and talk about databases - I felt it was important to (at minimum) gain a basic understanding of relational and graph databases and why one is better suited than the other for network infrastructure. Graph databases form a crucial pillar of Apstra.</p> <p>Finally, we'll close this out by looking at some generic workflows within Apstra - I've broken this down into simple, high level flows without going into too much detail. Not to worry, the details will come in subsequent posts. </p> <p>Let's get started!</p> <p>Disclaimer</p> <p>As a disclaimer, I'd like to say that a lot of what I've written about IBN and IBN systems here, comes from reading papers and book(s) written by the fantastic Jeff Doyle. There is no intent to plagiarize his work.</p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/06/30/juniper-apstra-part-i---introducing-a-true-ibns/#apstra-the-what-and-the-why","title":"Apstra - the what and the why","text":"<p>Juniper Apstra is a multivendor solution for Data Center automation and insights. It falls under the general bracket of Intent Based Networks/Networking (IBN) - this is a term thrown around by many vendors today, but Apstra really exemplifies this and is essentially an IBN system. The goal is to maintain and validate the network as a whole, and not just individual components of the network. </p> <p>The network itself is driven by intent - intent that is provided by the end user, but converted into deployable network elements by Apstra. As a user, the intent is captured in the form of various inputs - how many racks in a DC, how many leafs per rack, how many (and what kind) of systems are connected to these leafs, what kind of redundancy is required per rack and so on. This is then converted into vendor specific configuration by Apstra (thus providing a level of abstraction), and pushed via netconf over ssh to the respective devices to bring your intent to life. </p> <p>In his book called 'IBN for Dummies', Jeff Doyle has a great statement about IBN systems - \"self-operates, self-adjusts and self-corrects within the parameters of your expressed technical objective\". What is the expressed technical objective? It is nothing but the intent.  </p> <p>As an IBN system, Apstra provides/is:</p> <ol> <li>Base automation and orchestration - conversion of intent into vendor specific configuration, eliminating any human error and conforming to reference designs built for different deployment models.</li> <li>A single source of truth - this is important because you cannot have uniform and non-conflicting intent with multiple sources of truth. Idempotency can only exist with a single source of truth. In the case of Apstra, the 'Blueprint' is the source of truth.</li> <li>Closed loop validation - this is continuous verification, in real time, of current network state with intended state, to flag any potential deviation. This eventually leads to self correction and self operation.</li> <li>Self operation - Only through closed loop validation, can the network self correct or at minimum, alert the user that the network is out of compliance and provide possible remediation steps. </li> </ol> <p>These, as you may have guessed, are the main tiers of an IBN system as well. </p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/06/30/juniper-apstra-part-i---introducing-a-true-ibns/#topology","title":"Topology","text":"<p>This is the topology that we'll be working with for a majority of this series. Juniper Apstra is installed as a thin VM on ESXi, and is connected to all network devices via an OOB network. This is crucial to remember because Apstra only offers an OOB connection and nothing else as of today. </p> <p>There are three Juniper vQFXs acting as leafs, 2 vQFXs as spines, one Arista vEOS as a leaf and one Cisco NXOSv as a leaf. </p> <p></p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/06/30/juniper-apstra-part-i---introducing-a-true-ibns/#a-brief-look-at-databases","title":"A brief look at databases","text":"<p>Databases are broadly divided into relational databases and non-relational databases. It is important to understand what a graph database is (since this is a founding pillar that Apstra was built on), and how it is different from relational databases. In this section, we'll take a brief look at relational databases and how they are structured, and then understand how graph databases are different and why they are ideal for network infrastructure.</p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/06/30/juniper-apstra-part-i---introducing-a-true-ibns/#relational-databases","title":"Relational databases","text":"<p>Relational databases (also called as SQL databases) are structured as tables (rows and columns). These are typically fixed schemas, and used for data that does not change very often. Let's take a simple example: we have a table called 'networkdevice' that is a database for network inventory.</p> <p>There are several fields in this database that describe a network device - hostname, software version, management IP address. This can be visualized as so:</p> <p></p> <p>Naturally, there are many other pieces of data that we might want to store or link to a network device. It doesn't make sense to have everything in one table itself. This is where relationships are built, and hence the term \"relational\" database.</p> <p>For example, let's say we'd like to have some device specific settings like AAA enabled or not, site name or site ID, its role in the network (spine/leaf/other) and so on. We could have another table for this called 'devicesettings' that has all of this information, and build a relationship between this table and our 'networkdevice' table. This is done using something called as 'foreign keys'. A foreign key is simply a column that uniquely identifies a row in another table.</p> <p>Let's add another column in our 'networkdevice' table, and we'll call it 'device_id'. This is the foreign key that will connect our two tables together. Visually, this would be something like this:</p> <p></p> <p>Relational databases, despite the name, are not great at capturing data that is connected. Because multiple tables need to be queried to get the final data, the computation becomes increasingly expensive. This is usually okay when only a low number of tables exist, but this is hardly the case in real world deployments - the number of tables in use, and related, will likely be quite high. </p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/06/30/juniper-apstra-part-i---introducing-a-true-ibns/#graph-databases","title":"Graph databases","text":"<p>Graph databases are a type of NoSQL databases (non-relational databases), which are ideal for navigating connected data. They are essentially nodes linked via edges, where the edges are called 'relationships'. Relationships are treated as first class citizens - it has a relationship first approach to storing and querying data. </p> <p>One of the biggest factors for using graph databases for network infrastructure is that graphs make it very easy to do dependency analysis and they are very good at finding out indirect relationships - what is the impact of shutting a link, bringing down a node, making a policy change and so on. A natural extension of this is that RCA is greatly improved - feeding a graph into machine learning pipelines generates far better outcomes and decisions as compared to feeding in isolated data sets. </p> <p>For the scope of our discussions here, we will look at something called as labeled property graphs. Property graphs allow you to store properties against a node, as well as the relationship itself. These are stored in a 'key:value' format. They are also called as 'labeled' property graphs because you can assign labels to a node, describing the 'type' of the node. </p> <p>Visually, this looks like so:</p> <p></p> <p>I didn't want to reinvent the wheel, so the above is just a stripped down version from a data center deployment (which we'll look at) in Apstra. I've just focused on two nodes and the relationship between them. </p> <p>The nodes here are a 'system' and an 'interface'. The relationship between them is 'hosted interfaces', essentially implying that the system (with its listed properties) has an interface (with its listed properties). </p> <p>As you can see, the 'system' node has 'key:value' properties assigned to it - these include a base identifier called 'id', the name of the system ('spine1', in this case), its role in the network and so on. Similarly, the 'interface' node has some 'key:value' properties assigned to it as well - again, an identifier, the interface name, IPv4 address and so on.</p> <p>Finally, the relationship has some 'key:value' properties also, largely to specify the direction, indicated via a source ID (which matches the 'system' node) and a destination ID (which matches the 'interface' node).</p> <p>These are just two nodes that I've shown as an example - the entire blueprint that is deployed in Apstra is instantiated as a graph that can be traversed. We'll take a more detailed look at it once we actually deploy a blueprint in later posts.</p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/06/30/juniper-apstra-part-i---introducing-a-true-ibns/#navigating-the-gui","title":"Navigating the GUI","text":"<p>This is your first look at Juniper Apstra (at least the first within the context of this blog post). This is Apstra version 4.1.0. </p> <p></p> <p>On this main page, a simple workflow is provided - build racks, design the network, create and deploy a blueprint. Of course, it is a little more involved than this and we're going to break it down in this post, and several others that will follow.</p> <p>On the left hand side, there is a pane with several elements. A lot of the work is done within the 'Devices', 'Design' and 'Resources' tab. In this post, we're going to focus on these only. </p> <p>The 'Devices' tab has the following options:</p> <p></p> <p>Here, you can create agents for devices that need to be onboarded (we'll detail out the process in a subsequent post), create agent profiles, create or view device profiles and so on.</p> <p>The 'Design' tab has the following options:</p> <p></p> <p>'Design' is where you start to sculpture what your data center will look like. We'll look at this in more detail shortly as well. </p> <p>Finally, 'Resources' are fairly straightforward:</p> <p></p> <p>These are various resources that Apstra will use to automate your data center deployment. </p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/06/30/juniper-apstra-part-i---introducing-a-true-ibns/#general-workflow-within-apstra","title":"General workflow within Apstra","text":"<p>The first step in any SDN system or IBN system is typically to get devices registered and onboarded in order to be able to manage them. </p> <p>A high level workflow for this is as follows:</p> <p></p> <p>In Apstra, devices are onboarded and managed through something called 'Agents'. You can also create different 'Agent Profiles' to cater to different kinds of network operating systems, vendors and logins. </p> <p>Once your devices are in the system, you get to the 'designing' part. Here, you are largely building a rack or racks for your data center. There are several parameters that are fed into a rack and this is fairly involved but very logical. </p> <p>A rack will typically have one or more leafs, it may or may not have redundancy between these leafs, there may be some access switches connected to the leafs and finally there will be some hosts in the rack. All of these are options that you control and define. The 'LD' in the workflow above stands for 'Logical Device' - we'll look at all of this and how to define these options in more detail in subsequent posts.</p> <p>Once a rack (or racks) have been created, you can move onto creating a template for your data center. The template has several feeds as well, including many important options for your data center - ASN allocation, overlay and underlay details, leaf and spine information and so on. </p> <p></p> <p>Finally, once a template is ready, you can create a blueprint for your data center. The blueprint is built from the template itself, and takes in various resources to start mapping out actual tangible and deployable network elements. These are things like Autonomous System numbers, IPs pools, interface maps and so on.</p> <p></p> <p>In the next post, we'll actually start to do all of this - we'll design our racks, build out a template, create all these resources that we just talked about. All of the fun stuff! Stay tuned, lot's more to follow!</p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/06/30/juniper-apstra-part-i---introducing-a-true-ibns/#references","title":"References","text":"<ol> <li>IBN for Dummies by Jeff Doyle.</li> </ol>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/07/10/juniper-apstra-part-ii---building-a-data-center-rack/","title":"Juniper Apstra Part II - Building a data center rack","text":"<p>In this post, we'll start designing the building blocks for our data center deployment with Juniper Apstra. We'll look at how to design a rack.</p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/07/10/juniper-apstra-part-ii---building-a-data-center-rack/#introduction","title":"Introduction","text":"<p>Designing a rack is a fairly involved process. To be completely thorough, we're going to define new logical devices, create interface maps for these and then put it all together to build a rack. Naturally, some of these terms (logical devices, interface maps and so on) are Apstra specific - not to worry, we'll break it all down and understand what these are and how they are used. </p> <p>Before we dive into this, I'd like to talk about why Apstra does things this way and what the methodology is. Why build these abstracted models? Why racks? </p> <p>When you're building out a data center (physically), you'd know that there are a lot of things that simply repeat. Racks are usually identical - or at best you have a few types of racks servicing specific things, and those types of racks repeat. This can include how many leafs per rack, the redundancy model is usually common across the organization, how many servers and so on. You're typically buying identical physical racks as well (like a 42RU rack) and so on. </p> <p>This repeatability also drives consistency - the more \"unique\" data points you add in your network, the more complex it becomes. And complexity means more potential outages, slower time to resolution when issues do arise. Simple is good. </p> <p>Apstra just took these repeatable units and made logical, abstracted models of these. It's a simple, natural extension of how your physical data center would look, which makes it so easy to work with, and understand what Apstra is doing and why.</p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/07/10/juniper-apstra-part-ii---building-a-data-center-rack/#topology","title":"Topology","text":"<p>Our topology is the same one we used for Part I of this series. We're going to be focusing on just the vQFX devices and leave out the Arista vEOS and Cisco N9Kv for now (multivendor via Apstra will be a later post).</p> <p></p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/07/10/juniper-apstra-part-ii---building-a-data-center-rack/#designing-a-data-center-rack","title":"Designing a data center rack","text":"<p>Note</p> <p>Our reference for this entire process is based on Apstra 4.1.0.</p> <p>Let's go back to the workflow we'd described in the first post:</p> <p></p> <p>This is what we're going to detail out here, and understand various components involved in building a rack. </p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/07/10/juniper-apstra-part-ii---building-a-data-center-rack/#agents-and-agent-profiles","title":"Agents and Agent Profiles","text":"<p>To take action on any device (or collect data from any device), the device must first be known to the system (Apstra, in this case) - it must be a part of the inventory. In Apstra, this is facilitated by something called as 'Agents'. </p> <p>Agents can be of two types:</p> <ol> <li>On-box - these are agents that are installed directly on the network device itself (applicable only if supported by the network operating system).</li> <li>Off-box - these are agents installed on the Apstra server, as a container, for a particular network device. </li> </ol> <p>A complete compatibility list (what network operating system supports what type of agent) is available in the official Apstra documentation, found here. Navigate to the 'User Guide'. </p> <p>Juniper supports only the off-box agent as of Apstra 4.1.0, and detailed instructions regarding the initial configuration required for the off-box agent to be installed can be found here for Apstra 4.1. </p> <p>If you don't want to read the guide, here's a quick summary: a super-user that Apstra will use to login to the system, ssh and netconf over ssh, and finally reachability to the network device from Apstra.</p> <pre><code>{master:0}[edit]\nroot@vqfx# show system login \nuser aninchat {\n    uid 2000;\n    class super-user;\n    authentication {\n        encrypted-password \"$6$CR8rOdvC$fZiYRGtALKZNGU1hWCyoNg5fqxqyJlANacEwEfRPHLawUWzzpMLdMw1NpR5Z3//hPckVZbTjYFkCHPBdaKzxo.\"; ## SECRET-DATA\n    }\n}\n\n{master:0}[edit]\nroot@vqfx# show system services \nssh {\n    root-login allow;\n}\nnetconf {\n    ssh;\n}\n</code></pre> <p>As part of the device discovery, some basic parameters are needed, which are a part of constructing the Agent itself - this includes the platform (junos/eos/nxos), the IP address to reach the device and the username/password for logging into the device. </p> <p></p> <p>As you can imagine, this can get pretty tedious if you have to choose the platform and type in the username/password every single time for every new device that needs to be onboarded. This is where Agent Profiles are useful - you can create a profile for a specific type of device and just use the profile whenever that type of network device needs to be onboarded.</p> <p>As an example, let's create an Agent Profile for my vQFX devices that need to be onboarded:</p> <p></p> <p>You can see that I also have similar profiles created for Arista vEOS and Cisco N9Kv:</p> <p></p> <p>Now, I can type in the IP addresses of the devices I want to onboard (my two vQFX spines, and three vQFX leafs, in this case) and choose the vQFX profile.</p> <p></p> <p>These devices are now listed under 'Managed Devices' under the main 'Devices' tab. While Apstra makes contact with the devices that need to be onboarded, and collect data that it needs to onboard it, you can view the logs for any device that is being onboarded (or was onboarded already). </p> <p>Just click on the button with the three dots (extreme right of each device row), and you get a set of actions available to you for that particular device. One of these actions is 'Show Log'.</p> <p></p> <p>The logs from a successful onboard of a vQFX device looks something like this:</p> <p></p> <p>As part of the process, Apstra collects the configuration from the device and stores it as the pristine configuration - this is the baseline configuration without any services and features added to the device, apart from what was needed to get the device onboarded into Apstra. The Apstra state for the device at this point is called 'OOS-QUARANTINED' where 'OOS' stands for 'Out Of Service'.</p> <p>The entire device lifecycle can be found here. Once a device is onboarded, it needs to be explicitly acknowledged by Apstra - this acknowledgment kicks off the 'Discovery 1' process where LLDP is enabled on all interfaces. You can acknowledge a device by selecting the device (or selecting a subset of devices) and clicking on the 'acknowledge selected systems' button, and a subsequent confirmation:</p> <p></p> <p></p> <p>At this point, connected devices should see each other as LLDP neighbors. The Apstra state for the device moves from 'OOS-QUARANTINED' to 'OOS-READY' which essentially implies that the device is out of service, but it is ready and can now be added into a blueprint.</p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/07/10/juniper-apstra-part-ii---building-a-data-center-rack/#logical-devices","title":"Logical Devices","text":"<p>Logical Devices, as the name suggests, are just a logical or abstracted representation of your network device. The important thing to remember is that this is not vendor specific at all - you're not choosing Cisco or Arista or Juniper with a logical device. You're simply stating that you'd like a device that has, for example, 24x10G ports, with 4x100G uplinks. Or whatever other speed/port combination you're looking for.</p> <p>A logical device also allows you to specify what kind of devices these ports can be connected to and used for - again, for example, you'd like your 4x100G uplinks to be used for connections to a spine, and your 24x10G ports as connections to servers.</p> <p>Logical devices can be found under 'Design'. There are numerous logical devices that are already pre-built, covering a large range of existing (and supported) network devices from different vendors such as Cisco, Arista and Juniper. </p> <p></p> <p>We're going to create two new logical devices to go through the workflow: one for a vQFX leaf, and another for a vQFX spine. We're also going to be specific with what ports can connect to what network devices.</p> <p>We're going to have 6 ports available on the leaf (10G each) to connect to spines:</p> <p></p> <p>Click on the 'Create Port Group' option to create a port group for these 6x10G ports. </p> <p></p> <p>You can continue to add more ports by dragging the port group further. We're going to add another 6 ports that can be used to connect to the hosts. Hosts can be selected as 'Generic':</p> <p></p> <p>You should now have a 12x10G vQFX logical device:</p> <p></p> <p>Click on 'Create' to create this logical device. We're going to do the same thing for our vQFX spines, but all ports are going to be marked as connected to leafs.</p> <p></p> <p>Create the port group, and click on create just like before to build this vQFX spine logical device:</p> <p></p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/07/10/juniper-apstra-part-ii---building-a-data-center-rack/#device-profiles","title":"Device Profiles","text":"<p>Device Profiles are where the actual hardware is defined. This includes (among other things): </p> <ul> <li>what kind of vendor specific model the hardware is (PID) </li> <li>the network operating system </li> <li>what versions to match on (based on a regular expression) </li> <li>CPU and RAM on the device </li> <li>the kind of ASIC it supports</li> <li>CoPP support</li> <li>interfaces, the kind of connector type for it, and the naming convention</li> </ul> <p>For vQFX (as well as vEOS and N9Kv), a device profile already exists, so we're not going to create a new one. But I'd still like to walk you through some of these options to show you how it can be done, particularly how the ports are defined via \"transformations\".</p> <p>Device Profiles can be found under 'Devices'. The main page for this lists out all of the available device profiles, with a search bar to filter as you wish.</p> <p></p> <p>If you'd like to create a new profile, you're met with the following page and options:</p> <p></p> <p>The selector option lets you specific more vendor specific information:</p> <p></p> <p>I want to focus on the 'Ports' section for a little bit, as that is very interesting. Here, you're greeted with a famiilar looking UI (this is similar to how we created logical devices). By default, one panel is present which is what you'd have for a non-modular network device. Considering our vQFX example, this panel should have 12 ports:</p> <p></p> <p>You can now select a port, a subset of ports or all the ports to associate properties against them, like the connector type (SFP, QSFP and so on). For example, I have selected port 1 (by simply clicking on it):</p> <p></p> <p>You also need to associate a \"transformation\" against the port. This is how the interface naming is actually generated (which, as you can imagine, can be vastly different from vendor to vendor), as well as the speed of the interface. Clicking on 'Add new transformation' lets you do this. </p> <p></p> <p>Before we move on from this, I'd like to show you the pre-built vQFX device profile:</p> <p></p> <p>These are 12x10G ports, with the transformation defining the interface naming as xe-0/0/0 for port 1 and so on, which is what we expect. </p> <p>The transformations do more than just this - these are extremely crucial for defining breakouts as well. You can simply have multiple transformations defined which allows you to create port breakouts - for example, let's say the actual vendor hardware you buy allows for a 40G port to be split into 4x10G ports. How would you deal with this in Apstra? You would simply create two transformations - one for the 4x10G ports and another for the 1x40G port.</p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/07/10/juniper-apstra-part-ii---building-a-data-center-rack/#interface-maps","title":"Interface Maps","text":"<p>Interface Maps are fairly straightforward, but extremely important. A logical device and a device profile feed into it, and are used to create an interface map. This is where your logical abstraction of a device, and it's hardware specific information all comes together to form an actual deployable network device in your blueprint (which we'll talk about in the next post).</p> <p>From the UI, this is found under 'Design' again. Like I said, an interface map just takes in two feeds - a logical device, and a device profile.</p> <p></p> <p>For our case, we're going to create two interface maps - one for the vQFX leaf and another for the vQFX spine. Starting with the leaf, we have our leaf logical device and the vQFX device profile as the input. Once this is done, there is some additional selection that needed - this is the port selection for the port groups that were created within the logical device:</p> <p></p> <p>The 'Select Interface' is kind of a drop down, which when clicked, lists all the available ports that can be selected (and mapped using the device profile) for that port group.</p> <p></p> <p>This can be a little confusing, since it might seem like we did this exercise while creating a logical device as well, but think of it as this way - with the logical device, you're only defining the number of ports, their speeds, and what they are allowed to be connected to. In the interface map (and using the device profile), you're actually selecting which ports can do what and mapping them to a vendors standard naming convention (using a transformation that was defined in the device profile) for that kind of port.</p> <p></p> <p>So, in our case, I'd like the first 6 ports to be available as my spine connections, and use transformation #1 which was the default transformation present in the pre-built vQFX device profile. Let's select those:</p> <p></p> <p>The remaining 6 ports are now chosen for my host/server connections:</p> <p></p> <p>Finally, as a confirmation, if you scroll down a bit during the interface map creation, you can select any port to see a summary of what it would look like when actually deployed. I've selected port 1, which tells me it is port #1 of panel #1, and it is named xe-0/0/0.</p> <p></p> <p>For our spine interface map, a similar process can be followed. In this case, there is just one port group with all 12 ports, and we select all of them with the same default transformation #1 from the pre-built vQFX device profile.</p> <p></p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/07/10/juniper-apstra-part-ii---building-a-data-center-rack/#putting-it-all-together-to-build-a-rack","title":"Putting it all together to build a rack","text":"<p>Well, we're finally here. We've got all our building blocks setup, and it's time to actually build a data center rack. </p> <p>Racks are found under 'Design' and like everything else, there are several pre-built racks for you. To demonstrate how it's done, we're going to build a rack from scratch. Our goal is to build a rack that has one vQFX leaf and two hosts attached to it. It is minimal, and a little unrealistic, but the aim is to largely show you how these different pieces fit together and what the workflow looks like for this.</p> <p>We're going to give our rack a name - '1L vQFX12x10G', which makes it easy for me to understand that this is a one leaf rack, where the leaf is a vQFX with 12x10G ports. We want to build a 3-stage Clos fabric, so we're going to choose the 'L3 Clos' option. Alternatively, you can choose the 'L3 Collapsed' design, where there are no spines.</p> <p></p> <p>Now we can move into the configuration of the rack - this is where you define the number of leafs, kind of leafs, how many hosts, if there are any access switches (that sit between the leaf and the hosts)) and so on.</p> <p></p> <p>The main input for the leafs here are:</p> <ul> <li>a leaf logical device</li> <li>the redundancy model (none, MLAG or ESI)</li> <li>how many links per spine (these are taken from how many ports you've assigned as a connection to a spine)</li> <li>what kind of port speed for your spine facing links</li> </ul> <p>Let's provide these inputs for our rack:</p> <p></p> <p>Because this is a one leaf rack, the redundacy model chosen is simply none. But watch what happens when you choose ESI:</p> <p></p> <p>The number of leafs automatically becomes two. Any guesses on why MLAG is greyed out and cannot be selected? Go back to our logical device for this leaf - we never marked any ports that could be connected to another leaf, which naturally eliminates MLAG as an option since that requires a leaf to leaf connection.</p> <p>Now that our leaf is defined, we need to declare the generic systems that will be present in the rack (and how they will be connected to the leaf).</p> <p></p> <p>We'd like to have two hosts per rack, and each link is a 10G link back up to the leaf:</p> <p></p> <p>As part of the generic system definition, you need to also define properties of the link that will connect the host and the leaf. 'Add logical link' at the bottom allows you to do this. Here, you can configure how many links per host, if there is a need for LACP for link aggregation and so on.</p> <p></p> <p>And finally, you can click on 'Create' to create this rack. This is what it looks like visually:</p> <p></p> <p>This visual representation of your rack is so cool! It shows you exactly what you built, what it looks like, and if you scroll further down, it shows you each element that was used to build this entire rack.</p> <p>In the next post, we're going to build a template, and then stage and deploy our data center.</p>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/07/10/juniper-apstra-part-ii---building-a-data-center-rack/#references","title":"References","text":"<ol> <li>Apstra 4.1 documentation</li> </ol>","tags":["juniper","apstra","evpn","vxlan","bgp"]},{"location":"blog/2022/02/16/junos-part-ii---bgp-and-bgp-unnumbered/","title":"Junos Part II - BGP and BGP Unnumbered","text":"<p>In this post, we look at BGP on Junos OS and a typical BGP configuration for the underlay, for a 3-stage Clos fabric. We also introduce BGP unnumbered, which is a great way of building the underlay, without the need of any IP addressing. </p>","tags":["juniper","junos","bgp"]},{"location":"blog/2022/02/16/junos-part-ii---bgp-and-bgp-unnumbered/#introduction-and-topology","title":"Introduction and topology","text":"<p>The goal of these introductory Junos posts is to get familiar with some basic configuration for commonly used protocols, since it may be a bit of a transition if you're coming from the Cisco/Arista world. BGP is a big one, especially on the Data Center front - it is most often used as the underlay and overlay (for EVPN). To that end, it is extremely important to understand how to configure and process BGP on Junos OS.</p> <p>We're going to be working with the following topology for this post:</p> <p></p> <p>This is a typical 3-stage Clos design, with T0 being the leaf layer and T1 being the spine layer.</p>","tags":["juniper","junos","bgp"]},{"location":"blog/2022/02/16/junos-part-ii---bgp-and-bgp-unnumbered/#breaking-down-a-simple-bgp-configuration","title":"Breaking down a simple BGP configuration","text":"<p>BGP configuration is done in 'groups' - think of this as something similar to peer-groups in the Cisco world. I have a group called 'underlay' which defines two neighbors - 172.100.100.1 and 172.100.100.3, which are my p2p ends of the leaf links to the spines. I also have the peer AS number defined - since this is not specific to any neighbor, all neighbors inherit this peer AS and this does not have to be defined per neighbor.</p> <pre><code>root@Leaf1# show protocols bgp \ngroup underlay {\n    peer-as 65500;\n    neighbor 172.100.100.1 {\n        family inet {\n            unicast;\n        }\n    }\n    neighbor 172.100.100.3 {\n        family inet {\n            unicast;\n        }\n    }\n}\n</code></pre> <p>Outside of this, we also have some common parameters defined under 'routing-options':</p> <pre><code>root@Leaf1# show routing-options \nrouter-id 192.168.100.1;\nautonomous-system 64521;\n</code></pre> <p>This is where we have our local AS number and the router ID defined. These can always be overridden under the BGP configuration, specific to a neighbor. However, if there is no configuration for this under BGP, these routing-option parameters are used. This allows for an easy way to declare the AS number and router ID (among other things) for your system globally, instead of having to specify this every time for every potential protocol that needs it. </p> <p>Let's look at the configuration from one of the spines as well:</p> <pre><code>root@Spine1# show protocols bgp \ngroup underlay {\n    neighbor 172.100.100.0 {\n        family inet {\n            unicast;\n        }\n        peer-as 64521;\n    }\n    neighbor 172.100.100.4 {\n        family inet {\n            unicast;\n        }\n        peer-as 64522;\n    }\n}\n</code></pre> <p>There's not a whole lot that's different here - the only thing is that we have the peer-as defined per neighbor, since each leaf is in its own AS. At this point, I have IPv4 unicast (or inet unicast peering in Juniper terminology) peering up between all the spines and the leafs.</p> <pre><code>// Spine1\n\nroot@Spine1# run show bgp summary \nThreading mode: BGP I/O\nDefault eBGP mode: advertise - accept, receive - accept\nGroups: 1 Peers: 2 Down peers: 0\nTable          Tot Paths  Act Paths Suppressed    History Damp State    Pending\ninet.0               \n                       0          0          0          0          0          0\nPeer                     AS      InPkt     OutPkt    OutQ   Flaps Last Up/Dwn State|#Active/Received/Accepted/Damped...\n172.100.100.0         64521        149        149       0       0     1:06:35 Establ\n  inet.0: 0/0/0/0\n172.100.100.4         64522        149        148       0       0     1:06:33 Establ\n  inet.0: 0/0/0/0\n\n// Spine2\n\nroot@Spine2# run show bgp summary \nThreading mode: BGP I/O\nDefault eBGP mode: advertise - accept, receive - accept\nGroups: 1 Peers: 2 Down peers: 0\nTable          Tot Paths  Act Paths Suppressed    History Damp State    Pending\ninet.0               \n                       0          0          0          0          0          0\nPeer                     AS      InPkt     OutPkt    OutQ   Flaps Last Up/Dwn State|#Active/Received/Accepted/Damped...\n172.100.100.2         64521        141        140       0       0     1:03:13 Establ\n  inet.0: 0/0/0/0\n172.100.100.6         64522        141        141       0       0     1:03:15 Establ\n  inet.0: 0/0/0/0\n</code></pre>","tags":["juniper","junos","bgp"]},{"location":"blog/2022/02/16/junos-part-ii---bgp-and-bgp-unnumbered/#advertising-prefixes-into-bgp","title":"Advertising prefixes into BGP","text":"<p>Coming from Cisco, I was hunting for the 'network' statement to advertise prefixes into BGP. This is done a little differently in Junos OS, so it's important to cover this as well. Export policies are how you do this in Junos OS - you can have multiple export policies and this is evaluated from left to right. The first matching policy is applied.</p> <p>Outside of this, the export policy can be applied at various levels of the hierarchy. The lower you go in the hierarchy, the more specific/explicit the policy is considered to be. The most specific/explicit policy always wins. For example, consider several export policies applied as below:</p> <pre><code>bgp {\n    export policy_a;\n    group underlay {\n        export policy_b;\n        neighbor x.x.x.x {\n            export policy_c;\n        }\n    }\n}\n</code></pre> <p>There are three policies in this case - policy_c is what is applied since that is the most specific policy. </p> <p>Note</p> <p>If no export policy exists, BGP will advertise only the prefixes learned via BGP by default.</p> <p>Let's build a policy that advertises our loopbacks into BGP:</p> <pre><code>root@Leaf1# show policy-options \npolicy-statement allow_loopback0 {\n    from {\n        interface lo0.0;\n        route-filter 192.168.100.1/32 exact;\n    }\n    then accept;\n}\n</code></pre> <p>I've defined a policy-statement called 'allow_loopback0'. In this, we match on interface lo0.0 and also specifically match the IP address 192.168.100.1/32 (as an exact match). We then add this as an export policy under BGP:</p> <pre><code>{master:0}[edit]\nroot@Leaf1# set protocols bgp group underlay export allow_loopback0 \n</code></pre> <p>Now, Spine1 and Spine2 see this prefix from Leaf1. Taking Spine1 as an example:</p> <pre><code>root@Spine1# run show bgp summary    \nThreading mode: BGP I/O\nDefault eBGP mode: advertise - accept, receive - accept\nGroups: 1 Peers: 2 Down peers: 0\nTable          Tot Paths  Act Paths Suppressed    History Damp State    Pending\ninet.0               \n                       1          1          0          0          0          0\nPeer                     AS      InPkt     OutPkt    OutQ   Flaps Last Up/Dwn State|#Active/Received/Accepted/Damped...\n172.100.100.0         64521        202        201       0       0     1:29:51 Establ\n  inet.0: 1/1/1/0\n172.100.100.4         64522        201        201       0       0     1:29:49 Establ\n  inet.0: 0/0/0/0\n\nroot@Spine1# run show route receive-protocol bgp 172.100.100.0        \n\ninet.0: 7 destinations, 7 routes (7 active, 0 holddown, 0 hidden)\n  Prefix                  Nexthop              MED     Lclpref    AS path\n* 192.168.100.1/32        172.100.100.0                           64521 I\n\ninet6.0: 2 destinations, 2 routes (2 active, 0 holddown, 0 hidden)\n\nroot@Spine1# run show route protocol bgp \n\ninet.0: 7 destinations, 7 routes (7 active, 0 holddown, 0 hidden)\n+ = Active Route, - = Last Active, * = Both\n\n192.168.100.1/32   *[BGP/170] 00:01:19, localpref 100\n                      AS path: 64521 I, validation-state: unverified\n                    &gt;  to 172.100.100.0 via xe-0/0/0.0\n\ninet6.0: 2 destinations, 2 routes (2 active, 0 holddown, 0 hidden)\n</code></pre> <p>We do the same thing on Leaf2 as well:</p> <pre><code>root@Leaf2# show policy-options \npolicy-statement allow_loopback0 {\n    from {\n        interface lo0.0;\n        route-filter 192.168.100.2/32 exact;\n    }\n    then accept;\n}\n</code></pre> <p>Note</p> <p>The command 'show route receive-protocol bgp' shows the routes received by the BGP process from a specific neighbor. This is essentially BGP RIB In.</p> <p>At this point, both Leaf1 and Leaf2 should have each others loopback, and it should be reachable.</p> <pre><code>// Leaf1\n\nroot@Leaf1# run show route protocol bgp \n\ninet.0: 8 destinations, 9 routes (8 active, 0 holddown, 0 hidden)\n+ = Active Route, - = Last Active, * = Both\n\n192.168.100.2/32   *[BGP/170] 00:00:54, localpref 100\n                      AS path: 65500 64522 I, validation-state: unverified\n                    &gt;  to 172.100.100.1 via xe-0/0/0.0\n                    [BGP/170] 00:00:54, localpref 100\n                      AS path: 65500 64522 I, validation-state: unverified\n                    &gt;  to 172.100.100.3 via xe-0/0/1.0\n\ninet6.0: 2 destinations, 2 routes (2 active, 0 holddown, 0 hidden)\n\n// Leaf2\n\nroot@Leaf2# run show route protocol bgp \n\ninet.0: 8 destinations, 9 routes (8 active, 0 holddown, 0 hidden)\n+ = Active Route, - = Last Active, * = Both\n\n192.168.100.1/32   *[BGP/170] 00:03:38, localpref 100\n                      AS path: 65500 64521 I, validation-state: unverified\n                    &gt;  to 172.100.100.5 via xe-0/0/0.0\n                    [BGP/170] 00:03:38, localpref 100\n                      AS path: 65500 64521 I, validation-state: unverified\n                    &gt;  to 172.100.100.7 via xe-0/0/1.0\n\ninet6.0: 2 destinations, 2 routes (2 active, 0 holddown, 0 hidden)\n\nroot@Leaf1# run ping 192.168.100.2 source 192.168.100.1 \nPING 192.168.100.2 (192.168.100.2): 56 data bytes\n64 bytes from 192.168.100.2: icmp_seq=0 ttl=63 time=237.986 ms\n64 bytes from 192.168.100.2: icmp_seq=1 ttl=63 time=216.589 ms\n64 bytes from 192.168.100.2: icmp_seq=2 ttl=63 time=126.104 ms\n64 bytes from 192.168.100.2: icmp_seq=3 ttl=63 time=124.349 ms\n64 bytes from 192.168.100.2: icmp_seq=4 ttl=63 time=122.077 ms\n64 bytes from 192.168.100.2: icmp_seq=5 ttl=63 time=123.286 ms\n64 bytes from 192.168.100.2: icmp_seq=6 ttl=63 time=123.885 ms\n^C\n--- 192.168.100.2 ping statistics ---\n7 packets transmitted, 7 packets received, 0% packet loss\nround-trip min/avg/max/stddev = 122.077/153.468/237.986/47.050 ms\n</code></pre>","tags":["juniper","junos","bgp"]},{"location":"blog/2022/02/16/junos-part-ii---bgp-and-bgp-unnumbered/#a-note-on-load-balancing","title":"A note on load-balancing","text":"<p>By default, BGP does not load-balance over multiple paths. This must be explicitly enabled, and this is no different on Junos OS as well. This can be done using the 'multipath' option:</p> <pre><code>root@Leaf1# set protocols bgp group underlay multipath\n</code></pre> <p>However, even after this, the forwarding-table maintained by the Routing Engine (the RE maintains a master copy, and this is sent to the PFE which eventually installs it in the ASICs) does not show ECMP for these loopbacks:</p> <pre><code>// Leaf1\n\nroot@Leaf1# run show route forwarding-table destination 192.168.100.2/32 \nRouting table: default.inet\nInternet:\nDestination        Type RtRef Next hop           Type Index    NhRef Netif\n192.168.100.2/32   user     0 172.100.100.3      ucst     1735     4 xe-0/0/1.0\n\n// Leaf2\n\nroot@Leaf2# run show route forwarding-table destination 192.168.100.1/32 \nRouting table: default.inet\nInternet:\nDestination        Type RtRef Next hop           Type Index    NhRef Netif\n192.168.100.1/32   user     0 172.100.100.7      ucst     1735     4 xe-0/0/1.0\n</code></pre> <p>This is where a second export policy comes in - this time the export is from the routing table in the RE to the forwarding table in the RE, and the export is applied against the forwarding table itself. The policy looks like this:</p> <pre><code>root@Leaf1# show policy-options                     \npolicy-statement ECMP {\n    then {\n        load-balance per-flow;\n    }\n}\n</code></pre> <p>It says match anything, and allow for a per-flow load balancing. </p> <p>Note</p> <p>In previous versions, the parameter 'per-flow' did not exists and the only option was 'per-packet'. This was confusing because 'per-packet' would also really do 'per-flow' load balancing. So, in summary, regardless of what you choose, you're really doing per-flow load balancing. The 'per-packet' option will be deprecated in future releases.</p> <p>Finally, this policy is applied as follows:</p> <pre><code>root@Leaf1# set routing-options forwarding-table export ECMP\n</code></pre> <p>Now you can see two next hops in the forwarding table as well:</p> <pre><code>root@Leaf1# run show route forwarding-table destination 192.168.100.2/32 \nRouting table: default.inet\nInternet:\nDestination        Type RtRef Next hop           Type Index    NhRef Netif\n192.168.100.2/32   user     0                    ulst   131070     2\n                              172.100.100.1      ucst     1734     4 xe-0/0/0.0\n                              172.100.100.3      ucst     1735     4 xe-0/0/1.0\n</code></pre> <p>This covers an eBGP underlay design and configuration. Our goal was to simply provide reachability from leaf loopbacks, which will eventually be used as the VTEP source interfaces, and next hops for the EVPN overlay (more on that in a later post).</p>","tags":["juniper","junos","bgp"]},{"location":"blog/2022/02/16/junos-part-ii---bgp-and-bgp-unnumbered/#bgp-unnumbered-bgp-auto-discovered-neighbors","title":"BGP Unnumbered (BGP auto discovered neighbors)","text":"","tags":["juniper","junos","bgp"]},{"location":"blog/2022/02/16/junos-part-ii---bgp-and-bgp-unnumbered/#why-bgp-unnumbered","title":"Why BGP unnumbered","text":"<p>I've covered BGP Unnumbered in one of my earlier Cumulus posts. This one is going to be specific to Junos OS - the methodology is largely the same, so for the most part, this section will help you understand the Junos OS configuration better and what it is doing.</p> <p>Note</p> <p>Juniper calls this BGP Auto Discovered Neighbors</p> <p>BGP unnumbered is gaining popularity (as it should) as the primary choice for underlay design in a VXLAN EVPN fabric. The reason is fairly simple - a big part of designing your underlay is IPv4 address allocation (assuming you're building an IPv4 underlay). These are typically /30s (that should really be /31s). BGP unnumbered alleviates the pain of this address allocation, and simply functions on the fact that an IPv6 peering can be used to advertise an IPv4 NLRI with an IPv6 next-hop (RFC 5549).</p>","tags":["juniper","junos","bgp"]},{"location":"blog/2022/02/16/junos-part-ii---bgp-and-bgp-unnumbered/#ipv6-configuration-for-bgp-unnumbered","title":"IPv6 configuration for BGP unnumbered","text":"<p>Assuming we start fresh, there's no IPv4 address assigned to any of the p2p links between the leafs and spines. The first thing we need to do is enable IPv6 and IPv6 Router Advertisements on our p2p links - this allows for a link local inet6 address to be assigned to the interface, and advertised via RA. Note, you must enable the inet family as well.</p> <p>Let's take the connection between Leaf1 and Spine1 to understand this better.</p> <pre><code>// Leaf1 \n\nroot@Leaf1# show interfaces xe-0/0/0  \nunit 0 {\n    family inet;\n    family inet6;\n}\n\nroot@Leaf1# show protocols router-advertisement \ninterface xe-0/0/0.0;\n\n// Spine1\n\nroot@Spine1# show interfaces xe-0/0/0  \nunit 0 {\n    family inet;\n    family inet6;\n}\n\nroot@Spine1# show protocols router-advertisement \ninterface xe-0/0/0.0;\n</code></pre> <p>Enabling inet6 family under the interface should assign a link local inet6 address to these interfaces:</p> <pre><code>// Leaf1\n\nroot@Leaf1# run show interfaces xe-0/0/0.0  \n  Logical interface xe-0/0/0.0 (Index 555) (SNMP ifIndex 526)\n    Flags: Up SNMP-Traps 0x4004000 Encapsulation: ENET2\n    Input packets : 113\n    Output packets: 119\n    Protocol inet6, MTU: 1500\n    Max nh cache: 75000, New hold nh limit: 75000, Curr nh cnt: 1, Curr new hold cnt: 0, NH drop cnt: 0\n      Flags: Is-Primary\n      Addresses, Flags: Is-Preferred\n        Destination: fe80::/64, Local: fe80::205:86ff:fefd:4203\n\n// Spine1\n\nroot@Spine1# run show interfaces xe-0/0/0.0  \n  Logical interface xe-0/0/0.0 (Index 555) (SNMP ifIndex 527)\n    Flags: Up SNMP-Traps 0x4004000 Encapsulation: ENET2\n    Input packets : 118\n    Output packets: 122\n    Protocol inet6, MTU: 1500\n    Max nh cache: 75000, New hold nh limit: 75000, Curr nh cnt: 1, Curr new hold cnt: 0, NH drop cnt: 0\n      Flags: Is-Primary\n      Addresses, Flags: Is-Preferred\n        Destination: fe80::/64, Local: fe80::205:86ff:fe3d:f503\n</code></pre> <p>Outside of this, both Leaf1 and Spine1 should now be sending RAs as well, advertisting their link local inet6 address. Through this, they should also learn each others link local addresses. A packet capture shows these RAs sent out:</p> <p></p> <p>From the CLI, we should see our peers link local address as well:</p> <pre><code>// Leaf1\n\nroot@Leaf1# run show ipv6 router-advertisement \nInterface: xe-0/0/0.0\n  Advertisements sent: 7, last sent 00:03:02 ago\n  Solicits sent: 1, last sent 00:24:35 ago\n  Solicits received: 1, last received 00:24:03 ago\n  Advertisements received: 7\n  Solicited router advertisement unicast: Disable\n  IPv6 RA Preference: DEFAULT/MEDIUM\n  Advertisement from fe80::205:86ff:fe3d:f503, heard 00:01:04 ago\n    Managed: 0\n    Other configuration: 0\n    Reachable time: 0 ms\n    Default lifetime: 1800 sec\n    Retransmit timer: 0 ms\n    Current hop limit: 64\n\nroot@Leaf1# run show ipv6 neighbors    \nIPv6 Address                            Linklayer Address  State       Exp   Rtr  Secure  Interface               \nfe80::205:86ff:fe3d:f503                 02:05:86:3d:f5:03  stale       852   yes  no      xe-0/0/0.0\n\n// Spine1\n\nroot@Spine1# run show ipv6 router-advertisement \nInterface: xe-0/0/0.0\n  Advertisements sent: 7, last sent 00:01:30 ago\n  Solicits sent: 1, last sent 00:24:29 ago\n  Solicits received: 0\n  Advertisements received: 4\n  Solicited router advertisement unicast: Disable\n  IPv6 RA Preference: DEFAULT/MEDIUM\n  Advertisement from fe80::205:86ff:fefd:4203, heard 00:03:28 ago\n    Managed: 0\n    Other configuration: 0\n    Reachable time: 0 ms\n    Default lifetime: 1800 sec\n    Retransmit timer: 0 ms\n    Current hop limit: 64\n\nroot@Spine1# run show ipv6 neighbors \nIPv6 Address                            Linklayer Address  State       Exp   Rtr  Secure  Interface               \nfe80::205:86ff:fefd:4203                 02:05:86:fd:42:03  stale       773   yes  no      xe-0/0/0.0\n</code></pre> <p>We'll complete our configuration by applying the same commands to all other relevant devices and interfaces. At the end of this, the two spines have learned the link local addresses of both leafs:</p> <pre><code>// Spine1\n\nroot@Spine1# run show ipv6 neighbors    \nIPv6 Address                            Linklayer Address  State       Exp   Rtr  Secure  Interface               \nfe80::205:86ff:fea8:a803                 02:05:86:a8:a8:03  reachable   11    yes  no      xe-0/0/1.0              \nfe80::205:86ff:fefd:4203                 02:05:86:fd:42:03  reachable   4     yes  no      xe-0/0/0.0              \nTotal entries: 2\n\n// Spine2\n\nroot@Spine2# run show ipv6 neighbors \nIPv6 Address                            Linklayer Address  State       Exp   Rtr  Secure  Interface               \nfe80::205:86ff:fea8:a807                 02:05:86:a8:a8:07  reachable   27    yes  no      xe-0/0/1.0              \nfe80::205:86ff:fefd:4207                 02:05:86:fd:42:07  reachable   10    yes  no      xe-0/0/0.0              \nTotal entries: 2\n</code></pre>","tags":["juniper","junos","bgp"]},{"location":"blog/2022/02/16/junos-part-ii---bgp-and-bgp-unnumbered/#enabling-bgp-to-auto-discover-ipv6-neighbors","title":"Enabling BGP to auto discover IPv6 neighbors","text":"<p>Now that we have our base IPv6 configuration deployed, we can start to enable BGP for the underlay. But there's a catch here - since we're no longer using IPv4 addressing for our underlay, what do we really peer against? BGP needs an explicit neighbor address to peer with. This is where we leverage BGP dynamic peering to build neighbors over these IPv6 link local addresses, using IPv6 Neighbor Discovery.</p> <p>Consider Leaf1 and Spine1 again to understand this better. We're going to create a BGP 'underlay' group again, but this time, instead of specifying IPv4 neighbors, we're going to enable BGP peer discovery and enable it for the IPv6 enabled interfaces (you're specifying interfaces for peering instead of neighbor addresses now).</p> <p>This is what we have so far for this part of the configuration:</p> <pre><code>root@Leaf1# show | compare \n[edit protocols]\n+   bgp {\n+       group underlay {\n+           dynamic-neighbor bgp_unnumbered {\n+               peer-auto-discovery {\n+                   family inet6 {\n+                       ipv6-nd;\n+                   }\n+                   interface xe-0/0/0.0;\n+               }\n+           }\n+       }\n+   }\n</code></pre> <p>Outside of this, we also need to enable this group for both inet and inet6. The inet family must allow for Extended Next Hop, since the next-hop is going to be an IPv6 address. Also, since all our spines are in the same AS, we can define the peer-as for the entire 'underlay' group.</p> <p>Note</p> <p>Extended Next Hop is a new capability that was introduced as part of RFC 5549. This is what allows an IPv4 NLRI to be advertised using an IPv6 next hop.</p> <p>Our final configuration looks like this:</p> <pre><code>root@Leaf1# show | compare \n[edit protocols]\n+   bgp {\n+       group underlay {\n+           family inet {\n+               unicast {\n+                   extended-nexthop;\n+               }\n+           }\n+           family inet6 {\n+               unicast;\n+           }\n+           peer-as 65500;\n+           multipath;\n+           dynamic-neighbor bgp_unnumbered {\n+               peer-auto-discovery {\n+                   family inet6 {\n+                       ipv6-nd;\n+                   }\n+                   interface xe-0/0/0.0;\n+               }\n+           }\n+       }\n+   }\n</code></pre> <p>Once we commit this on Leaf1, it picks up Spine1s link local IPv6 address from ND (Neighbor Discovery) and builds a dynamic peering against it:</p> <pre><code>root@Leaf1# run show bgp summary                    \nThreading mode: BGP I/O\nDefault eBGP mode: advertise - accept, receive - accept\nGroups: 1 Peers: 1 Down peers: 1\nAuto-discovered peers: 1\nTable          Tot Paths  Act Paths Suppressed    History Damp State    Pending\ninet.0               \n                       0          0          0          0          0          0\ninet6.0              \n                       0          0          0          0          0          0\nPeer                     AS      InPkt     OutPkt    OutQ   Flaps Last Up/Dwn State|#Active/Received/Accepted/Damped...\nfe80::205:86ff:fe3d:f503%xe-0/0/0.0       65500          0          0       0       0        2:05 Active\n</code></pre> <p>The session remains active because Spine1 is not yet configured and any TCP request on port 179 is rejected by it, with a TCP RST sent back:</p> <p></p> <p>Junos OS does not allow you to specify a peers BGP AS number against an interface. Because of this, our spine configuration is going to have an addition - since the leafs are in their own unique autonomous systems, we can't just have one peer-as for the entire underlay group. Thus, we configure a 'peer-list' instead and apply this under the BGP group. </p> <p>This is done via a type of policy-option called 'as-list', which allows you to specify a range of BGP AS numbers. The final configuration on Spine1 is:</p> <pre><code>root@Spine1# show | compare \n[edit]\n+  policy-options {\n+      as-list bgp_unnumbered_as_list members 64521-64530;\n+  }\n[edit protocols]\n+   bgp {\n+       group underlay {\n+           family inet {\n+               unicast {\n+                   extended-nexthop;\n+               }\n+           }\n+           family inet6 {\n+               unicast;\n+           }\n+           multipath;\n+           dynamic-neighbor bgp_unnumbered {\n+               peer-auto-discovery {\n+                   family inet6 {\n+                       ipv6-nd;\n+                   }\n+                   interface xe-0/0/0.0;\n+               }\n+           }\n+           peer-as-list bgp_unnumbered_as_list;\n+       }\n+   }\n</code></pre> <p>Let's commit this now and take a packet capture to look at the BGP peering bringup. From this packet capture we can see that things look much better now - a TCP 3-way handshake successfully completed, and each peer is sending a BGP OPEN message. It's important to confirm that this new Extended Next Hop capability is exchanged during BGP OPEN, as we can see below:</p> <p></p> <p>We also see our peering in an Established state now:</p> <pre><code>root@Leaf1# run show bgp summary \nThreading mode: BGP I/O\nDefault eBGP mode: advertise - accept, receive - accept\nGroups: 1 Peers: 1 Down peers: 0\nAuto-discovered peers: 1\nTable          Tot Paths  Act Paths Suppressed    History Damp State    Pending\ninet.0               \n                       0          0          0          0          0          0\ninet6.0              \n                       0          0          0          0          0          0\nPeer                     AS      InPkt     OutPkt    OutQ   Flaps Last Up/Dwn State|#Active/Received/Accepted/Damped...\nfe80::205:86ff:fe3d:f503%xe-0/0/0.0       65500         18         17       0       0        6:38 Establ\n  inet.0: 0/0/0/0\n  inet6.0: 0/0/0/0\n</code></pre> <p>Similar configuration can be done for all leafs and spines and their respective interfaces now. At the end of it, both the spines see the leafs as BGP peers:</p> <pre><code>// Spine1\n\nroot@Spine1# run show bgp summary    \nThreading mode: BGP I/O\nDefault eBGP mode: advertise - accept, receive - accept\nGroups: 1 Peers: 2 Down peers: 0\nAuto-discovered peers: 2\nTable          Tot Paths  Act Paths Suppressed    History Damp State    Pending\ninet.0               \n                       0          0          0          0          0          0\ninet6.0              \n                       0          0          0          0          0          0\nPeer                     AS      InPkt     OutPkt    OutQ   Flaps Last Up/Dwn State|#Active/Received/Accepted/Damped...\nfe80::205:86ff:fea8:a803%xe-0/0/1.0       64522         15         15       0       0        5:45 Establ\n  inet.0: 0/0/0/0\n  inet6.0: 0/0/0/0\nfe80::205:86ff:fefd:4203%xe-0/0/0.0       64521         41         41       0       0       17:07 Establ\n  inet.0: 0/0/0/0\n  inet6.0: 0/0/0/0\n\n// Spine2\n\nroot@Spine2# run show bgp summary    \nThreading mode: BGP I/O\nDefault eBGP mode: advertise - accept, receive - accept\nGroups: 1 Peers: 2 Down peers: 0\nAuto-discovered peers: 2\nTable          Tot Paths  Act Paths Suppressed    History Damp State    Pending\ninet.0               \n                       0          0          0          0          0          0\ninet6.0              \n                       0          0          0          0          0          0\nPeer                     AS      InPkt     OutPkt    OutQ   Flaps Last Up/Dwn State|#Active/Received/Accepted/Damped...\nfe80::205:86ff:fea8:a807%xe-0/0/1.0       64522         15         14       0       0        5:04 Establ\n  inet.0: 0/0/0/0\n  inet6.0: 0/0/0/0\nfe80::205:86ff:fefd:4207%xe-0/0/0.0       64521         14         13       0       0        5:03 Establ\n  inet.0: 0/0/0/0\n  inet6.0: 0/0/0/0\n</code></pre> <p>Note</p> <p>If you want to see just the auto discovered peers and nothing else, you can use the 'show bgp summary auto-discovered' CLI instead</p>","tags":["juniper","junos","bgp"]},{"location":"blog/2022/02/16/junos-part-ii---bgp-and-bgp-unnumbered/#analyzing-an-ipv4-nlri-advertisement-with-bgp-unnumbered","title":"Analyzing an IPv4 NLRI advertisement with BGP unnumbered","text":"<p>Again, with dynamic neighbors using IPv6 ND, Junos OS does not allow you to apply an export policy against the interface (and since you don't have explicit neighbors, you can't apply it there either). In this case, we need to apply it for the entire group, which is why it is important to keep your policy as specific as possible:</p> <pre><code>policy-statement allow_loopback0 {\n    from {\n        interface lo0.0;\n        route-filter 192.168.100.1/32 exact;\n    }\n    then accept;\n}\n</code></pre> <p>This can now be exported under the BGP group 'underlay':</p> <pre><code>root@Leaf1# set protocols bgp group underlay export allow_loopback0 \n</code></pre> <p>Note</p> <p>In some vQFX versions, you might see your BGP peering flap when an export policy is applied (which is quite disruptive, naturally). Rest assured, this does not happen on actual physical hardware.</p> <p>Once this is done, you should see the spines receiveing this prefix, with an IPv6 next hop:</p> <pre><code>// Spine1\n\nroot@Spine1# run show route receive-protocol bgp fe80::205:86ff:fefd:4203%xe-0/0/0.0 \n\ninet.0: 3 destinations, 3 routes (3 active, 0 holddown, 0 hidden)\n  Prefix                  Nexthop              MED     Lclpref    AS path\n* 192.168.100.1/32        fe80::205:86ff:fefd:4203                64521 I\n\ninet6.0: 4 destinations, 4 routes (4 active, 0 holddown, 0 hidden)\n\n// Spine2\n\nroot@Spine2# run show route receive-protocol bgp fe80::205:86ff:fefd:4207%xe-0/0/0.0 \n\ninet.0: 3 destinations, 3 routes (3 active, 0 holddown, 0 hidden)\n  Prefix                  Nexthop              MED     Lclpref    AS path\n* 192.168.100.1/32        fe80::205:86ff:fefd:4207                64521 I\n\ninet6.0: 4 destinations, 4 routes (4 active, 0 holddown, 0 hidden)\n</code></pre> <p>On Leaf2, this should now be a multipath prefix, load-balanced over both the links to the spines:</p> <pre><code>root@Leaf2# run show route 192.168.100.1/32 \n\ninet.0: 4 destinations, 5 routes (4 active, 0 holddown, 0 hidden)\n+ = Active Route, - = Last Active, * = Both\n\n192.168.100.1/32   *[BGP/170] 00:40:26, localpref 100, from fe80::205:86ff:fe3d:f507\n                      AS path: 65500 64521 I, validation-state: unverified\n                       to fe80::205:86ff:fe3d:f507 via xe-0/0/0.0\n                    &gt;  to fe80::205:86ff:fe85:5a07 via xe-0/0/1.0\n                    [BGP/170] 02:00:34, localpref 100\n                      AS path: 65500 64521 I, validation-state: unverified\n                    &gt;  to fe80::205:86ff:fe85:5a07 via xe-0/0/1.0\n\nroot@Leaf2# run show route forwarding-table destination 192.168.100.1/32 \nRouting table: default.inet\nInternet:\nDestination        Type RtRef Next hop           Type Index    NhRef Netif\n192.168.100.1/32   user     0                    ulst   131070     2\n                              fe80::205:86ff:fe3d:f507\n                                                 ucst     1741     5 xe-0/0/0.0\n                              fe80::205:86ff:fe85:5a07\n                                                 ucst     1738     5 xe-0/0/1.0\n</code></pre> <p>As a final confirmation, Leaf1 should be able to ping Leaf2s loopback just like before:</p> <pre><code>root@Leaf1# run ping 192.168.100.2 source 192.168.100.1  \nPING 192.168.100.2 (192.168.100.2): 56 data bytes\n64 bytes from 192.168.100.2: icmp_seq=0 ttl=63 time=168.321 ms\n64 bytes from 192.168.100.2: icmp_seq=1 ttl=63 time=168.718 ms\n64 bytes from 192.168.100.2: icmp_seq=2 ttl=63 time=147.416 ms\n64 bytes from 192.168.100.2: icmp_seq=3 ttl=63 time=300.588 ms\n64 bytes from 192.168.100.2: icmp_seq=4 ttl=63 time=144.453 ms\n64 bytes from 192.168.100.2: icmp_seq=5 ttl=63 time=135.477 ms\n^C\n--- 192.168.100.2 ping statistics ---\n6 packets transmitted, 6 packets received, 0% packet loss\nround-trip min/avg/max/stddev = 135.477/177.495/300.588/56.384 ms\n</code></pre>","tags":["juniper","junos","bgp"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/","title":"Junos Part I - the basics","text":"<p>In this post, we take an introductory look at the operating system used in Juniper platforms, called Junos OS. This post serves as an introduction to the CLI. We also cover basic bridging.</p>","tags":["juniper","junos"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/#introduction-and-topology","title":"Introduction and topology","text":"<p>As a new user of the Junos OS, I thought I'd take this opportunity to blog about what I generally start with when learning a new platform/OS. It's important to understand the basics before moving onto the more complicated technologies. For me, the basics have always included L2 protocols like STP, some form of link aggregation (LAG), at least one IGP, and then finally, understand the tools available to troubleshoot a problem on the platform. Naturally, you also need to get comfortable with navigating the CLI. </p> <p>To that end, here's the topology that we'll be using for this post. The devices in use are Juniper Networks vQFXs.</p> <p></p>","tags":["juniper","junos"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/#navigating-the-cli","title":"Navigating the CLI","text":"","tags":["juniper","junos"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/#factory-default-settings","title":"Factory default settings","text":"<p>The first, most natural thing, is to learn how to navigate the CLI itself. The Junos OS is a beast of it's own - at first glance, it seems far more complex than your typical run-of-the-mill Cisco style CLI. You need to give it some time - the kind of quality of life features the Junos OS has is AMAZING. </p> <p>When you first boot up a factory default Juniper switch (virtual, or hardware), you'll see something like this:</p> <pre><code>Amnesiac (ttyd0)\n\nlogin:\n</code></pre> <p>'Amnesiac' essentially means the switch is in factory default state. At this point, you can login with 'root' and no password. </p> <pre><code>Amnesiac (ttyd0)\n\nlogin: root\n\n--- JUNOS 21.4R1.12 built 2021-12-17 14:37:27 UTC\n\nroot@:RE:0%\n</code></pre>","tags":["juniper","junos"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/#cli-modes","title":"CLI modes","text":"<p>In general, you can be in one of three places in the OS - the shell, operational mode or configuration mode. A non-root user will land in operational mode when logging in. </p> <p>Moving from the shell to operational mode can be done using the 'cli' command, and moving from operational mode to configuration mode can be done using the 'configure' command. </p> <pre><code>// shell \n\nroot@SW1:RE:0%\n\n// moving from shell to operational mode\n\nroot@:RE:0% cli\n{master:0}\n\n// moving from operational mode to configuration mode\n\nroot&gt; configure \nEntering configuration mode\n\n{master:0}[edit]\n</code></pre>","tags":["juniper","junos"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/#basic-configuration-and-syntax","title":"Basic configuration and syntax","text":"<p>Junos OS configuration is hierarchical. Notice the 'edit' keyword, in brackets, when you entered configuration mode above. This signifies that you are in the edit mode, which is the top most configuration hierarchy. From here, you can start configuring things in two ways:</p> <ol> <li>You can use set commands directly from the edit mode.</li> <li>You can go further down the hierarchy by using additional 'edit' commands and then use 'set' commands.</li> </ol> <p>To demonstrate this, let's set a root password for the device, and change the hostname. </p> <p>Using set commands, we can do this as follows:</p> <pre><code>{master:0}[edit]\nroot# set system host-name SW1 \n\n{master:0}[edit]\nroot# set system root-authentication plain-text-password \nNew password:\nRetype new password:\n\n{master:0}[edit]\n</code></pre> <p>Using edit commands to go down the hierarchy, we can do the following:</p> <pre><code>{master:0}[edit]\nroot# edit system \n\n{master:0}[edit system]\nroot# set host-name SW1\n</code></pre> <p>Notice how the OS tells you exactly where you are in the hierarchy. The 'edit' changed to 'edit system' because you went to the system level hierarchy. From here, you can issue system level set commands. </p> <p>To set the root password, we can do the following:</p> <pre><code>{master:0}[edit system]\nroot# set root-authentication plain-text-password \nNew password:\nRetype new password:\n\n{master:0}[edit system]\nroot# \n</code></pre>","tags":["juniper","junos"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/#saving-configuration","title":"Saving configuration","text":"<p>Junos OS works with the concept of an active and candidate configuration. The active configuration is what the device booted up with and is actively running/using. A candidate configuration is created as soon as you enter configuration mode by typing 'configure' or 'edit' while in operational mode.</p> <p>The candidate configuration is essentially a copy of the active configuration, but any changes you make in configuration mode are made to the candidate configuration and not the active configuration. Let that sink in - this is a big shift to what you were potentially used to with vendors like Cisco, and operating systems like IOS/IOS-XE/NX-OS. Any changes made on these would be applied immediately - the purpose of saving the configuration was just to ensure that when the device boots up again, these changes are a part of the configuration.</p> <p>In Junos OS, to move the changes from the candidate configuration to active configuration (essentially making the candidate configuration the active), you need to 'commit' your changes.</p> <pre><code>root# commit ?\nPossible completions:\n  &lt;[Enter]&gt;            Execute this command\n  activate             Activate a previously prepared commit\n  and-quit             Quit configuration mode if commit succeeds\n  at                   Time at which to activate configuration changes\n  check                Check correctness of syntax; do not apply changes\n  comment              Message to write to commit log\n  confirmed            Automatically rollback if not confirmed\n  no-synchronize       Don't synchronize commit\n  peers-synchronize    Synchronize commit on remote peers\n  prepare              Prepare for an upcoming commit activation\n  scripts              Push scripts to other RE\n  synchronize          Synchronize commit on both routing engines\n  |                    Pipe through a command\n</code></pre> <p>As you can see, there are multiple options to commit changes. The simplest way is just to type commit, and press enter. </p> <pre><code>root# commit    \nconfiguration check succeeds\ncommit complete\n</code></pre> <p>The OS checks for syntax errors, and if none are found, the changes are committed. There are several other handy ways to commit, some of which I use often:</p> <ol> <li>commit check - this simply checks the configuration for synatx errors, but does not commit them.</li> <li>commit confirmed - have you ever locked yourself out of a device because of a change you made? More often than not? Then 'commit confirmed' is for you. This commits the changes, but starts a timer as well (default being 10 minutes) - within this time, the user is expected to 'commit' the changes again, thus confirming the commit. If the user does not commit the changes again, then the configuration is automatically rolled back.</li> <li>commit comment - very git like. You can comment against every commit.</li> <li>commit at - allows for commits to be scheduled.</li> </ol>","tags":["juniper","junos"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/#change-diffs-and-rolling-back","title":"Change diffs and rolling back","text":"<p>Let's take SW2, as an example, and make some basic changes.</p> <pre><code>{master:0}[edit]\nroot# set system host-name SW2\n\n{master:0}[edit]\nroot# set system root-authentication plain-text-password \nNew password:\nRetype new password:\n</code></pre> <p>It is very easy to see what changes are pending to be committed using the 'compare' option. All of this is returned in diff style.</p> <pre><code>root# show | compare \n[edit system]\n+  host-name SW2;\n+  root-authentication {\n+      encrypted-password \"$6$im9YvqX2$DdZmTiWqRf2O7HAgjFW9ptFsRAT9KXRELcycjG30BkYFMavce7hqwaiz2HznSHVB4FnJStyVdaMD26P1/8WeL1\"; ## SECRET-DATA\n+  }\n\n{master:0}[edit]\nroot# \n</code></pre> <p>Remember, these changes are not yet commited - this is simply showing us what was added to the candidate configuration, and what isn't present in the active configuration.</p> <p>The 'commit confirmed' option sheds light into an AMAZING feature on the OS - rollback. Let's commit our changes first.</p> <pre><code>root# commit \nconfiguration check succeeds\ncommit complete\n\n{master:0}[edit]\nroot@SW2#\n</code></pre> <p>Junos OS has a rolling file structure for 'rollback' files, with upto 49 rollback files being stored (0 through 49 with 0 being the active configuration itself). The first three commits are stored under /config and anything after that is stored under /var/db/config. </p> <p>You can also see the full configuration of a rollback file using 'show system rollback '. Put together the 'compare' option from before, and you have a very powerful tool - you can view changes compared to an earlier version of the configuration, and combined with the rollback feature, you can roll back to it.   <p>So, let's say I didn't like the changes I made, and even though I have commited it, I want to go back to my earlier configuration. I can first check what the difference is between my current/active configuration and the rollback I want to go to. Remember, rollback 0 is my active configuration, so there should be no difference between the two. </p> <pre><code>root@SW2# show | compare rollback 0 \n\n{master:0}[edit]\n</code></pre> <p>As expected, we see no difference. Let's try that again with rollback 1 now:</p> <pre><code>root@SW2# show | compare rollback 1  \n[edit system]\n+  host-name SW2;\n+  root-authentication {\n+      encrypted-password \"$6$im9YvqX2$DdZmTiWqRf2O7HAgjFW9ptFsRAT9KXRELcycjG30BkYFMavce7hqwaiz2HznSHVB4FnJStyVdaMD26P1/8WeL1\"; ## SECRET-DATA\n+  }\n-  commit {\n-      factory-settings {\n-          reset-virtual-chassis-configuration;\n-          reset-chassis-lcd-menu;\n-      }\n-  }\n</code></pre> <p>This is comparing the active and the rollback configuration - the '+' indicates what is present in the active configuration, while the '-' indicates what is in the rollback configuration. When you rollback, the changes are not commited - just like before, they are simply a part of the candidate configuration. </p> <p>This can be confirmed by doing a compare again, after rolling back:</p> <pre><code>root@SW2# rollback 1 \nload complete\n\n{master:0}[edit]\nroot@SW2# show | compare \n[edit system]\n-  host-name SW2;\n-  root-authentication {\n-      encrypted-password \"$6$im9YvqX2$DdZmTiWqRf2O7HAgjFW9ptFsRAT9KXRELcycjG30BkYFMavce7hqwaiz2HznSHVB4FnJStyVdaMD26P1/8WeL1\"; ## SECRET-DATA\n-  }\n+  commit {\n+      factory-settings {\n+          reset-virtual-chassis-configuration;\n+          reset-chassis-lcd-menu;\n+      }\n+  }\n</code></pre> <p>This tells us that when we commit these changes, the '-' statements will be removed, and the '+' statements will be added.</p>","tags":["juniper","junos"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/#basic-bridging","title":"Basic Bridging","text":"<p>For bridging, we'll walk through the creation of VLANs, implement and verify STP, and finally configure SW1 for inter-VLAN routing using L3 interfaces for our VLANs.  </p>","tags":["juniper","junos"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/#craeting-vlans","title":"Craeting VLANs","text":"<p>VLAN creation is very straightfoward - </p> <pre><code>{master:0}[edit]\nroot@SW1# set vlans VLAN10 vlan-id 10 \n\n{master:0}[edit]\nroot@SW1# set vlans VLAN20 vlan-id 20\n</code></pre> <p>Once commited, you can see the VLANs via 'show vlans':</p> <pre><code>root@SW1# show vlans \nVLAN10 {\n    vlan-id 10;\n}\nVLAN20 {\n    vlan-id 20;\n}\ndefault {\n    vlan-id 1;\n}\n</code></pre>","tags":["juniper","junos"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/#creating-access-and-trunk-interfaces","title":"Creating access and trunk interfaces","text":"<p>On the Junos OS, interfaces function based on physical properties and logical properties. MTU is an example of a physical property. Logical properties fall under something called as a 'unit' - a unit can be thought of as a sub-interface (if you're from the Cisco world). However, having said that, all interfaces will have a unit (even switchports) - the functionality is similar to a sub-interface only when a VLAN tag is associated to it.</p> <p>The general schema for an interface:</p> <pre><code>interfaces {\n    interface-name {\n        physical-properties;\n        [...]\n        unit &lt;&gt; {\n        logical-properties;\n        [...]\n        }\n    }\n}\n</code></pre> <p>'family' is a logical property and this determines the kind of interface you're configuring. The various family options on the vQFX are:</p> <pre><code>root@SW1# set interfaces xe-0/0/0 unit 0 family ?\nPossible completions:\n&gt; ccc                  Circuit cross-connect parameters\n&gt; ethernet-switching   Ethernet switching parameters\n&gt; inet                 IPv4 parameters\n&gt; inet6                IPv6 protocol parameters\n&gt; iso                  OSI ISO protocol parameters\n&gt; mpls                 MPLS protocol parameters\n&gt; vpls                 Virtual private LAN service parameters\n</code></pre> <p>For switchports, the family is 'ethernet-switching'. This family is mutually exclusive to other families like inet or inet6, which means that it cannot be configured alongside any of these other families.</p> <p>Let's configure ports xe-0/0/0 and xe-0/0/1 on SW1 to be a trunk interface now:</p> <pre><code>{master:0}[edit]\nroot@SW1# set interfaces xe-0/0/0 unit 0 family ethernet-switching interface-mode trunk vlan members [VLAN10 VLAN20] \n\n{master:0}[edit]\nroot@SW1# set interfaces xe-0/0/1 unit 0 family ethernet-switching interface-mode trunk vlan members [VLAN10 VLAN20]\n</code></pre> <p>Remember to delete any other families (which may be configured by default) first. For example, if I have family inet already configured for the interface, and I try to commit family ethernet-switching to the same interface, I get the following error:</p> <pre><code>root@SW1# set interfaces xe-0/0/4 unit 0 family ethernet-switching \n\n{master:0}[edit]\nroot@SW1# commit \n[edit interfaces xe-0/0/0 unit 0 family]\n  'ethernet-switching'\n    Family ethernet-switching and rest of the families are mutually exclusive\nerror: commit failed: (statements constraint check failed)\n</code></pre> <p>Before we move on, let's also configure our host facing interfaces (xe-0/0/3) to be access interfaces in VLAN 10 and 20 respectively.</p> <pre><code>{master:0}[edit]\nroot@SW3# set interfaces xe-0/0/3 unit 0 family ethernet-switching interface-mode access vlan members VLAN10\n\n{master:0}[edit]\nroot@SW4# set interfaces xe-0/0/3 unit 0 family ethernet-switching interface-mode access vlan members VLAN20\n</code></pre>","tags":["juniper","junos"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/#implementing-and-verifying-stp","title":"Implementing and verifying STP","text":"<p>Assuming all relevant interfaces were configured in the same way on other switches, let's jump to everyone's favorite protocol, shall we? </p> <p>Junos OS offers three flavors of spanning-tree that can be enabled:</p> <ol> <li>RSTP - this is 802.1w and creates one instance of STP with RSTP mechanisms and enhancements.</li> <li>VSTP - this creates one instance of STP for every VLAN, similar to PVST+ on the Cisco side.</li> <li>MSTP - this allows you to create your own VLAN to STP instance mappings.</li> </ol> <p>To keep things simple, we'll enable RSTP across the network now. This is done on a per-interface basis, like so:</p> <pre><code>{master:0}[edit]\nroot@SW1# set protocols rstp interface xe-0/0/0  \n\n{master:0}[edit]\nroot@SW1# set protocols rstp interface xe-0/0/1    \n\n{master:0}[edit]\nroot@SW1# set protocols rstp interface xe-0/0/2  \n</code></pre> <p>We also want to set SW1 as the root bridge for the network, and SW2 as a secondary root bridge. We'll adjust the bridge priority to influence the election (remember, the default priority is 32768):</p> <pre><code>{master:0}[edit]\nroot@SW1# set protocols rstp bridge-priority 4096 \n\n{master:0}[edit]\nroot@SW2# set protocols rstp bridge-priority 8192 \n</code></pre> <p>Once these changes are commited, we can verify STP state. First, let's confirm that SW1 is the root bridge. This can be done by looking at the bridge data, and making sure that the bridge ID is the same as the root ID on SW1:</p> <pre><code>root@SW1&gt; show spanning-tree bridge \nSTP bridge parameters \nRouting instance name               : GLOBAL\nContext ID                          : 0\nEnabled protocol                    : RSTP\n  Root ID                           : 4096.02:05:86:71:c1:02\n  Hello time                        : 2 seconds\n  Maximum age                       : 20 seconds\n  Forward delay                     : 15 seconds\n  Message age                       : 0 \n  Number of topology changes        : 5\n  Time since last topology change   : 13227 seconds\n  Local parameters \n    Bridge ID                       : 4096.02:05:86:71:c1:02\n    Extended system ID              : 0\n</code></pre> <p>Since SW1 is the root bridge, all participating interfaces should ideally be designated forwarding (barring any exceptions). Interface state can be confirmed using 'show spanning-tree interface':</p> <pre><code>root@SW1&gt; show spanning-tree interface \n\nSpanning tree interface parameters for instance 0\n\nInterface                  Port ID    Designated         Designated         Port    State  Role\n                                       port ID           bridge ID          Cost\nxe-0/0/0                   128:490      128:490   4096.02058671c102         2000    FWD    DESG \nxe-0/0/1                   128:491      128:491   4096.02058671c102         2000    FWD    DESG \nxe-0/0/2                   128:492      128:492   4096.02058671c102         2000    FWD    DESG \n</code></pre> <p>Let's look at SW2 now to verify what it sees:</p> <pre><code>root@SW2&gt; show spanning-tree bridge \nSTP bridge parameters \nRouting instance name               : GLOBAL\nContext ID                          : 0\nEnabled protocol                    : RSTP\n  Root ID                           : 4096.02:05:86:71:c1:02\n  Root cost                         : 2000\n  Root port                         : xe-0/0/2\n  Hello time                        : 2 seconds\n  Maximum age                       : 20 seconds\n  Forward delay                     : 15 seconds\n  Message age                       : 1 \n  Number of topology changes        : 4\n  Time since last topology change   : 13336 seconds\n  Local parameters \n    Bridge ID                       : 8192.02:05:86:71:39:02\n    Extended system ID              : 0\n</code></pre> <p>The bridge ID is not the same as the root ID, which implies that SW2 is not the root bridge for this STP instance and the root bridge is the switch with a bridge ID of 4096.02:05:86:71:c1:02. All non-root bridges must have a root port, which is the shortest path to the root - for SW2, this is xe-0/0/2, as seen below:</p> <pre><code>root@SW2&gt; show spanning-tree interface     \n\nSpanning tree interface parameters for instance 0\n\nInterface                  Port ID    Designated         Designated         Port    State  Role\n                                       port ID           bridge ID          Cost\nxe-0/0/0                   128:490      128:490   8192.020586713902         2000    FWD    DESG \nxe-0/0/1                   128:491      128:491   8192.020586713902         2000    FWD    DESG \nxe-0/0/2                   128:492      128:492   4096.02058671c102         2000    FWD    ROOT \n</code></pre> <p>This same process can be carried out for each of the switches in our network.</p>","tags":["juniper","junos"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/#creating-l3-interfaces-for-vlans-and-inter-vlan-routing","title":"Creating L3 interfaces for VLANs and inter-VLAN routing","text":"<p>Creating SVIs or L3 interfaces for corresponding VLANs is a little different from what you'd typically be used to, if you're coming from the Cisco. For example, in Cisco's IOS/IOS-XE/NX-OS, you can create the L3 interface as follows:</p> <pre><code>interface vlan x\n    ip address ...\n</code></pre> <p>In Junos OS, we use irb interface (on the QFX platform) and vlan interface (on the EX platform) to do the same. This is not enough though - you need to map these L3 interfaces to the L2 VLAN itself. </p> <p>Let's create L3 interfaces for both VLAN 10 and VLAN 20 now, on SW1:</p> <pre><code>{master:0}[edit]\nroot@SW1# set interfaces irb unit 10 family inet address 10.1.1.1/24 \n\n{master:0}[edit]\nroot@SW1# set interfaces irb unit 20 family inet address 20.1.1.1/24 \n</code></pre> <p>These are created with a unit, and a family, like any other interface. A good practice is to match the unit to the VLAN number itself (easy to remember it that way) and since we needed an IPv4 interface, we used the inet family here. </p> <p>We need to map these to the L2 VLAN:</p> <pre><code>{master:0}[edit]\nroot@SW1# set vlans VLAN10 l3-interface irb.10    \n\n{master:0}[edit]\nroot@SW1# set vlans VLAN20 l3-interface irb.20  \n</code></pre> <p>These are now up and running:</p> <pre><code>{master:0}\nroot@SW1&gt; show interfaces irb.10 \n  Logical interface irb.10 (Index 558) (SNMP ifIndex 550)\n    Flags: Up SNMP-Traps 0x4004000 Encapsulation: ENET2\n    Bandwidth: 1Gbps\n    Routing Instance: default-switch Bridging Domain: VLAN10\n    Input packets : 14\n    Output packets: 325\n    Protocol inet, MTU: 1500\n    Max nh cache: 75000, New hold nh limit: 75000, Curr nh cnt: 1, Curr new hold cnt: 0, NH drop cnt: 0\n      Flags: Sendbcast-pkt-to-re\n      Addresses, Flags: Is-Preferred Is-Primary\n        Destination: 10.1.1/24, Local: 10.1.1.1, Broadcast: 10.1.1.255\n\n{master:0}\nroot@SW1&gt; show interfaces irb.20    \n  Logical interface irb.20 (Index 567) (SNMP ifIndex 551)\n    Flags: Up SNMP-Traps 0x4000 Encapsulation: ENET2\n    Bandwidth: 1Gbps\n    Routing Instance: default-switch Bridging Domain: VLAN20\n    Input packets : 14\n    Output packets: 264\n    Protocol inet, MTU: 1500\n    Max nh cache: 75000, New hold nh limit: 75000, Curr nh cnt: 1, Curr new hold cnt: 0, NH drop cnt: 0\n      Flags: Sendbcast-pkt-to-re\n      Addresses, Flags: Is-Preferred Is-Primary\n        Destination: 20.1.1/24, Local: 20.1.1.1, Broadcast: 20.1.1.255\n</code></pre> <p>Host1 is configured to have an IP address of 10.1.1.100/4 and Host2 is configured to have an IP address of 20.1.1.100/24, with each of them pointing to the respective L3 VLAN interface on SW1 as their default gateways. </p> <p>As a final check, let's confirm reachability. Host1 and Host2 can both ping their gateways, as well as each other.</p> <pre><code>Host1&gt; ping 10.1.1.1\n\n84 bytes from 10.1.1.1 icmp_seq=1 ttl=64 time=302.328 ms\n84 bytes from 10.1.1.1 icmp_seq=2 ttl=64 time=113.679 ms\n84 bytes from 10.1.1.1 icmp_seq=3 ttl=64 time=113.723 ms\n84 bytes from 10.1.1.1 icmp_seq=4 ttl=64 time=117.448 ms\n84 bytes from 10.1.1.1 icmp_seq=5 ttl=64 time=147.437 ms\n\nHost2&gt; ping 20.1.1.1\n\n84 bytes from 20.1.1.1 icmp_seq=1 ttl=64 time=213.357 ms\n84 bytes from 20.1.1.1 icmp_seq=2 ttl=64 time=114.515 ms\n84 bytes from 20.1.1.1 icmp_seq=3 ttl=64 time=155.711 ms\n84 bytes from 20.1.1.1 icmp_seq=4 ttl=64 time=119.110 ms\n84 bytes from 20.1.1.1 icmp_seq=5 ttl=64 time=121.898 ms\n\nHost1&gt; ping 20.1.1.100\n\n84 bytes from 20.1.1.100 icmp_seq=1 ttl=63 time=129.036 ms\n84 bytes from 20.1.1.100 icmp_seq=2 ttl=63 time=140.484 ms\n84 bytes from 20.1.1.100 icmp_seq=3 ttl=63 time=208.669 ms\n84 bytes from 20.1.1.100 icmp_seq=4 ttl=63 time=121.458 ms\n84 bytes from 20.1.1.100 icmp_seq=5 ttl=63 time=221.929 ms\n</code></pre>","tags":["juniper","junos"]},{"location":"blog/2022/01/24/junos-part-i---the-basics/#following-the-packet","title":"Following the packet","text":"<p>When Host1 tries to ping Host2, it knows that Host2 is in a different subnet and ARPs for its own default gateway. This ARP is flooded in VLAN 10, and is only stopped from looping around because STP blocked redundant L2 paths. SW1 should get this as well.</p> <p></p> <p>A packet capture on SW1 shows this ARP packet. Also notice that the packet is coming in with a 802.1q tag, with a VLAN ID of 10:</p> <p></p> <p>SW1 will respond with an ARP response and Host1 can build its ARP cache using this. Now, it constructs the ICMP request packet and sends it out to SW3.</p> <p></p> <p>SW3 will do a MAC address table lookup for the destination MAC address. This tells it to forward the packet out interface xe-0/0/0:</p> <pre><code>root@SW3&gt; show ethernet-switching table 02:05:86:71:c1:00 vlan-id 10 \n\nMAC flags (S - static MAC, D - dynamic MAC, L - locally learned, P - Persistent static, C - Control MAC\n           SE - statistics enabled, NM - non configured MAC, R - remote PE MAC, O - ovsdb MAC)\n\n\nEthernet switching table : 3 entries, 3 learned\nRouting instance : default-switch\n    Vlan                MAC                 MAC         Age    Logical                NH        RTR \n    name                address             flags              interface              Index     ID\n    VLAN10              02:05:86:71:c1:00   D             -   xe-0/0/0.0             0         0       \n</code></pre> <p>The packet is now sent to SW1. SW1 owns this MAC address, so it does a route lookup now for the destination IP address:</p> <pre><code>root@SW1&gt; show route table inet.0 20.1.1.100 \n\ninet.0: 6 destinations, 6 routes (6 active, 0 holddown, 0 hidden)\n+ = Active Route, - = Last Active, * = Both\n\n20.1.1.0/24        *[Direct/0] 20:09:17\n                    &gt;  via irb.20\n</code></pre> <p>It finds that the destination is directly connected to it, which means SW1 can ARP for it now. The ARP process happens again, and once complete, SW1 forwards the packet out the interface over which it was learned. The packet is now routed from irb.10 to irb.20.</p> <pre><code>root@SW1&gt; show arp hostname 20.1.1.100 \nMAC Address       Address         Name                      Interface               Flags\n00:50:79:66:68:0a 20.1.1.100      20.1.1.100                irb.20 [xe-0/0/1.0]     none\n</code></pre> <p>SW4 gets this and again does a MAC address table lookup for the destination MAC address. </p> <pre><code>{master:0}\nroot@SW4&gt; show ethernet-switching table 00:50:79:66:68:0a vlan-id 20    \n\nMAC flags (S - static MAC, D - dynamic MAC, L - locally learned, P - Persistent static, C - Control MAC\n           SE - statistics enabled, NM - non configured MAC, R - remote PE MAC, O - ovsdb MAC)\n\n\nEthernet switching table : 2 entries, 2 learned\nRouting instance : default-switch\n    Vlan                MAC                 MAC         Age    Logical                NH        RTR \n    name                address             flags              interface              Index     ID\n    VLAN20              00:50:79:66:68:0a   D             -   xe-0/0/3.0             0         0       \n</code></pre> <p>The ICMP request is forwarded out to Host2 and the same process happens in reverse now.</p> <p>I hope this was informative, and I'd like to thank you for reading!</p>","tags":["juniper","junos"]},{"location":"blog/2022/01/24/juniper-vqfx-and-containerlab/","title":"Juniper vQFX and Containerlab","text":"<p>In this post, we look at how Containerlab can be used to quickly spin up vQFX topologies for network validation and testing. We'll walk through the entire process - how to build docker images from vQFX images, what happens behind the scenes when bringing these containers up and how to build/verify your topology.</p>","tags":["juniper","junos","containerlab"]},{"location":"blog/2022/01/24/juniper-vqfx-and-containerlab/#what-is-containerlab","title":"What is Containerlab?","text":"<p>Containerlab is an open source, network validation/testing platform that allows you to easily spin up network labs and manage end to end lab life cycle. It supports several network operating systems, across many vendors. As of this writing, it supports Nokia SR Linux, Juniper cRPD, Cumulus VX, Arista cEOS. Outside of these natively containerized operating systems, it also supports VM based devcies like Junipers vQFX and vMX, Nexus 9000v, IOS XRv, Arista vEOS and so on.</p> <p>More information can be found on their homepage - https://containerlab.srlinux.dev/</p>","tags":["juniper","junos","containerlab"]},{"location":"blog/2022/01/24/juniper-vqfx-and-containerlab/#building-the-vqfx-docker-container","title":"Building the vQFX docker container","text":"","tags":["juniper","junos","containerlab"]},{"location":"blog/2022/01/24/juniper-vqfx-and-containerlab/#downloading-vrnetlab","title":"Downloading vrnetlab","text":"<p>To build VM based containers, you need to download a fork of vrnetlab that was modified for this - https://github.com/hellt/vrnetlab. It essentially packages the VM inside a container. Simply gitclone the repo to your working directory (clear instructions are provided here).</p> <pre><code>root@aninchat-ubuntu:~# git clone https://github.com/hellt/vrnetlab .\nCloning into '.'...\nremote: Enumerating objects: 3442, done.\nremote: Counting objects: 100% (312/312), done.\nremote: Compressing objects: 100% (205/205), done.\nremote: Total 3442 (delta 158), reused 241 (delta 107), pack-reused 3130\nReceiving objects: 100% (3442/3442), 1.81 MiB | 2.15 MiB/s, done.\nResolving deltas: 100% (2090/2090), done.\n</code></pre> <p>Within this folder, you have various vendor VM options and instructions on how to build containers for these, along with relevant code files to do this.</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab# ls -l\ntotal 128\n-rw-r--r-- 1 root root   94 Feb  5 15:41 CODE_OF_CONDUCT.md\n-rw-r--r-- 1 root root  706 Feb  5 15:41 CONTRIBUTING.md\n-rw-r--r-- 1 root root 1109 Feb  5 15:41 LICENSE\n-rw-r--r-- 1 root root  324 Feb  5 15:41 Makefile\n-rw-r--r-- 1 root root 3973 Feb  5 15:41 README.md\ndrwxr-xr-x 2 root root 4096 Feb  5 15:41 ci-builder-image\ndrwxr-xr-x 2 root root 4096 Feb  5 15:41 common\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 config-engine-lite\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 csr\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 ftosv\n-rwxr-xr-x 1 root root 5210 Feb  5 15:41 git-lfs-repo.sh\n-rw-r--r-- 1 root root 3158 Feb  5 15:41 makefile-install.include\n-rw-r--r-- 1 root root  370 Feb  5 15:41 makefile-sanity.include\n-rw-r--r-- 1 root root 1898 Feb  5 15:41 makefile.include\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 n9kv\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 nxos\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 openwrt\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 pan\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 routeros\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 sros\ndrwxr-xr-x 2 root root 4096 Feb  5 15:41 topology-machine\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 veos\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 vmx\ndrwxr-xr-x 3 root root 4096 Feb  5 15:47 vqfx\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 vr-bgp\ndrwxr-xr-x 2 root root 4096 Feb  5 15:41 vr-xcon\n-rw-r--r-- 1 root root 1135 Feb  5 15:41 vrnetlab.sh\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 vrp\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 vsr1000\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 xrv\ndrwxr-xr-x 3 root root 4096 Feb  5 15:41 xrv9k\n</code></pre>","tags":["juniper","junos","containerlab"]},{"location":"blog/2022/01/24/juniper-vqfx-and-containerlab/#inspecting-the-makefile","title":"Inspecting the Makefile","text":"<p>For the purposes of this post, we'll be focused on the vQFX, so let's look what's inside there.</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/vqfx# ls -l\ntotal 12\n-rw-r--r-- 1 root root 2045 Feb  6 07:29 Makefile\n-rw-r--r-- 1 root root  161 Feb  6 07:29 README.md\ndrwxr-xr-x 2 root root 4096 Feb  6 07:29 docker\n</code></pre> <p>There's a Makefile, a readme, and a 'docker' folder that contains the Dockerfile and a launch python file.</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/vqfx/docker# ls -l\ntotal 16\n-rw-r--r-- 1 root root  638 Feb  6 07:29 Dockerfile\n-rwxr-xr-x 1 root root 8400 Feb  6 07:29 launch.py\n</code></pre> <p>Before we do anything, let's confirm that no docker images are created for any VM. The only thing present is the base Ubuntu 20.04 image.</p> <pre><code>root@aninchat-ubuntu:~/clabs/juniper# docker images\nREPOSITORY   TAG       IMAGE ID       CREATED      SIZE\nubuntu       20.04     54c9d81cbb44   3 days ago   72.8MB\n</code></pre> <p>The Makefile for vQFX contains the following:</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/vqfx# cat Makefile\nVENDOR=Juniper\nNAME=vQFX\nIMAGE_FORMAT=qcow2\nIMAGE_GLOB=*.qcow2\n\n# New vqfx are named: vqfx-19.4R1.10-re-qemu.qcow2\nVERSION=$(shell echo $(IMAGE) |  sed -e 's/^vqfx-//'|sed -e 's/-re-qemu.qcow2//' )\n# Old QFX are named: vqfx10k-re-15.1X53-D60.vmdk, and vqfx10k-re-18.4R1.8.vmdk and vqfx10k-pfe-18.4R1.8.vmdk so we need that version number extracted to convert them.\nVMDK_VERSION:=$(shell ls *-re-*.vmdk | sed -re 's/vqfx10k-re-([^;]*)\\.vmdk.*$$/\\1/')\n\nPFE_BASE_VERSION=$(shell echo $VERSION | sed -e s'/.*//')\nPFE_IMAGE=$(shell ls vqfx-$(PFE_BASE_VERSION)*-pfe-qemu.qcow*)\n\n-include ../makefile-sanity.include\n-include ../makefile.include\n\nformat-legacy-images:\n    @if ls *.vmdk; then echo \"VMDKs exist, converting them to qcow format\"; qemu-img convert -f vmdk -O qcow2 *-re-*.vmdk vqfx-$(VMDK_VERSION)-re-qemu.qcow2 &amp;&amp; qemu-img convert -f vmdk -O qcow *-pfe-*.vmdk vqfx-$(VMDK_VERSION)-pfe-qemu.qcow; echo \"VMDKs have been converted\"; fi\n\n# TODO: we should make sure we only copy one PFE image (the latest?), in case there are many\ndocker-pre-build:\n\n    echo \"pfe     $(PFE_IMAGE)\"\n    cp vqfx*-pfe*.qcow* docker/\n    echo \"image   $(IMAGE)\"\n    echo \"version $(VERSION)\"\n\n# mostly copied from makefile.include, i wish this was easier to change\ndocker-build-common: docker-clean-build docker-pre-build\n    @if [ -z \"$$IMAGE\" ]; then echo \"ERROR: No IMAGE specified\"; exit 1; fi\n    @if [ \"$(IMAGE)\" = \"$(VERSION)\" ]; then echo \"ERROR: Incorrect version string ($(IMAGE)). The regexp for extracting version information is likely incorrect, check the regexp in the Makefile or open an issue at https://github.com/plajjan/vrnetlab/issues/new including the image file name you are using.\"; exit 1; fi\n    @echo \"Building docker image using $(IMAGE) as $(REGISTRY)vr-$(VR_NAME):$(VERSION)\"\n    cp ../common/* docker/\n    $(MAKE) IMAGE=$$IMAGE docker-build-image-copy\n    (cd docker; docker build --build-arg http_proxy=$(http_proxy) --build-arg https_proxy=$(https_proxy) --build-arg RE_IMAGE=$(IMAGE) --build-arg PFE_IMAGE=$(PFE_IMAGE) -t $(REGISTRY)vr-$(VR_NAME):$(VERSION) .)\n</code></pre> <p>The Makefile is essentially looking for your vQFX images (there needs to be two - one PFE and one RE image). This version of Containerlab also has support for qcow/qcow2 based file systems (some older versions only supported vmdk files). As part of the Makefile, we're also copying over files from vrnetlab/common/ to the 'Docker' folder under vQFX. These files are the healthcheck and vrnetlab python files:</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/vqfx# ls -l ../common/\ntotal 32\n-rwxr-xr-x 1 root root   339 Feb  5 15:41 healthcheck.py\n-rw-r--r-- 1 root root 24925 Feb  5 15:41 vrnetlab.py\n</code></pre> <p>Then finally, the Makefile calls docker build to build the image itself. </p>","tags":["juniper","junos","containerlab"]},{"location":"blog/2022/01/24/juniper-vqfx-and-containerlab/#start-the-vqfx-docker-image-build","title":"Start the vQFX docker image build","text":"<p>First, copy over the RE and PFE images to the vqfx folder under vrnetlab:</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/vqfx# ls -l\ntotal 1965464\n-rw-r--r-- 1 root root      2045 Feb  3 06:27 Makefile\n-rw-r--r-- 1 root root       161 Feb  3 06:27 README.md\ndrwxr-xr-x 2 root root      4096 Feb  3 16:34 docker\n-rw-r--r-- 1 root root 762839040 Feb  3 08:06 vqfx-20.2R1-2019010209-pfe-qemu.qcow\n-rw-r--r-- 1 root root 675020800 Feb  3 08:08 vqfx-20.2R1.10-re-qemu.qcow2\n</code></pre> <p>vQFX images (upto a certain version) can be downloaded for free from Juniper's software downloads page. Once you have both images in there, you can run 'make' to trigger the docker image build process. 'make' may not be installed by default, so if it is not, simply run 'apt-get install make' to download it. A snippet of what it does below:</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/vqfx# make\nls: cannot access '*-re-*.vmdk': No such file or directory\nMakefile:23: warning: overriding recipe for target 'docker-pre-build'\n../makefile.include:18: warning: ignoring old recipe for target 'docker-pre-build'\nMakefile:30: warning: overriding recipe for target 'docker-build-common'\n../makefile.include:24: warning: ignoring old recipe for target 'docker-build-common'\nfor IMAGE in vqfx-20.2R1.10-re-qemu.qcow2 vqfx-21.4R1.12-re-qemu.qcow2; do \\\n    echo \"Making $IMAGE\"; \\\n    make IMAGE=$IMAGE docker-build; \\\ndone\nMaking vqfx-20.2R1.10-re-qemu.qcow2\nmake[1]: Entering directory '/root/vrnetlab/vqfx'\nls: cannot access '*-re-*.vmdk': No such file or directory\nMakefile:23: warning: overriding recipe for target 'docker-pre-build'\n../makefile.include:18: warning: ignoring old recipe for target 'docker-pre-build'\nMakefile:30: warning: overriding recipe for target 'docker-build-common'\n../makefile.include:24: warning: ignoring old recipe for target 'docker-build-common'\nrm -f docker/*.qcow2* docker/*.tgz* docker/*.vmdk* docker/*.iso\necho \"pfe     vqfx-20.2R1-2019010209-pfe-qemu.qcow\"\npfe     vqfx-20.2R1-2019010209-pfe-qemu.qcow\ncp vqfx*-pfe*.qcow* docker/\n\n*snip*\n\nStep 5/16 : ARG RE_IMAGE\n ---&gt; Running in ff86eca5251c\nRemoving intermediate container ff86eca5251c\n ---&gt; 5e47c7b532d7\nStep 6/16 : ARG PFE_IMAGE\n ---&gt; Running in 9507880e1206\nRemoving intermediate container 9507880e1206\n ---&gt; 7fc77a85c443\nStep 7/16 : COPY $RE_IMAGE /\n ---&gt; aca4ac1a8b0a\nStep 8/16 : COPY $PFE_IMAGE /\n ---&gt; 7797dfaf0bc4\nStep 9/16 : RUN echo $RE_IMAGE &gt; /re_image\n ---&gt; Running in fcf1a20d1000\nRemoving intermediate container fcf1a20d1000\n ---&gt; 7b64b2529b14\nStep 10/16 : RUN echo $PFE_IMAGE &gt; /pfe_image\n ---&gt; Running in ae939bea46fc\nRemoving intermediate container ae939bea46fc\n ---&gt; 03e28ae683ca\nStep 11/16 : COPY healthcheck.py /\n ---&gt; 4305e6c8e60e\nStep 12/16 : COPY vrnetlab.py /\n ---&gt; a0ada6fb5780\nStep 13/16 : COPY launch.py /\n ---&gt; 5dd5b9427cb8\nStep 14/16 : EXPOSE 22 161/udp 830 5000 10000-10099\n ---&gt; Running in f19a64ee2141\nRemoving intermediate container f19a64ee2141\n ---&gt; 2d9f70303f5b\nStep 15/16 : HEALTHCHECK CMD [\"/healthcheck.py\"]\n ---&gt; Running in 280b13b45453\nRemoving intermediate container 280b13b45453\n ---&gt; 71e3c515a0d9\nStep 16/16 : ENTRYPOINT [\"/launch.py\"]\n ---&gt; Running in 6e6c979ccab4\nRemoving intermediate container 6e6c979ccab4\n ---&gt; 3e9c3e64af46\nSuccessfully built 3e9c3e64af46\nSuccessfully tagged vrnetlab/vr-vqfx:20.2R1.10\nmake[1]: Leaving directory '/root/vrnetlab/vqfx'\n</code></pre> <p>At the end of this, the 'docker' folder inside /vrnetlab/vqfx should have been populated with several python scripts and your RE/PFE images:</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/vqfx/docker# ls -l\ntotal 1306296\n-rw-r--r-- 1 root root       638 Feb  3 06:27 Dockerfile\n-rwxr-xr-x 1 root root       339 Feb  5 03:14 healthcheck.py\n-rwxr-xr-x 1 root root      8395 Feb  3 16:34 launch.py\n-rw-r--r-- 1 root root 762839040 Feb  5 03:14 vqfx-20.2R1-2019010209-pfe-qemu.qcow\n-rw-r--r-- 1 root root 574750720 Feb  5 03:14 vqfx-21.4R1.12-re-qemu.qcow2\n-rw-r--r-- 1 root root     24925 Feb  5 03:14 vrnetlab.py\n</code></pre> <p>A new docker image should also be available for vQFX. This is under the vrnetlab/vr-vqfx repo and is tagged appropriately with the vQFX version.</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/vqfx# docker images\nREPOSITORY         TAG         IMAGE ID       CREATED         SIZE\nvrnetlab/vr-vqfx   20.2R1.10   3e9c3e64af46   4 minutes ago   1.84GB\nubuntu             20.04       54c9d81cbb44   3 days ago      72.8MB\n</code></pre> <p>Note</p> <pre><code>Make sure to stay on the newer versions of containerlab because some of the older versions do not support building a docker image from qcow files (the makefile does not have code for this).\n</code></pre>","tags":["juniper","junos","containerlab"]},{"location":"blog/2022/01/24/juniper-vqfx-and-containerlab/#whats-happening-behind-the-scenes","title":"What's happening behind the scenes","text":"<p>So, let's break down what's actually happening. Like a Makefile, there is a Dockerfile that contains instructions on how to build the docker image itself (for vQFX, in this case). When Makefile calls 'docker build', it looks for a Dockerfile in that directory.</p> <p>The Dockerfile is available inside the 'docker' folder. This is what it contains for vQFX:</p> <pre><code>root@aninchat-ubuntu:~/clabs/juniper# cat ~/vrnetlab/vqfx/docker/Dockerfile\nFROM ubuntu:20.04\nMAINTAINER Kristian Larsson &lt;kristian@spritelink.net&gt;\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN apt-get update -qy \\\n &amp;&amp; apt-get upgrade -qy \\\n &amp;&amp; apt-get install -y \\\n    bridge-utils \\\n    iproute2 \\\n    python3-ipy \\\n    socat \\\n    qemu-kvm \\\n    procps \\\n    tcpdump \\\n    openvswitch-switch \\\n &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nARG RE_IMAGE\nARG PFE_IMAGE\n\nCOPY $RE_IMAGE /\nCOPY $PFE_IMAGE /\n\nRUN echo $RE_IMAGE &gt; /re_image\nRUN echo $PFE_IMAGE &gt; /pfe_image\n\nCOPY healthcheck.py /\nCOPY vrnetlab.py /\nCOPY launch.py /\n\nEXPOSE 22 161/udp 830 5000 10000-10099\nHEALTHCHECK CMD [\"/healthcheck.py\"]\nENTRYPOINT [\"/launch.py\"]\n</code></pre> <p>Outside of updating and installing several packages, it is copying the PFE and RE images as well as exposing several ports. The entrypoint ('ENTRYPOINT [\"/launch.py\"]') tells Docker what to run once the container starts. So, in this case, it is telling it to run the launch.py Python script. This script does numerous things, along with bootstrapping configuration for vQFX (we'll look at the logs further down when we actually start our lab).</p> <p>The Dockerfile also has a custom healthcheck command that points to the healthcheck.py file. With a custom healthcheck, a container can be starting, healthy or unhealthy. The health is determined with 0 or 1. If it is 0, the container is healthy and if it is 1, the container is unhealthy. </p> <pre><code>import sys\n\ntry:\n    health_file = open(\"/health\", \"r\")\n    health = health_file.read()\n    health_file.close()\nexcept FileNotFoundError:\n    print(\"health status file not found\")\n    sys.exit(2)\n\nexit_status, message = health.strip().split(\" \", 1)\n\nif message != '':\n    print(message)\n\nsys.exit(int(exit_status))\n</code></pre> <p>The actual health check is performed in the vrnetlab.py file - it essentially checks the health of the qemu (if the VM is running successfull or not) and writes into the /health folder (it will write the exit status as 0 or 1), which the healthcheck.py file periodically checks to determine container status.</p>","tags":["juniper","junos","containerlab"]},{"location":"blog/2022/01/24/juniper-vqfx-and-containerlab/#building-and-deploying-a-containerlab-lab-with-vqfx","title":"Building and deploying a Containerlab lab with vQFX","text":"","tags":["juniper","junos","containerlab"]},{"location":"blog/2022/01/24/juniper-vqfx-and-containerlab/#describing-the-topology-for-containerlab","title":"Describing the topology for Containerlab","text":"<p>Containerlab needs a description of what the topology looks and this is very easily done using a topology file, written in yaml. An example of a simple, two leaf two spine topology:</p> <pre><code>root@aninchat-ubuntu:~/clabs/juniper# cat 2L2S_vqfx.yml\nname: 2L2S_vqfx\ntopology:\n  nodes:\n    spine1:\n            kind: vr-vqfx\n            image: vrnetlab/vr-vqfx:20.2R1.10\n    spine2:\n            kind: vr-vqfx\n            image: vrnetlab/vr-vqfx:20.2R1.10\n    leaf1:\n            kind: vr-vqfx\n            image: vrnetlab/vr-vqfx:20.2R1.10\n    leaf2:\n            kind: vr-vqfx\n            image: vrnetlab/vr-vqfx:20.2R1.10\n  links:\n          - endpoints: [\"leaf1:eth1\", \"spine1:eth1\"]\n          - endpoints: [\"leaf2:eth1\", \"spine1:eth2\"]\n          - endpoints: [\"leaf1:eth2\", \"spine2:eth1\"]\n          - endpoints: [\"leaf2:eth2\", \"spine2:eth2\"]\n</code></pre> <p>In its simplest form, you're telling Containerlab the name of your devices, the 'kind' for each device (vr-vqfx is the kind for vQFX) and the docker image it needs to use to build this lab.</p> <p>Finally, you declare the device interconnections. You can boot up a lab without any links, but realistically, you'd want to have some connections. As you can see, all of this is written in yaml, which is super easy to consume and write. </p> <p>For vQFX, eth0 is the management interface, and shouldn't be used. eth1 corresponds to xe-0/0/0, eth2 corresponds to xe-0/0/1 and so on.</p>","tags":["juniper","junos","containerlab"]},{"location":"blog/2022/01/24/juniper-vqfx-and-containerlab/#starting-a-lab-using-containerlab","title":"Starting a lab using Containerlab","text":"<p>A lab can be started using 'containerlab deploy'. This command expects a topology as a reference, which is the .yml file we created earlier. Let's look at what happens when we start a lab, and more closely inspect a node bringup.</p> <pre><code>root@aninchat-ubuntu:~/clabs/juniper# containerlab deploy --topo 2L2S_vqfx.yml\nINFO[0000] Containerlab v0.23.0 started\nINFO[0000] Parsing &amp; checking topology file: 2L2S_vqfx.yml\nINFO[0000] Creating lab directory: /root/clabs/juniper/clab-2L2S_vqfx\nINFO[0000] Creating docker network: Name='clab', IPv4Subnet='172.20.20.0/24', IPv6Subnet='2001:172:20:20::/64', MTU='1500'\nINFO[0000] Creating container: leaf1\nINFO[0000] Creating container: spine2\nINFO[0000] Creating container: leaf2\nINFO[0000] Creating container: spine1\nINFO[0000] Creating virtual wire: leaf2:eth1 &lt;--&gt; spine1:eth2\nINFO[0000] Creating virtual wire: leaf2:eth2 &lt;--&gt; spine2:eth2\nINFO[0001] Creating virtual wire: leaf1:eth1 &lt;--&gt; spine1:eth1\nINFO[0001] Creating virtual wire: leaf1:eth2 &lt;--&gt; spine2:eth1\nINFO[0001] Adding containerlab host entries to /etc/hosts file\n+---+-----------------------+--------------+----------------------------+---------+---------+----------------+----------------------+\n| # |         Name          | Container ID |           Image            |  Kind   |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+-----------------------+--------------+----------------------------+---------+---------+----------------+----------------------+\n| 1 | clab-2L2S_vqfx-leaf1  | 12f805d46bae | vrnetlab/vr-vqfx:20.2R1.10 | vr-vqfx | running | 172.20.20.5/24 | 2001:172:20:20::5/64 |\n| 2 | clab-2L2S_vqfx-leaf2  | 0b8bfb802838 | vrnetlab/vr-vqfx:20.2R1.10 | vr-vqfx | running | 172.20.20.4/24 | 2001:172:20:20::4/64 |\n| 3 | clab-2L2S_vqfx-spine1 | 99ed06e26513 | vrnetlab/vr-vqfx:20.2R1.10 | vr-vqfx | running | 172.20.20.3/24 | 2001:172:20:20::3/64 |\n| 4 | clab-2L2S_vqfx-spine2 | 97edeb106888 | vrnetlab/vr-vqfx:20.2R1.10 | vr-vqfx | running | 172.20.20.2/24 | 2001:172:20:20::2/64 |\n+---+-----------------------+--------------+----------------------------+---------+---------+----------------+----------------------+\n</code></pre> <p>Containerlab spins up all nodes in the topology as containers, along with the declared virtual wiring. You can look at a node bringup using 'docker logs' or watch it live using 'docker logs -f'. Let's look at the bringup for one of the nodes.</p> <pre><code>root@aninchat-ubuntu:~/clabs/juniper# docker logs clab-2L2S_vqfx-leaf1\n2022-02-06 02:34:51,977: vrnetlab   DEBUG    Creating overlay disk image\n2022-02-06 02:34:51,995: vrnetlab   DEBUG    Creating overlay disk image\n2022-02-06 02:34:52,116: vrnetlab   DEBUG    Starting vrnetlab VQFX\n2022-02-06 02:34:52,116: vrnetlab   DEBUG    VMs: [&lt;__main__.VQFX_vcp object at 0x7fbbc91342e0&gt;, &lt;__main__.VQFX_vpfe object at 0x7fbbc90b7670&gt;]\n2022-02-06 02:34:52,122: vrnetlab   DEBUG    VM not started; starting!\n2022-02-06 02:34:52,122: vrnetlab   INFO     Starting VQFX_vcp\n\n*snip* \n\n2022-02-05 14:48:21,281: launch     INFO     matched login prompt\n2022-02-05 14:48:21,282: launch     DEBUG    writing to serial console: root\n2022-02-05 14:48:21,282: launch     TRACE    Waiting for Password:\n2022-02-05 14:48:49,224: launch     TRACE    Read:  root\nPassword:\n2022-02-05 14:48:49,224: launch     DEBUG    writing to serial console: Juniper\n2022-02-05 14:48:52,228: launch     TRACE    OUTPUT VCP:\n\n--- JUNOS 19.4R1.10 built 2019-12-19 03:54:05 UTC\naunch     DEBUG    writing to serial console: cli\n2022-02-05 14:49:00,585: launch     TRACE    Waiting for &gt;\n2022-02-05 14:49:00,975: launch     TRACE    Read:  cli\n{master:0}\nroot@vqfx-re&gt;\n2022-02-05 14:49:00,976: launch     DEBUG    writing to serial console: configure\n2022-02-05 14:49:00,976: launch     TRACE    Waiting for #\n2022-02-05 14:49:01,046: launch     TRACE    Read:  configure\nEntering configuration mode\n\n{master:0}[edit]\nroot@vqfx-re#\n2022-02-05 14:49:01,046: launch     DEBUG    writing to serial console: set system services ssh\n2022-02-05 14:49:01,046: launch     TRACE    Waiting for #\n2022-02-05 14:49:01,090: launch     TRACE    Read:  set system services ssh\n\n{master:0}[edit]\nroot@vqfx-re#\n2022-02-05 14:49:01,091: launch     DEBUG    writing to serial console: set system services netconf ssh\n2022-02-05 14:49:01,091: launch     TRACE    Waiting for #\n2022-02-05 14:49:01,143: launch     TRACE    Read:  set system services netconf ssh\n\n{master:0}[edit]\nroot@vqfx-re#\n2022-02-05 14:49:01,143: launch     DEBUG    writing to serial console: set system services netconf rfc-compliant\n2022-02-05 14:49:01,143: launch     TRACE    Waiting for #\n2022-02-05 14:49:01,201: launch     TRACE    Read:  set system services netconf rfc-compliant\n\n{master:0}[edit]\nroot@vqfx-re#\n2022-02-05 14:49:01,201: launch     DEBUG    writing to serial console: delete system login user vagrant\n2022-02-05 14:49:01,201: launch     TRACE    Waiting for #\n2022-02-05 14:49:01,260: launch     TRACE    Read:  delete system login user vagrant\n\n{master:0}[edit]\nroot@vqfx-re#\n2022-02-05 14:49:01,260: launch     DEBUG    writing to serial console: set system login user admin class super-user authentication plain-text-password\n2022-02-05 14:49:01,260: launch     TRACE    Waiting for New password:\nroot@vqfx-re# ...dmin class super-user authentication plain-text-password    in class super-user authentication\nNew password:\n2022-02-05 14:49:04,250: launch     DEBUG    writing to serial console: admin@123\n2022-02-05 14:49:04,250: launch     TRACE    Waiting for Retype new password:\n2022-02-05 14:49:04,263: launch     TRACE    Read:\nRetype new password:\n2022-02-05 14:49:04,263: launch     DEBUG    writing to serial console: admin@123\n2022-02-05 14:49:04,263: launch     TRACE    Waiting for #\n2022-02-05 14:49:04,302: launch     TRACE    Read:\n\n{master:0}[edit]\nroot@vqfx-re#\n2022-02-05 14:49:04,302: launch     DEBUG    writing to serial console: set system root-authentication plain-text-password\n2022-02-05 14:49:04,303: launch     TRACE    Waiting for New password:\n2022-02-05 14:49:04,356: launch     TRACE    Read:  set system root-authentication plain-text-password\nNew password:\n2022-02-05 14:49:04,356: launch     DEBUG    writing to serial console: admin@123\n2022-02-05 14:49:04,356: launch     TRACE    Waiting for Retype new password:\n2022-02-05 14:49:04,369: launch     TRACE    Read:\nRetype new password:\n2022-02-05 14:49:04,369: launch     DEBUG    writing to serial console: admin@123\n2022-02-05 14:49:04,369: launch     TRACE    Waiting for #\n2022-02-05 14:49:04,407: launch     TRACE    Read:\n\n{master:0}[edit]\nroot@vqfx-re#\n2022-02-05 14:49:04,408: launch     DEBUG    writing to serial console: delete interfaces\n2022-02-05 14:49:04,408: launch     TRACE    Waiting for #\n2022-02-05 14:49:04,877: launch     TRACE    Read:  delete interfaces\n\n{master:0}[edit]\nroot@vqfx-re#\n2022-02-05 14:49:04,877: launch     DEBUG    writing to serial console: set interfaces em0 unit 0 family inet address 10.0.0.15/24\n2022-02-05 14:49:04,877: launch     TRACE    Waiting for #\n2022-02-05 14:49:05,020: launch     TRACE    Read:  set interfaces em0 unit 0 family inet address 10.0.0.15/24\n\n{master:0}[edit]\nroot@vqfx-re#\n2022-02-05 14:49:05,020: launch     DEBUG    writing to serial console: set interfaces em1 unit 0 family inet address 169.254.0.2/24\n2022-02-05 14:49:05,020: launch     TRACE    Waiting for #\nroot@vqfx-re#:49:05,076: launch     TRACE    Read:  set interfaces em1 unit 0 family inet address 169.254.0.2/24\n2022-02-05 14:49:05,077: launch     DEBUG    writing to serial console: set system host-name leaf1\n2022-02-05 14:49:05,077: launch     TRACE    Waiting for #\n2022-02-05 14:49:05,119: launch     TRACE    Read:  ... 0 family inet address 169.254.0.2/24\n\n{master:0}[edit]\nroot@vqfx-re#\n2022-02-05 14:49:05,119: launch     DEBUG    writing to serial console: commit\n2022-02-05 14:49:05,119: launch     TRACE    Waiting for #\n2022-02-05 14:49:05,163: launch     TRACE    Read:  set system host-name leaf1\n\n{master:0}[edit]\nroot@vqfx-re#\n2022-02-05 14:49:05,163: launch     DEBUG    writing to serial console: exit\n2022-02-05 14:49:05,163: launch     INFO     Startup complete in: 0:05:29.249487\n</code></pre> <p>As you can see, as part of the container bringup, the launch file bootstraps the configuration for it, which includes basic settings such as enabling SSH, setting a hostname and root password, and so on.</p> <p>Make sure the containers are healthy:</p> <pre><code>root@aninchat-ubuntu:~/clabs/juniper# docker ps\nCONTAINER ID   IMAGE                        COMMAND                  CREATED          STATUS                    PORTS                                                 NAMES\nb6e669f3eede   vrnetlab/vr-vqfx:20.2R1.10   \"/launch.py --userna\u2026\"   28 minutes ago   Up 28 minutes (healthy)   22/tcp, 830/tcp, 5000/tcp, 10000-10099/tcp, 161/udp   clab-2L2S_vqfx-spine2\n31d62725210d   vrnetlab/vr-vqfx:20.2R1.10   \"/launch.py --userna\u2026\"   28 minutes ago   Up 28 minutes (healthy)   22/tcp, 830/tcp, 5000/tcp, 10000-10099/tcp, 161/udp   clab-2L2S_vqfx-leaf2\n33c01d3fdab8   vrnetlab/vr-vqfx:20.2R1.10   \"/launch.py --userna\u2026\"   28 minutes ago   Up 28 minutes (healthy)   22/tcp, 830/tcp, 5000/tcp, 10000-10099/tcp, 161/udp   clab-2L2S_vqfx-spine1\nef1e6f245787   vrnetlab/vr-vqfx:20.2R1.10   \"/launch.py --userna\u2026\"   28 minutes ago   Up 28 minutes (healthy)   22/tcp, 830/tcp, 5000/tcp, 10000-10099/tcp, 161/udp   clab-2L2S_vqfx-leaf1\n</code></pre> <p>If it remains unhealthy for a long period of time (some of these images can take 5-10 minutes to fully bootup), then most likely your container bringup is stalled somewhere (check docker logs to find out where and possibly why).</p>","tags":["juniper","junos","containerlab"]},{"location":"blog/2022/01/24/juniper-vqfx-and-containerlab/#accessing-the-lab","title":"Accessing the lab","text":"<p>Once Containerlab deploys a lab, it also assigns IP addresses for logging into the devices. You can view this any time using 'containerlab inspect --topo &lt;&gt;'</p> <pre><code>root@aninchat-ubuntu:~/clabs/juniper# containerlab inspect --topo 2L2S_vqfx.yml\nINFO[0000] Parsing &amp; checking topology file: 2L2S_vqfx.yml\n+---+-----------------------+--------------+----------------------------+---------+---------+----------------+----------------------+\n| # |         Name          | Container ID |           Image            |  Kind   |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+-----------------------+--------------+----------------------------+---------+---------+----------------+----------------------+\n| 1 | clab-2L2S_vqfx-leaf1  | ef1e6f245787 | vrnetlab/vr-vqfx:20.2R1.10 | vr-vqfx | running | 172.20.20.4/24 | 2001:172:20:20::4/64 |\n| 2 | clab-2L2S_vqfx-leaf2  | 31d62725210d | vrnetlab/vr-vqfx:20.2R1.10 | vr-vqfx | running | 172.20.20.3/24 | 2001:172:20:20::3/64 |\n| 3 | clab-2L2S_vqfx-spine1 | 33c01d3fdab8 | vrnetlab/vr-vqfx:20.2R1.10 | vr-vqfx | running | 172.20.20.5/24 | 2001:172:20:20::5/64 |\n| 4 | clab-2L2S_vqfx-spine2 | b6e669f3eede | vrnetlab/vr-vqfx:20.2R1.10 | vr-vqfx | running | 172.20.20.2/24 | 2001:172:20:20::2/64 |\n+---+-----------------------+--------------+----------------------------+---------+---------+----------------+----------------------+\n</code></pre> <p>Let's login to leaf1 as an example now:</p> <pre><code>root@aninchat-ubuntu:~/clabs/juniper# ssh -l admin 172.20.20.4\nThe authenticity of host '172.20.20.4 (172.20.20.4)' can't be established.\nECDSA key fingerprint is SHA256:S5gTisaBW1vp3N2yiMYkQjVzTEbM/DAuFtPNPYrhHno.\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added '172.20.20.4' (ECDSA) to the list of known hosts.\nPassword:\n--- JUNOS 19.4R1.10 built 2019-12-19 03:54:05 UTC\n{master:0}\nadmin@leaf1&gt;\n</code></pre> <p>The login is admin/admin@123. And you're in! Let's actually configure something to make sure it's all good. Our goal is to enable LLDP between all peers, configure loopbacks on leaf1 and leaf2, configure OSPF in area 0 and advertise the loopbacks into OSPF. As a final check, leaf1 should be able to reach leaf2 via the loopbacks. </p> <pre><code>admin@leaf1&gt; configure\nEntering configuration mode\n\n{master:0}[edit]\nadmin@leaf1# set protocols lldp interface xe-0/0/0\n\n{master:0}[edit]\nadmin@leaf1# set protocols lldp interface xe-0/0/1\n\n{master:0}[edit]\nadmin@leaf1# commit\nconfiguration check succeeds\ncommit complete\n</code></pre> <p>We do the same thing on all the other devices as well, and we see our LLDP peerings now:</p> <pre><code>// Leaf1\n\n{master:0}[edit]\nadmin@leaf1# run show lldp neighbors\nLocal Interface    Parent Interface    Chassis Id          Port info          System Name\nxe-0/0/1           -                   02:05:86:71:37:00   xe-0/0/0           spine2\nxe-0/0/0           -                   02:05:86:71:e8:00   xe-0/0/0           spine1\n\n// Leaf2\n\n{master:0}[edit]\nadmin@leaf2# run show lldp neighbors\nLocal Interface    Parent Interface    Chassis Id          Port info          System Name\nxe-0/0/1           -                   02:05:86:71:37:00   xe-0/0/1           spine2\nxe-0/0/0           -                   02:05:86:71:e8:00   xe-0/0/1           spine1\n</code></pre> <p>Configure the loopbacks on both leaf1 and leaf2 now:</p> <pre><code>// Leaf1\n\n{master:0}[edit]\nadmin@leaf1# set interfaces lo0 unit 0 family inet address 192.100.100.1/32\n\n// Leaf2\n\n{master:0}[edit]\nadmin@leaf2# set interfaces lo0 unit 0 family inet address 192.100.100.2/32\n</code></pre> <p>Configure the p2p connections between the leaf and spines, and enable OSPF for each p2p link, as well as the loopback. At the end of this, we see the expected OSPF neighbors:</p> <pre><code>{master:0}[edit]\nadmin@leaf1# run show ospf neighbor\nAddress          Interface              State           ID               Pri  Dead\n10.100.100.1     xe-0/0/0.0             Full            11.11.11.11      128    38\n10.100.100.3     xe-0/0/1.0             Full            22.22.22.22      128    38\n\n{master:0}[edit]\nadmin@leaf2# run show ospf neighbor\nAddress          Interface              State           ID               Pri  Dead\n10.100.100.5     xe-0/0/0.0             Full            11.11.11.11      128    36\n10.100.100.7     xe-0/0/1.0             Full            22.22.22.22      128    38\n</code></pre> <p>On each leaf, we see the corresponding loopback as well, and we can confirm reachability by pinging between them:</p> <pre><code>// Leaf1\n\nadmin@leaf1# run show route protocol ospf\n\ninet.0: 13 destinations, 14 routes (13 active, 0 holddown, 0 hidden)\n+ = Active Route, - = Last Active, * = Both\n\n10.100.100.4/31    *[OSPF/10] 00:04:17, metric 2\n                    &gt;  to 10.100.100.1 via xe-0/0/0.0\n10.100.100.6/31    *[OSPF/10] 00:02:45, metric 2\n                    &gt;  to 10.100.100.3 via xe-0/0/1.0\n192.100.100.1/32    [OSPF/10] 00:05:09, metric 0\n                    &gt;  via lo0.0\n192.100.100.2/32   *[OSPF/10] 00:02:45, metric 2\n                    &gt;  to 10.100.100.1 via xe-0/0/0.0\n                       to 10.100.100.3 via xe-0/0/1.0\n224.0.0.5/32       *[OSPF/10] 00:05:14, metric 1\n                       MultiRecv\n\ninet6.0: 2 destinations, 2 routes (2 active, 0 holddown, 0 hidden)\n\n// Leaf2 \n\nadmin@leaf2# run show route protocol ospf\n\ninet.0: 13 destinations, 14 routes (13 active, 0 holddown, 0 hidden)\n+ = Active Route, - = Last Active, * = Both\n\n10.100.100.0/31    *[OSPF/10] 00:04:51, metric 2\n                    &gt;  to 10.100.100.5 via xe-0/0/0.0\n10.100.100.2/31    *[OSPF/10] 00:03:25, metric 2\n                    &gt;  to 10.100.100.7 via xe-0/0/1.0\n192.100.100.1/32   *[OSPF/10] 00:03:25, metric 2\n                    &gt;  to 10.100.100.5 via xe-0/0/0.0\n                       to 10.100.100.7 via xe-0/0/1.0\n192.100.100.2/32    [OSPF/10] 00:05:45, metric 0\n                    &gt;  via lo0.0\n224.0.0.5/32       *[OSPF/10] 00:05:50, metric 1\n                       MultiRecv\n\ninet6.0: 2 destinations, 2 routes (2 active, 0 holddown, 0 hidden)\n\nadmin@leaf1# run ping 192.100.100.2 source 192.100.100.1\nPING 192.100.100.2 (192.100.100.2): 56 data bytes\n64 bytes from 192.100.100.2: icmp_seq=0 ttl=63 time=122.324 ms\n64 bytes from 192.100.100.2: icmp_seq=1 ttl=63 time=129.199 ms\n64 bytes from 192.100.100.2: icmp_seq=2 ttl=63 time=119.371 ms\n64 bytes from 192.100.100.2: icmp_seq=3 ttl=63 time=120.535 ms\n64 bytes from 192.100.100.2: icmp_seq=4 ttl=63 time=125.425 ms\n^C\n--- 192.100.100.2 ping statistics ---\n5 packets transmitted, 5 packets received, 0% packet loss\nround-trip min/avg/max/stddev = 119.371/123.371/129.199/3.559 ms\n</code></pre> <p>How awesome is this? vQFX as containers, and a fully functional lab! </p> <p>One last thing to add - to destroy the lab, you can simply use the 'containerlab destroy' command.</p> <pre><code>root@aninchat-ubuntu:~/clabs/juniper# containerlab destroy --topo 2L2S_vqfx.yml\nINFO[0000] Parsing &amp; checking topology file: 2L2S_vqfx.yml\nINFO[0000] Destroying lab: 2L2S_vqfx\nINFO[0000] Removed container: clab-2L2S_vqfx-leaf2\nINFO[0001] Removed container: clab-2L2S_vqfx-leaf1\nINFO[0001] Removed container: clab-2L2S_vqfx-spine1\nINFO[0001] Removed container: clab-2L2S_vqfx-spine2\nINFO[0001] Removing containerlab host entries from /etc/hosts file\n</code></pre> <p>I hope this was informative.</p>","tags":["juniper","junos","containerlab"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/","title":"Multi-vendor EVPN VXLAN setup with Containerlab","text":"<p>In this post, we deploy a multivendor EVPN L2 overlay fabric, with BGP in the underlay as well. The entire fabric deployment is automated with Ansible, and Containerlab is used to define and deploy the actual topology.</p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#introduction-and-topology","title":"Introduction and topology","text":"<p>This post continues to build and showcase the power of containerlab. In this post, we will create a multivendor EVPN topology for L2 overlays. The vendors included are Arista (vEOS), Juniper (vQFX), Cumulus (Cumulus VX) and Cisco (N9Kv). Every piece of software used here is free (some might be behind a login, so you'll have to register to get access to image downloads).</p> <p>The topology is as follows:</p> <p></p> <p>Arista vEOS is used for both the spines and leaf1. Cumulus VX is leaf2, Cisco N9Kv is leaf3 and Juniper vQFX is leaf4. Behind each leaf is a host, and all hosts are in the same 10.100.100.0/24 subnet. Router-IDs for each spine and leaf, along with the host details are as follows. The router-ID with a /32 mask makes up the loopback of each device as well:</p> <p></p> <p>The endgoal is to get all the hosts to talk to each other, and also understand if these vendors play nice with each other when deploying L2 overlays with BGP EVPN and VXLAN (we're in for a few interesting surprises here).</p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#building-the-docker-images","title":"Building the docker images","text":"","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#juniper-vqfx","title":"Juniper vQFX","text":"<p>Juniper vQFX can be built using their qcow2 images that are available publicly. I've already written about this in an earlier blog post with complete instructions on how this is built and what is happening behind the scenes.</p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#cumulus-vx","title":"Cumulus VX","text":"<p>Docker images are not officially maintained for this by Cumulus Networks (now Nvidia). However, Michael (@networkop) maintains some unofficial images here. You can simply pull this using 'docker pull'.</p> <pre><code>root@aninchat-ubuntu:~/clabs# docker pull networkop/cx:4.4.0\n4.4.0: Pulling from networkop/cx\nbd8f6a7501cc: Pull complete\ncd45a4bb2831: Pull complete\n3267eb82436d: Pull complete\n43c69aa0f07d: Pull complete\n5467689b757b: Pull complete\n47813b34fad7: Pull complete\ndffd6a1e6cf7: Pull complete\nc5ff024177cd: Pull complete\n8fbd9fcb1a0b: Pull complete\nb24d53edac2f: Pull complete\na287a0d978c0: Pull complete\n050552fdbe9c: Pull complete\n12c6d13747e2: Pull complete\n7a90f1a90542: Pull complete\nd4e73cf57d80: Pull complete\ne90926507c70: Pull complete\n2476c9c7d51c: Pull complete\n28e6b7764b01: Pull complete\n81c55abe901f: Pull complete\n9072c910401f: Pull complete\nf4e46c63c0c6: Pull complete\n4347adc92cde: Pull complete\n1e2015807ed1: Pull complete\nbb63128af12d: Pull complete\nff47aad4ed38: Pull complete\n8956d77e1873: Pull complete\n41a3ceebbb60: Pull complete\n440c42b04957: Pull complete\nbb3ae1d72db8: Pull complete\n085ab76472c5: Pull complete\ne851254ed5e5: Pull complete\n6122afe252b1: Pull complete\n2156d27e047f: Pull complete\nd1b2f513878e: Pull complete\n836eb1aec088: Pull complete\n5aa925411d8b: Pull complete\nDigest: sha256:2ef73abf91c2214ceec08df00580c8754d7a4163391841fe2ad714596a361a4a\nStatus: Downloaded newer image for networkop/cx:4.4.0\ndocker.io/networkop/cx:4.4.0\n</code></pre> <p>You should now see this image available for use:</p> <pre><code>root@aninchat-ubuntu:~/clabs# docker images | grep cx\nnetworkop/cx         5.0.1       4d6152fa636b   2 weeks ago     805MB\nnetworkop/cx         4.4.0       468cdd1a4be5   7 months ago    772MB\n</code></pre> <p>He maintains a fairly up-to-date list of corresponding docker images, but I've pulled an older one because I am not too fond of the new NVUE interface that Cumulus Linux has shifted to - I prefer the older NCLU, which 4.4.x still runs.</p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#arista-veos","title":"Arista vEOS","text":"<p>Arista vEOS can be downloaded for free from Arista's software download site. This is locked behind a guest registration though, so you still need to go through that entire process if you'd like to download this. Once you have the vEOS image (it should be a vmdk file), place it in the vrnet/veos folder.</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/veos# pwd\n/root/vrnetlab/veos\nroot@aninchat-ubuntu:~/vrnetlab/veos# ls -l\ntotal 491984\n-rw-r--r-- 1 root root      1028 Feb  5 15:41 Makefile\n-rw-r--r-- 1 root root      1281 Feb  5 15:41 README.md\ndrwxr-xr-x 2 root root      4096 Feb 16 06:35 docker\n-rw-r--r-- 1 root root 503775232 Feb 16 06:35 vEOS-lab-4.27.2F.vmdk\n</code></pre> <p>You can now trigger the docker image build using 'make'. Since I already have the image built, it doesn't do much for me at this point.</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/veos# make\nMakefile:18: warning: overriding recipe for target 'docker-pre-build'\n../makefile.include:18: warning: ignoring old recipe for target 'docker-pre-build'\nfor IMAGE in vEOS-lab-4.27.2F.vmdk; do \\\n    echo \"Making $IMAGE\"; \\\n    make IMAGE=$IMAGE docker-build; \\\ndone\nMaking vEOS-lab-4.27.2F.vmdk\nmake[1]: Entering directory '/root/vrnetlab/veos'\nMakefile:18: warning: overriding recipe for target 'docker-pre-build'\n../makefile.include:18: warning: ignoring old recipe for target 'docker-pre-build'\nrm -f docker/*.qcow2* docker/*.tgz* docker/*.vmdk* docker/*.iso\n# checking if ZTP config contains a string (DISABLE=True) in the file /zerotouch-config\n# if it does, we don't need to write this file\nChecking ZTP status\nZTPOFF=DISABLE=True; \\\necho \"docker-pre-build: ZTPOFF is $ZTPOFF\" &amp;&amp; \\\n    if [ \"$ZTPOFF\" != \"DISABLE=True\" ]; then \\\n      echo \"Disabling ZTP\" &amp;&amp; docker run --rm -it -e LIBGUESTFS_DEBUG=0 -v $(pwd):/work cmattoon/guestfish -a vEOS-lab-4.27.2F.vmdk -m /dev/sda2 write /zerotouch-config \"DISABLE=True\"; \\\n    fi\ndocker-pre-build: ZTPOFF is DISABLE=True\nBuilding docker image using vEOS-lab-4.27.2F.vmdk as vrnetlab/vr-veos:4.27.2F\ncp ../common/* docker/\nmake IMAGE=$IMAGE docker-build-image-copy\nmake[2]: Entering directory '/root/vrnetlab/veos'\nMakefile:18: warning: overriding recipe for target 'docker-pre-build'\n../makefile.include:18: warning: ignoring old recipe for target 'docker-pre-build'\ncp vEOS-lab-4.27.2F.vmdk* docker/\nmake[2]: Leaving directory '/root/vrnetlab/veos'\n(cd docker; docker build --build-arg http_proxy= --build-arg https_proxy= --build-arg IMAGE=vEOS-lab-4.27.2F.vmdk -t vrnetlab/vr-veos:4.27.2F .)\nSending build context to Docker daemon  503.8MB\nStep 1/11 : FROM ubuntu:20.04\n ---&gt; 54c9d81cbb44\nStep 2/11 : LABEL maintainer=\"Kristian Larsson &lt;kristian@spritelink.net&gt;\"\n ---&gt; Using cache\n ---&gt; b8a0857a144e\nStep 3/11 : LABEL maintainer=\"Roman Dodin &lt;dodin.roman@gmail.com&gt;\"\n ---&gt; Using cache\n ---&gt; a22af11cc083\nStep 4/11 : ARG DEBIAN_FRONTEND=noninteractive\n ---&gt; Using cache\n ---&gt; 1735e5bccc44\nStep 5/11 : RUN apt-get update -qy  &amp;&amp; apt-get upgrade -qy  &amp;&amp; apt-get install -y     bridge-utils     iproute2     python3-ipy     socat     qemu-kvm     tcpdump     tftpd-hpa     ssh     inetutils-ping     dnsutils     openvswitch-switch     iptables     telnet  &amp;&amp; rm -rf /var/lib/apt/lists/*\n ---&gt; Using cache\n ---&gt; 43deb024677e\nStep 6/11 : ARG IMAGE\n ---&gt; Using cache\n ---&gt; 36e30f700548\nStep 7/11 : COPY $IMAGE* /\n ---&gt; Using cache\n ---&gt; 858ae07ca107\nStep 8/11 : COPY *.py /\n ---&gt; Using cache\n ---&gt; 62058063f86c\nStep 9/11 : EXPOSE 22 80 161/udp 443 830 5000 6030 10000-10099 57400\n ---&gt; Using cache\n ---&gt; 53c2102098d6\nStep 10/11 : HEALTHCHECK CMD [\"/healthcheck.py\"]\n ---&gt; Using cache\n ---&gt; 9b3ebcc4ab71\nStep 11/11 : ENTRYPOINT [\"/launch.py\"]\n ---&gt; Using cache\n ---&gt; 11aed84d6be0\nSuccessfully built 11aed84d6be0\nSuccessfully tagged vrnetlab/vr-veos:4.27.2F\nmake[1]: Leaving directory '/root/vrnetlab/veos'\n</code></pre> <p>You should now see this image available to use:</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/n9kv# docker images | grep veos\nvrnetlab/vr-veos     4.27.2F     11aed84d6be0   2 weeks ago     932MB\n</code></pre>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#cisco-n9kv","title":"Cisco N9Kv","text":"<p>Cisco's N9Kv is also available for free (again, locked behind an account registration). Similar to the earlier processes, download the qcow2 image and move it to the vrnetlab/n9kv folder.</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/n9kv# pwd\n/root/vrnetlab/n9kv\nroot@aninchat-ubuntu:~/vrnetlab/n9kv# ls -l\ntotal 1934160\n-rw-r--r-- 1 root root        266 Feb  5 15:41 Makefile\n-rw-r--r-- 1 root root        585 Feb  5 15:41 README.md\ndrwxr-xr-x 2 root root       4096 Feb 16 15:54 docker\n-rw-r--r-- 1 root root 1980563456 Feb 16 06:51 nxosv.9.3.9.qcow2\n</code></pre> <p>Be sure to use the 'n9kv' folder and not the 'nxos' folder - the 'nxos' folder is for the older titanium images. Once the image is copied here, trigger 'make' to build the docker image for this.</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/n9kv# make\nfor IMAGE in nxosv.9.3.9.qcow2; do \\\n    echo \"Making $IMAGE\"; \\\n    make IMAGE=$IMAGE docker-build; \\\ndone\nMaking nxosv.9.3.9.qcow2\nmake[1]: Entering directory '/root/vrnetlab/n9kv'\nrm -f docker/*.qcow2* docker/*.tgz* docker/*.vmdk* docker/*.iso\nBuilding docker image using nxosv.9.3.9.qcow2 as vrnetlab/vr-n9kv:9.3.9\ncp ../common/* docker/\nmake IMAGE=$IMAGE docker-build-image-copy\nmake[2]: Entering directory '/root/vrnetlab/n9kv'\ncp nxosv.9.3.9.qcow2* docker/\nmake[2]: Leaving directory '/root/vrnetlab/n9kv'\n(cd docker; docker build --build-arg http_proxy= --build-arg https_proxy= --build-arg IMAGE=nxosv.9.3.9.qcow2 -t vrnetlab/vr-n9kv:9.3.9 .)\nSending build context to Docker daemon  1.985GB\nStep 1/12 : FROM ubuntu:20.04\n ---&gt; 54c9d81cbb44\nStep 2/12 : LABEL maintainer=\"Kristian Larsson &lt;kristian@spritelink.net&gt;\"\n ---&gt; Using cache\n ---&gt; b8a0857a144e\nStep 3/12 : LABEL maintainer=\"Roman Dodin &lt;dodin.roman@gmail.com&gt;\"\n ---&gt; Using cache\n ---&gt; a22af11cc083\nStep 4/12 : ARG DEBIAN_FRONTEND=noninteractive\n ---&gt; Using cache\n ---&gt; 1735e5bccc44\nStep 5/12 : RUN apt-get update -qy  &amp;&amp; apt-get upgrade -qy  &amp;&amp; apt-get install -y     bridge-utils     iproute2     python3-ipy     socat     qemu-kvm     tcpdump     tftpd-hpa     ssh     inetutils-ping     dnsutils     openvswitch-switch     iptables     telnet  &amp;&amp; rm -rf /var/lib/apt/lists/*\n ---&gt; Using cache\n ---&gt; 43deb024677e\nStep 6/12 : ARG IMAGE\n ---&gt; Using cache\n ---&gt; 36e30f700548\nStep 7/12 : COPY $IMAGE* /\n ---&gt; Using cache\n ---&gt; 66fbaa5d2045\nStep 8/12 : COPY OVMF.fd /\n ---&gt; Using cache\n ---&gt; 87c96b2ebf08\nStep 9/12 : COPY *.py /\n ---&gt; Using cache\n ---&gt; 756902fd2036\nStep 10/12 : EXPOSE 22 80 161/udp 443 830 5000 6030 10000-10099 57400\n ---&gt; Using cache\n ---&gt; df3be4a02195\nStep 11/12 : HEALTHCHECK CMD [\"/healthcheck.py\"]\n ---&gt; Using cache\n ---&gt; 5143e36e8998\nStep 12/12 : ENTRYPOINT [\"/launch.py\"]\n ---&gt; Using cache\n ---&gt; 0eb634fdcd16\nSuccessfully built 0eb634fdcd16\nSuccessfully tagged vrnetlab/vr-n9kv:9.3.9\nmake[1]: Leaving directory '/root/vrnetlab/n9kv'\n</code></pre> <p>You should now see this image available to use:</p> <pre><code>root@aninchat-ubuntu:~/vrnetlab/n9kv# docker images | grep n9k\nvrnetlab/vr-n9kv     9.3.9       0eb634fdcd16   2 weeks ago     2.41GB\n</code></pre>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#end-hosts","title":"End hosts","text":"<p>For hosts, I like to use Michael's host docker image (networkop/host) which are based on Ubuntu 20.04. You can pull the latest tag ('ifreload') from dockerhub.</p> <pre><code>root@aninchat-ubuntu:~# docker pull networkop/host:ifreload\nifreload: Pulling from networkop/host\nf7ec5a41d630: Pull complete\ndcf92054a263: Pull complete\n3179c3d1124f: Pull complete\neea897900801: Pull complete\n717337aab6d8: Pull complete\n23ffec7efd97: Pull complete\n6bdc05c9f934: Pull complete\nDigest: sha256:ed38567dc55f88cda67a208eae36f6a6569b461c767f7be6db04298ae89f3fc5\nStatus: Downloaded newer image for networkop/host:ifreload\ndocker.io/networkop/host:ifreload\n</code></pre> <p>This image should now be available to use:</p> <pre><code>root@aninchat-ubuntu:~# docker images | grep host\nnetworkop/host       ifreload    b73cafd09ad8   7 months ago    283MB\n</code></pre> <p>We have all our vendor images built and ready to go now. </p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#the-containerlab-topology","title":"The Containerlab topology","text":"","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#the-topology-file","title":"The topology file","text":"<p>The following topology file describes the nodes and the interconnections between them for containerlab:</p> <pre><code>root@aninchat-ubuntu:~/clabs/multivendor# cat evpn-multivendor.yml\nname: evpn-l2-multivendor\ntopology:\n  nodes:\n    spine1:\n            kind: vr-veos\n            image: vrnetlab/vr-veos:4.27.2F\n            mgmt_ipv4: 172.20.20.101\n    spine2:\n            kind: vr-veos\n            image: vrnetlab/vr-veos:4.27.2F\n            mgmt_ipv4: 172.20.20.102\n    leaf1:\n            kind: vr-veos\n            image: vrnetlab/vr-veos:4.27.2F\n            mgmt_ipv4: 172.20.20.11\n    leaf2:\n            kind: cvx\n            image: networkop/cx:4.4.0\n    leaf3:\n            kind: vr-n9kv\n            image: vrnetlab/vr-n9kv:9.3.9\n            mgmt_ipv4: 172.20.20.13\n    leaf4:\n            kind: vr-vqfx\n            image: vrnetlab/vr-vqfx:20.2R1.10\n            mgmt_ipv4: 172.20.20.14\n    h1:\n            kind: linux\n            image: networkop/host:ifreload\n            binds:\n                    - hosts/h1_interfaces:/etc/network/interfaces\n            mgmt_ipv4: 172.20.20.21\n    h2:\n            kind: linux\n            image: networkop/host:ifreload\n            binds:\n                    - hosts/h2_interfaces:/etc/network/interfaces\n            mgmt_ipv4: 172.20.20.22\n    h3:\n            kind: linux\n            image: networkop/host:ifreload\n            binds:\n                    - hosts/h3_interfaces:/etc/network/interfaces\n            mgmt_ipv4: 172.20.20.23\n    h4:\n            kind: linux\n            image: networkop/host:ifreload\n            binds:\n                    - hosts/h4_interfaces:/etc/network/interfaces\n            mgmt_ipv4: 172.20.20.24\n  links:\n          - endpoints: [\"leaf1:eth1\", \"spine1:eth1\"]\n          - endpoints: [\"leaf1:eth2\", \"spine2:eth1\"]\n          - endpoints: [\"leaf2:swp1\", \"spine1:eth2\"]\n          - endpoints: [\"leaf2:swp2\", \"spine2:eth2\"]\n          - endpoints: [\"leaf3:eth1\", \"spine1:eth3\"]\n          - endpoints: [\"leaf3:eth2\", \"spine2:eth3\"]\n          - endpoints: [\"leaf4:eth1\", \"spine1:eth4\"]\n          - endpoints: [\"leaf4:eth2\", \"spine2:eth4\"]\n          - endpoints: [\"leaf1:eth3\", \"h1:eth1\"]\n          - endpoints: [\"leaf2:swp3\", \"h2:eth1\"]\n          - endpoints: [\"leaf3:eth3\", \"h3:eth1\"]\n          - endpoints: [\"leaf4:eth3\", \"h4:eth1\"]\n</code></pre>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#using-binds-in-the-topology-file","title":"Using 'binds' in the topology file","text":"<p>Note that the 'binds' block allows us to bind a file in the host OS to the container itself. In this case, I am simply pre-configuring the network interfaces for the fabric hosts by copying relevant configuration into the /etc/network/interfaces file of the container.</p> <p>For example, let's take host h1 and it's file, hosts/h1_interfaces:</p> <pre><code>root@aninchat-ubuntu:~/clabs/multivendor# cat hosts/h1_interfaces\nauto lo\niface lo inet loopback\n\nauto eth1\niface eth1 inet static\n    address 10.100.100.1\n    netmask 255.255.255.0\n</code></pre> <p>All my hosts are in the same subnet, so h2 is 10.100.100.2/24, h3 is 10.100.100.3/24 and h4 is 10.100.100.4/24. </p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#deploying-the-lab","title":"Deploying the lab","text":"<p>We can now deploy this lab using 'containerlab deploy':</p> <pre><code>root@aninchat-ubuntu:~/clabs/multivendor# containerlab deploy --topo evpn-multivendor.yml\nINFO[0000] Containerlab v0.23.0 started\nINFO[0000] Parsing &amp; checking topology file: evpn-multivendor.yml\nINFO[0000] Creating lab directory: /root/clabs/multivendor/clab-evpn-l2-multivendor\nINFO[0000] Creating docker network: Name='clab', IPv4Subnet='172.20.20.0/24', IPv6Subnet='2001:172:20:20::/64', MTU='1500'\nINFO[0000] Creating container: h4\nINFO[0000] Creating container: h3\nINFO[0000] Creating container: spine2\nINFO[0000] Creating container: leaf1\nINFO[0000] Creating container: leaf3\nINFO[0000] Creating container: leaf4\nINFO[0000] Creating container: h1\nINFO[0000] Creating container: h2\nINFO[0000] Creating container: spine1\nINFO[0001] Creating virtual wire: leaf1:eth3 &lt;--&gt; h1:eth1\nINFO[0001] Creating virtual wire: leaf4:eth3 &lt;--&gt; h4:eth1\nINFO[0001] Creating virtual wire: leaf4:eth2 &lt;--&gt; spine2:eth4\nINFO[0001] Creating virtual wire: leaf1:eth2 &lt;--&gt; spine2:eth1\nINFO[0002] Creating virtual wire: leaf1:eth1 &lt;--&gt; spine1:eth1\nINFO[0002] Creating virtual wire: leaf4:eth1 &lt;--&gt; spine1:eth4\nINFO[0002] Creating virtual wire: leaf3:eth1 &lt;--&gt; spine1:eth3\nINFO[0002] Creating virtual wire: leaf3:eth2 &lt;--&gt; spine2:eth3\nINFO[0002] Creating virtual wire: leaf3:eth3 &lt;--&gt; h3:eth1\nINFO[0002] Networking is handled by \"docker-bridge\"\nINFO[0002] Started Firecracker VM \"b01b941136a18c36\" in a container with ID \"86191b18f7f9fd90011aa9627ee972c882beb0d2fdb43a98e45347ccd2ca4014\"\nINFO[0002] Creating virtual wire: leaf2:swp1 &lt;--&gt; spine1:eth2\nINFO[0002] Creating virtual wire: leaf2:swp2 &lt;--&gt; spine2:eth2\nINFO[0002] Creating virtual wire: leaf2:swp3 &lt;--&gt; h2:eth1\nINFO[0003] Adding containerlab host entries to /etc/hosts file\nINFO[0003] \ud83c\udf89 New containerlab version 0.24.1 is available! Release notes: https://containerlab.srlinux.dev/rn/0.24/#0241\nRun 'containerlab version upgrade' to upgrade or go check other installation options at https://containerlab.srlinux.dev/install/\n+----+---------------------------------+--------------+------------------------------+---------+---------+------------------+----------------------+\n| #  |              Name               | Container ID |            Image             |  Kind   |  State  |   IPv4 Address   |     IPv6 Address     |\n+----+---------------------------------+--------------+------------------------------+---------+---------+------------------+----------------------+\n|  1 | clab-evpn-l2-multivendor-h1     | e54edb974d39 | networkop/host:ifreload      | linux   | running | 172.20.20.21/24  | 2001:172:20:20::4/64 |\n|  2 | clab-evpn-l2-multivendor-h2     | 11a73b7514c0 | networkop/host:ifreload      | linux   | running | 172.20.20.22/24  | 2001:172:20:20::5/64 |\n|  3 | clab-evpn-l2-multivendor-h3     | 9f22eff7b0ca | networkop/host:ifreload      | linux   | running | 172.20.20.23/24  | 2001:172:20:20::8/64 |\n|  4 | clab-evpn-l2-multivendor-h4     | a02ead147994 | networkop/host:ifreload      | linux   | running | 172.20.20.24/24  | 2001:172:20:20::3/64 |\n|  5 | clab-evpn-l2-multivendor-leaf1  | ff2871985b4e | vrnetlab/vr-veos:4.27.2F     | vr-veos | running | 172.20.20.11/24  | 2001:172:20:20::2/64 |\n|  6 | clab-evpn-l2-multivendor-leaf2  | 86191b18f7f9 | docker.io/networkop/cx:4.4.0 | cvx     | running | 172.17.0.2/24    | N/A                  |\n|  7 | clab-evpn-l2-multivendor-leaf3  | 8a317c31929d | vrnetlab/vr-n9kv:9.3.9       | vr-n9kv | running | 172.20.20.13/24  | 2001:172:20:20::9/64 |\n|  8 | clab-evpn-l2-multivendor-leaf4  | 690414536e2b | vrnetlab/vr-vqfx:20.2R1.10   | vr-vqfx | running | 172.20.20.14/24  | 2001:172:20:20::6/64 |\n|  9 | clab-evpn-l2-multivendor-spine1 | b628b4de29b9 | vrnetlab/vr-veos:4.27.2F     | vr-veos | running | 172.20.20.101/24 | 2001:172:20:20::a/64 |\n| 10 | clab-evpn-l2-multivendor-spine2 | 3f426a4c2db4 | vrnetlab/vr-veos:4.27.2F     | vr-veos | running | 172.20.20.102/24 | 2001:172:20:20::7/64 |\n+----+---------------------------------+--------------+------------------------------+---------+---------+------------------+----------------------+\n</code></pre> <p>Remember to wait till the containers are reported as healthy. If they remain unhealthy, then something was wrong with the docker build and you should probably re-visit that. </p> <pre><code>root@aninchat-ubuntu:~/clabs/multivendor# docker ps\nCONTAINER ID   IMAGE                        COMMAND                  CREATED         STATUS                   PORTS                                                                                       NAMES\n86191b18f7f9   networkop/ignite:dev         \"/usr/local/bin/igni\u2026\"   8 minutes ago   Up 7 minutes                                                                                                         ignite-b01b941136a18c36\nb628b4de29b9   vrnetlab/vr-veos:4.27.2F     \"/launch.py --userna\u2026\"   8 minutes ago   Up 7 minutes (healthy)   22/tcp, 80/tcp, 443/tcp, 830/tcp, 5000/tcp, 6030/tcp, 10000-10099/tcp, 57400/tcp, 161/udp   clab-evpn-l2-multivendor-spine1\n8a317c31929d   vrnetlab/vr-n9kv:9.3.9       \"/launch.py --userna\u2026\"   8 minutes ago   Up 7 minutes (healthy)   22/tcp, 80/tcp, 443/tcp, 830/tcp, 5000/tcp, 6030/tcp, 10000-10099/tcp, 57400/tcp, 161/udp   clab-evpn-l2-multivendor-leaf3\nff2871985b4e   vrnetlab/vr-veos:4.27.2F     \"/launch.py --userna\u2026\"   8 minutes ago   Up 8 minutes (healthy)   22/tcp, 80/tcp, 443/tcp, 830/tcp, 5000/tcp, 6030/tcp, 10000-10099/tcp, 57400/tcp, 161/udp   clab-evpn-l2-multivendor-leaf1\n3f426a4c2db4   vrnetlab/vr-veos:4.27.2F     \"/launch.py --userna\u2026\"   8 minutes ago   Up 7 minutes (healthy)   22/tcp, 80/tcp, 443/tcp, 830/tcp, 5000/tcp, 6030/tcp, 10000-10099/tcp, 57400/tcp, 161/udp   clab-evpn-l2-multivendor-spine2\n690414536e2b   vrnetlab/vr-vqfx:20.2R1.10   \"/launch.py --userna\u2026\"   8 minutes ago   Up 7 minutes (healthy)   22/tcp, 830/tcp, 5000/tcp, 10000-10099/tcp, 161/udp                                         clab-evpn-l2-multivendor-leaf4\ne54edb974d39   networkop/host:ifreload      \"/entrypoint.sh\"         8 minutes ago   Up 7 minutes                                                                                                         clab-evpn-l2-multivendor-h1\n11a73b7514c0   networkop/host:ifreload      \"/entrypoint.sh\"         8 minutes ago   Up 7 minutes                                                                                                         clab-evpn-l2-multivendor-h2\na02ead147994   networkop/host:ifreload      \"/entrypoint.sh\"         8 minutes ago   Up 7 minutes                                                                                                         clab-evpn-l2-multivendor-h4\n9f22eff7b0ca   networkop/host:ifreload      \"/entrypoint.sh\"         8 minutes ago   Up 7 minutes                                                                                                         clab-evpn-l2-multivendor-h3\n</code></pre>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#automating-the-fabric-bringup","title":"Automating the fabric bringup","text":"<p>In order to automate the fabric bringup, I've written a (terrible) Ansible script. The script can be found here. This is meant to configure the p2p interfaces between the leafs and the spines, underlay routing (BGP), overlay routing (BGP EVPN), and all of the necessary VXLAN and VLAN configuration for a L2 overlay.</p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#ansible-inventory-and-variables-for-fabric-deployment","title":"Ansible inventory and variables for fabric deployment","text":"<p>The inventory for the script is built from the IP addresses that were statically assigned in the containerlab topology file. The inventory is also crucial to grouping together devices, allowing for group variables to kick in:</p> <pre><code>root@aninchat-ubuntu:~/clabs/multivendor# cat inventory.yml\n---\n\nall:\n        children:\n                leafs:\n                        hosts:\n                                leaf1:\n                                        ansible_host: 172.20.20.11\n                                leaf2:\n                                        ansible_host: 172.17.0.2\n                                leaf3:\n                                        ansible_host: 172.20.20.13\n                                leaf4:\n                                        ansible_host: 172.20.20.14\n                spines:\n                        hosts:\n                                spine1:\n                                        ansible_host: 172.20.20.101\n                                spine2:\n                                        ansible_host: 172.20.20.102\n                nxos:\n                        hosts:\n                                leaf3:\n                junos:\n                        hosts:\n                                leaf4:\n                eos:\n                        hosts:\n                                spine1:\n                                spine2:\n                                leaf1:\n                cumulus:\n                        hosts:\n                                leaf2:\n</code></pre> <p>Each fabric node has its own host variables defined under 'host_vars'. </p> <pre><code>root@aninchat-ubuntu:~/clabs/multivendor# tree host_vars/\nhost_vars/\n\u251c\u2500\u2500 leaf1.yml\n\u251c\u2500\u2500 leaf2.yml\n\u251c\u2500\u2500 leaf3.yml\n\u251c\u2500\u2500 leaf4.yml\n\u251c\u2500\u2500 spine1.yml\n\u2514\u2500\u2500 spine2.yml\n</code></pre> <p>Let's take leaf1 as an example to see what host specific variables we've defined:</p> <pre><code>root@aninchat-ubuntu:~/clabs/multivendor# cat host_vars/leaf1.yml\n---\n\ninterfaces:\n        Loopback0:\n                address: '192.168.100.1/32'\n        Ethernet1:\n                address: '10.10.10.1/31'\n        Ethernet2:\n                address: '10.10.10.9/31'\nbgp:\n        as_number: 64521\n        router_id: '192.168.100.1'\n        neighbors:\n                10.10.10.0:\n                        remote-as: 65500\n                        ipv4: true\n                        evpn: true\n                        send-community: true\n                        loopback: '192.168.101.1'\n                10.10.10.8:\n                        remote-as: 65500\n                        ipv4: true\n                        evpn: true\n                        send-community: true\n                        loopback: '192.168.101.2'\nvlans:\n        100:\n                vni: 10100\n                name: 'VLAN100-VNI10100'\n                interfaces:\n                        - 'Ethernet3'\n                rd: '192.168.100.1:1'\n                export_rt: '100:100'\n                import_rt: '100:100'\n</code></pre> <p>We're essentially defining fabric specific attributes that will be used to build the configuration of the device via a jinja2 template in the ansible playbook.</p> <p>Grouping together devices allows us to now define group variables as well, under 'group_vars':</p> <pre><code>root@aninchat-ubuntu:~/clabs/multivendor# tree group_vars/\ngroup_vars/\n\u251c\u2500\u2500 cumulus.yml\n\u251c\u2500\u2500 eos.yml\n\u251c\u2500\u2500 junos.yml\n\u251c\u2500\u2500 leafs.yml\n\u251c\u2500\u2500 nxos.yml\n\u2514\u2500\u2500 spines.yml\n</code></pre> <p>These mostly contain connection details, and some other variables I've set to create some conditionals in the ansible script. For example, let's look at the eos.yml file from 'group_vars':</p> <pre><code>root@aninchat-ubuntu:~/clabs/multivendor# cat group_vars/eos.yml\n---\n\nansible_connection: ansible.netcommon.network_cli\nansible_network_os: eos\nansible_user: admin\nansible_password: admin\nplatform: 'eos'\nansible_become: yes\nansible_become_method: enable\n</code></pre> <p>To ensure that your inventory is built as expected, and all of the variables are inherited correctly, we can use the 'ansible-inventory' command as follows (only a snippet is shown here for brevity):</p> <pre><code>root@aninchat-ubuntu:~/clabs/multivendor# ansible-inventory inventory.yml --list\n{\n    \"_meta\": {\n        \"hostvars\": {\n            \"leaf1\": {\n                \"ansible_become\": true,\n                \"ansible_become_method\": \"enable\",\n                \"ansible_connection\": \"ansible.netcommon.network_cli\",\n                \"ansible_host\": \"172.20.20.11\",\n                \"ansible_network_os\": \"eos\",\n                \"ansible_password\": \"admin\",\n                \"ansible_user\": \"admin\",\n                \"bgp\": {\n                    \"as_number\": 64521,\n                    \"neighbors\": {\n                        \"10.10.10.0\": {\n                            \"evpn\": true,\n                            \"ipv4\": true,\n                            \"loopback\": \"192.168.101.1\",\n                            \"remote-as\": 65500,\n                            \"send-community\": true\n                        },\n                        \"10.10.10.8\": {\n                            \"evpn\": true,\n                            \"ipv4\": true,\n                            \"loopback\": \"192.168.101.2\",\n                            \"remote-as\": 65500,\n                            \"send-community\": true\n                        }\n                    },\n                    \"router_id\": \"192.168.100.1\"\n                },\n                \"interfaces\": {\n                    \"Ethernet1\": {\n                        \"address\": \"10.10.10.1/31\"\n                    },\n                    \"Ethernet2\": {\n                        \"address\": \"10.10.10.9/31\"\n                    },\n                    \"Loopback0\": {\n                        \"address\": \"192.168.100.1/32\"\n                    }\n                },\n                \"platform\": \"eos\",\n                \"role\": \"leaf\",\n                \"vlans\": {\n                    \"100\": {\n                        \"export_rt\": \"100:100\",\n                        \"import_rt\": \"100:100\",\n                        \"interfaces\": [\n                            \"Ethernet3\"\n                        ],\n                        \"name\": \"VLAN100-VNI10100\",\n                        \"rd\": \"192.168.100.1:1\",\n                        \"vni\": 10100\n                    }\n                }\n            },\n\n* snip *\n\n    \"all\": {\n        \"children\": [\n            \"cumulus\",\n            \"eos\",\n            \"junos\",\n            \"leafs\",\n            \"nxos\",\n            \"spines\",\n            \"ungrouped\"\n        ]\n    },\n    \"cumulus\": {\n        \"hosts\": [\n            \"leaf2\"\n        ]\n    },\n    \"eos\": {\n        \"hosts\": [\n            \"leaf1\",\n            \"spine1\",\n            \"spine2\"\n        ]\n    },\n    \"junos\": {\n        \"hosts\": [\n            \"leaf4\"\n        ]\n    },\n    \"leafs\": {\n        \"hosts\": [\n            \"leaf1\",\n            \"leaf2\",\n            \"leaf3\",\n            \"leaf4\"\n        ]\n    },\n    \"nxos\": {\n        \"hosts\": [\n            \"leaf3\"\n        ]\n    },\n    \"spines\": {\n        \"hosts\": [\n            \"spine1\",\n            \"spine2\"\n        ]\n    }\n}\n</code></pre>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#jinja2-templates-for-building-device-configuration","title":"Jinja2 templates for building device configuration","text":"<p>Jinja2 templates are awesome to build the actual configuration. This also lends very well into a multivendor deployment, where each vendor may have a different style of configuration and you cannot have a pre-defined set for all network operating systems. All templates are stored under the 'templates' folder:</p> <pre><code>root@aninchat-ubuntu:~/clabs/multivendor# tree templates/\ntemplates/\n\u251c\u2500\u2500 eos\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 access_interfaces_to_vlan_mapping.j2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bgp_evpn_vlan_vni_addition.j2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bgp_leaf.j2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bgp_spine.j2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 route_map.j2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 underlay_interfaces.j2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 vlans_create.j2\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 vxlan_interface_map_vni.j2\n\u251c\u2500\u2500 junos\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 access_interfaces_to_vlan_mapping.j2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bgp_evpn_vlan_vni_addition.j2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bgp_leaf.j2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 route_map.j2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 underlay_interfaces.j2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 vlans_create.j2\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 vxlan_interface_map_vni.j2\n\u2514\u2500\u2500 nxos\n    \u251c\u2500\u2500 access_interfaces_to_vlan_mapping.j2\n    \u251c\u2500\u2500 bgp_evpn_vlan_vni_addition.j2\n    \u251c\u2500\u2500 bgp_leaf.j2\n    \u251c\u2500\u2500 route_map.j2\n    \u251c\u2500\u2500 underlay_interfaces.j2\n    \u251c\u2500\u2500 vlans_create.j2\n    \u2514\u2500\u2500 vxlan_interface_map_vni.j2\n\n4 directories, 23 files\n</code></pre> <p>The directory is structured per vendor/OS (excluding Cumulus, see quirks section below for this). For every vendor, I have a jinja2 template for different parts of the configuration. For example, let's take EOS and NXOS to compaare.</p> <p>For EOS:</p> <pre><code>root@aninchat-ubuntu:~/clabs/multivendor# cat templates/eos/underlay_interfaces.j2\n{% for interface in interfaces -%}\ninterface {{ interface }}\n   {% if interface == 'Loopback0' -%}\n   ip address {{ interfaces[interface].address }}\n   {% else -%}\n   no switchport\n   ip address {{ interfaces[interface].address }}\n   {% endif %}\n{% endfor %}\n</code></pre> <p>For NXOS:</p> <pre><code>root@aninchat-ubuntu:~/clabs/multivendor# cat templates/nxos/underlay_interfaces.j2\nfeature lldp\n{% for interface in interfaces -%}\ninterface {{ interface }}\n  {% if interface == 'loopback0' -%}\n  ip address {{ interfaces[interface].address }}\n  {% else -%}\n  no switchport\n  ip address {{ interfaces[interface].address }}\n  no shutdown\n  {% endif %}\n{% endfor -%}\n</code></pre> <p>As you can see, there are three spaces for EOS configuration, while there are two spaces for NXOS configuration. This becomes crucial for idempotency. This jinja2 template is finally used in the playbook, as follows:</p> <pre><code>          - name: configure interfaces on network devices\n            block:\n                    - name: configure interfaces on non-cumulus switches\n                      when: platform != 'cumulus'\n                      cli_config:\n                              config: \"{{ lookup('template', 'templates/{{ platform }}/underlay_interfaces.j2') }}\"\n</code></pre> <p>'{{ platform }}' is substituted in real time with the actual platform variable from the flat inventory. This simple hierarchy structure allows for easy lookup into a vendor specific template.</p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#small-quirks-and-other-noteworthy-points-regarding-fabric-automation","title":"Small quirks and other noteworthy points regarding fabric automation","text":"<p>The playbook can now be executed to automate the full deployment. Couple of noteworthy things, specific to some of these vendors and their quirks:</p> <ol> <li>with Arista's EOS (and vEOS), you need to enable multi-agent service model (the default is ribd, which is single agent). The catch here is that after enabling this, the device must be reloaded - this is true even for the actual hardware platforms, and not just vEOS. As part of this automation, I am enabling multi-agent, but you, as the user, must reload the box at least once to get BGP peerings to come up (you'll see IPv4 unicast BGP come up, but no other address family will work until a reload).</li> <li>Juniper's vQFX takes a bit to load all of the network interfaces (the 'xe' interfaces). You need to be patient. You can verify if the interfaces have come up using the 'show interfaces terse' command. Until then, do not run the ansible playbook. </li> </ol> <p>Outside of these things, there were certain places where my hands felt like they were tied while building the actual ansible playbook. For example, for Cumulus' automation, I use the 'community.network.nclu' ansible module. This module can take in a template for configuration, however, it doesn't seem to take a template path and instead, expects you to define an inline template, which is really odd. This bloats up the playbook considerably, an example (taken from the playbook) below:</p> <pre><code>          - name: configure interfaces on network devices\n            block:\n                    - name: configure interfaces on non-cumulus switches\n                      when: platform != 'cumulus'\n                      cli_config:\n                              config: \"{{ lookup('template', 'templates/{{ platform }}/underlay_interfaces.j2') }}\"\n                    - name: configure interfaces on cumulus devices\n                      when: platform == 'cumulus'\n                      community.network.nclu:\n                              template: |\n                                      {% for interface in interfaces -%}\n                                      {% if interface == 'loopback0' -%}\n                                      add loopback lo ip address {{ interfaces[interface].address }}\n                                      {% else -%}\n                                      add interface {{ interface }} ip address {{ interfaces[interface].address }}\n                                      {% endif -%}\n                                      {% endfor -%}\n                              commit: yes\n</code></pre> <p>Along the same lines, for all other vendors I could use the network_cli connection but not for Cumulus. Because of this, each task in my playbook had to be divided into blocks, where one block would configure non-cumulus switches and the other would configure cumulus switches. This was done using simple conditional checks (using the 'when' statement in Ansible).</p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#verification-of-the-evpn-fabric","title":"Verification of the EVPN fabric","text":"","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#basic-connectivity-checks","title":"Basic connectivity checks","text":"<p>In this section, let's look at the fabric and if everything is automated correctly or not. Both the spines have an IPv4 unicast peering with all the leafs:</p> <pre><code>// spine1\n\nspine1#show bgp ipv4 unicast summary\nBGP summary information for VRF default\nRouter identifier 192.168.101.1, local AS number 65500\nNeighbor Status Codes: m - Under maintenance\n  Neighbor   V AS           MsgRcvd   MsgSent  InQ OutQ  Up/Down State   PfxRcd PfxAcc\n  10.10.10.1 4 64521            115       117    0    0 01:32:25 Estab   1      1\n  10.10.10.3 4 64522           1860      2172    0    0 01:32:25 Estab   1      1\n  10.10.10.5 4 64523             97       115    0    0 01:32:25 Estab   1      1\n  10.10.10.7 4 64524            207       223    0    0 01:32:26 Estab   1      1\n\n// spine2\n\nspine2#show bgp ipv4 unicast summary\nBGP summary information for VRF default\nRouter identifier 192.168.101.2, local AS number 65500\nNeighbor Status Codes: m - Under maintenance\n  Neighbor    V AS           MsgRcvd   MsgSent  InQ OutQ  Up/Down State   PfxRcd PfxAcc\n  10.10.10.9  4 64521            119       119    0    0 01:34:07 Estab   1      1\n  10.10.10.11 4 64522           1893      2211    0    0 01:34:07 Estab   1      1\n  10.10.10.13 4 64523             99       118    0    0 01:34:06 Estab   1      1\n  10.10.10.15 4 64524            211       226    0    0 01:34:06 Estab   1      1\n</code></pre> <p>The EVPN peering is also up:</p> <pre><code>// spine1\n\nspine1#show bgp evpn summary\nBGP summary information for VRF default\nRouter identifier 192.168.101.1, local AS number 65500\nNeighbor Status Codes: m - Under maintenance\n  Neighbor      V AS           MsgRcvd   MsgSent  InQ OutQ  Up/Down State   PfxRcd PfxAcc\n  192.168.100.1 4 64521            116       119    0    0 01:33:50 Estab   2      2\n  192.168.100.2 4 64522           1887      2211    0    0 01:33:50 Estab   3      3\n  192.168.100.3 4 64523             99       117    0    0 01:33:49 Estab   2      2\n  192.168.100.4 4 64524            211       229    0    0 01:33:51 Estab   2      2\n\n// spine2\n\nspine2#show bgp evpn summary\nBGP summary information for VRF default\nRouter identifier 192.168.101.2, local AS number 65500\nNeighbor Status Codes: m - Under maintenance\n  Neighbor      V AS           MsgRcvd   MsgSent  InQ OutQ  Up/Down State   PfxRcd PfxAcc\n  192.168.100.1 4 64521            120       121    0    0 01:34:18 Estab   2      2\n  192.168.100.2 4 64522           1896      2219    0    0 01:34:16 Estab   3      3\n  192.168.100.3 4 64523            100       118    0    0 01:34:15 Estab   2      2\n  192.168.100.4 4 64524            212       230    0    0 01:34:16 Estab   2      2\n</code></pre> <p>Let's take leaf1 now and confirm if it has received the loopback of all other leafs, and if it has reachability to these loopbacks:</p> <pre><code>leaf1#show ip route\n\nVRF: default\nCodes: C - connected, S - static, K - kernel,\n       O - OSPF, IA - OSPF inter area, E1 - OSPF external type 1,\n       E2 - OSPF external type 2, N1 - OSPF NSSA external type 1,\n       N2 - OSPF NSSA external type2, B - Other BGP Routes,\n       B I - iBGP, B E - eBGP, R - RIP, I L1 - IS-IS level 1,\n       I L2 - IS-IS level 2, O3 - OSPFv3, A B - BGP Aggregate,\n       A O - OSPF Summary, NG - Nexthop Group Static Route,\n       V - VXLAN Control Service, M - Martian,\n       DH - DHCP client installed default route,\n       DP - Dynamic Policy Route, L - VRF Leaked,\n       G  - gRIBI, RC - Route Cache Route\n\nGateway of last resort is not set\n\n C        10.0.0.0/24 is directly connected, Management1\n C        10.10.10.0/31 is directly connected, Ethernet1\n C        10.10.10.8/31 is directly connected, Ethernet2\n C        192.168.100.1/32 is directly connected, Loopback0\n B E      192.168.100.2/32 [200/0] via 10.10.10.0, Ethernet1\n                                   via 10.10.10.8, Ethernet2\n B E      192.168.100.3/32 [200/0] via 10.10.10.0, Ethernet1\n                                   via 10.10.10.8, Ethernet2\n B E      192.168.100.4/32 [200/0] via 10.10.10.0, Ethernet1\n                                   via 10.10.10.8, Ethernet2\n B E      192.168.101.1/32 [200/0] via 10.10.10.0, Ethernet1\n B E      192.168.101.2/32 [200/0] via 10.10.10.8, Ethernet2\n</code></pre> <p>We see all the loopbacks in there, which is good. Let's ping to confirm reachability, using leaf1s loopback as the source:</p> <pre><code>leaf1#ping 192.168.100.2 source 192.168.100.1\nPING 192.168.100.2 (192.168.100.2) from 192.168.100.1 : 72(100) bytes of data.\n80 bytes from 192.168.100.2: icmp_seq=1 ttl=63 time=9.03 ms\n80 bytes from 192.168.100.2: icmp_seq=2 ttl=63 time=14.8 ms\n80 bytes from 192.168.100.2: icmp_seq=3 ttl=63 time=9.03 ms\n80 bytes from 192.168.100.2: icmp_seq=4 ttl=63 time=9.23 ms\n80 bytes from 192.168.100.2: icmp_seq=5 ttl=63 time=12.1 ms\n\n--- 192.168.100.2 ping statistics ---\n5 packets transmitted, 5 received, 0% packet loss, time 41ms\nrtt min/avg/max/mdev = 9.034/10.847/14.807/2.305 ms, pipe 2, ipg/ewma 10.474/9.926 ms\n\nleaf1#ping 192.168.100.3 source 192.168.100.1\nPING 192.168.100.3 (192.168.100.3) from 192.168.100.1 : 72(100) bytes of data.\n80 bytes from 192.168.100.3: icmp_seq=1 ttl=254 time=11.6 ms\n80 bytes from 192.168.100.3: icmp_seq=2 ttl=254 time=12.1 ms\n80 bytes from 192.168.100.3: icmp_seq=3 ttl=254 time=8.81 ms\n80 bytes from 192.168.100.3: icmp_seq=4 ttl=254 time=7.50 ms\n80 bytes from 192.168.100.3: icmp_seq=5 ttl=254 time=8.88 ms\n\n--- 192.168.100.3 ping statistics ---\n5 packets transmitted, 5 received, 0% packet loss, time 47ms\nrtt min/avg/max/mdev = 7.502/9.783/12.108/1.774 ms, pipe 2, ipg/ewma 11.764/10.594 ms\n\nleaf1#ping 192.168.100.4 source 192.168.100.1\nPING 192.168.100.4 (192.168.100.4) from 192.168.100.1 : 72(100) bytes of data.\n80 bytes from 192.168.100.4: icmp_seq=1 ttl=63 time=112 ms\n80 bytes from 192.168.100.4: icmp_seq=2 ttl=63 time=200 ms\n80 bytes from 192.168.100.4: icmp_seq=3 ttl=63 time=192 ms\n80 bytes from 192.168.100.4: icmp_seq=4 ttl=63 time=185 ms\n80 bytes from 192.168.100.4: icmp_seq=5 ttl=63 time=176 ms\n\n--- 192.168.100.4 ping statistics ---\n5 packets transmitted, 5 received, 0% packet loss, time 42ms\nrtt min/avg/max/mdev = 112.935/173.590/200.128/31.282 ms, pipe 5, ipg/ewma 10.747/143.784 ms\n</code></pre>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#analyzing-the-evpn-routes-for-interoperability-issues","title":"Analyzing the EVPN routes for interoperability issues","text":"","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#clue-1","title":"Clue #1","text":"<p>Let's start by looking at some EVPN routes now. I am obviously aware of some of the interoperability issues in this network, so I'll start dropping hints to help you along the way.</p> <p>First, because I am using ingress replication, I should see a type-3 IMET route generated by each leaf. We'll stay on leaf1 for this analysis:</p> <pre><code>leaf1#show bgp evpn route-type imet\nBGP routing table information for VRF default\nRouter identifier 192.168.100.1, local AS number 64521\nRoute status codes: s - suppressed, * - valid, &gt; - active, E - ECMP head, e - ECMP\n                    S - Stale, c - Contributing to ECMP, b - backup\n                    % - Pending BGP convergence\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nAS Path Attributes: Or-ID - Originator ID, C-LST - Cluster List, LL Nexthop - Link Local Nexthop\n\n          Network                Next Hop              Metric  LocPref Weight  Path\n * &gt;Ec   RD: 192.168.100.2:2 imet 192.168.100.2\n                                 192.168.100.2         -       100     0       65500 64522 i\n *  ec   RD: 192.168.100.2:2 imet 192.168.100.2\n                                 192.168.100.2         -       100     0       65500 64522 i\n * &gt;Ec   RD: 192.168.100.3:3 imet 192.168.100.3\n                                 192.168.100.3         -       100     0       65500 64523 i\n *  ec   RD: 192.168.100.3:3 imet 192.168.100.3\n                                 192.168.100.3         -       100     0       65500 64523 i\n * &gt;     RD: 192.168.100.1:1 imet 192.168.100.1\n                                 -                     -       -       0       i\n * &gt;Ec   RD: 192.168.100.4:4 imet 10100 192.168.100.4\n                                 192.168.100.4         -       100     0       65500 64524 i\n *  ec   RD: 192.168.100.4:4 imet 10100 192.168.100.4\n                                 192.168.100.4         -       100     0       65500 64524 i\n</code></pre> <p>Notice anything weird in the output here? Anything that stands out? Let's look at two routes for comparison, the imet route from leaf2 and leaf4:</p> <pre><code>leaf1#show bgp evpn route-type imet rd 192.168.100.2:2 detail\nBGP routing table information for VRF default\nRouter identifier 192.168.100.1, local AS number 64521\nBGP routing table entry for imet 192.168.100.2, Route Distinguisher: 192.168.100.2:2\n Paths: 2 available\n  65500 64522\n    192.168.100.2 from 192.168.101.1 (192.168.101.1)\n      Origin IGP, metric -, localpref 100, weight 0, valid, external, ECMP head, ECMP, best, ECMP contributor\n      Extended Community: Route-Target-AS:100:100 TunnelEncap:tunnelTypeVxlan\n      VNI: 10100\n      PMSI Tunnel: Ingress Replication, MPLS Label: 10100, Leaf Information Required: false, Tunnel ID: 192.168.100.2\n  65500 64522\n    192.168.100.2 from 192.168.101.2 (192.168.101.2)\n      Origin IGP, metric -, localpref 100, weight 0, valid, external, ECMP, ECMP contributor\n      Extended Community: Route-Target-AS:100:100 TunnelEncap:tunnelTypeVxlan\n      VNI: 10100\n      PMSI Tunnel: Ingress Replication, MPLS Label: 10100, Leaf Information Required: false, Tunnel ID: 192.168.100.2\n</code></pre> <p>The same thing for leaf4 now:</p> <pre><code>leaf1#show bgp evpn route-type imet rd 192.168.100.4:4 detail\nBGP routing table information for VRF default\nRouter identifier 192.168.100.1, local AS number 64521\nBGP routing table entry for imet 10100 192.168.100.4, Route Distinguisher: 192.168.100.4:4\n Paths: 2 available\n  65500 64524\n    192.168.100.4 from 192.168.101.1 (192.168.101.1)\n      Origin IGP, metric -, localpref 100, weight 0, valid, external, ECMP head, ECMP, best, ECMP contributor\n      Extended Community: Route-Target-AS:100:100 TunnelEncap:tunnelTypeVxlan\n      VNI: 10100\n      PMSI Tunnel: Ingress Replication, MPLS Label: 10100, Leaf Information Required: false, Tunnel ID: 192.168.100.4\n  65500 64524\n    192.168.100.4 from 192.168.101.2 (192.168.101.2)\n      Origin IGP, metric -, localpref 100, weight 0, valid, external, ECMP, ECMP contributor\n      Extended Community: Route-Target-AS:100:100 TunnelEncap:tunnelTypeVxlan\n      VNI: 10100\n      PMSI Tunnel: Ingress Replication, MPLS Label: 10100, Leaf Information Required: false, Tunnel ID: 192.168.100.4\n</code></pre> <p>Do you see it yet?</p> <p>We should also see some type-2 macip routes since all my hosts are up and running:</p> <pre><code>leaf1#show bgp evpn route-type mac-ip\nBGP routing table information for VRF default\nRouter identifier 192.168.100.1, local AS number 64521\nRoute status codes: s - suppressed, * - valid, &gt; - active, E - ECMP head, e - ECMP\n                    S - Stale, c - Contributing to ECMP, b - backup\n                    % - Pending BGP convergence\nOrigin codes: i - IGP, e - EGP, ? - incomplete\nAS Path Attributes: Or-ID - Originator ID, C-LST - Cluster List, LL Nexthop - Link Local Nexthop\n\n          Network                Next Hop              Metric  LocPref Weight  Path\n * &gt;Ec   RD: 192.168.100.3:3 mac-ip aac1.ab48.5bcf\n                                 192.168.100.3         -       100     0       65500 64523 i\n *  ec   RD: 192.168.100.3:3 mac-ip aac1.ab48.5bcf\n                                 192.168.100.3         -       100     0       65500 64523 i\n * &gt;Ec   RD: 192.168.100.2:2 mac-ip aac1.ab93.c413\n                                 192.168.100.2         -       100     0       65500 64522 i\n *  ec   RD: 192.168.100.2:2 mac-ip aac1.ab93.c413\n                                 192.168.100.2         -       100     0       65500 64522 i\n * &gt;Ec   RD: 192.168.100.2:2 mac-ip aac1.ab93.c413 fe80::a8c1:abff:fe93:c413\n                                 192.168.100.2         -       100     0       65500 64522 i\n *  ec   RD: 192.168.100.2:2 mac-ip aac1.ab93.c413 fe80::a8c1:abff:fe93:c413\n                                 192.168.100.2         -       100     0       65500 64522 i\n * &gt;     RD: 192.168.100.1:1 mac-ip aac1.abde.327e\n                                 -                     -       -       0       i\n * &gt;Ec   RD: 192.168.100.4:4 mac-ip 10100 aac1.abe8.a87d\n                                 192.168.100.4         -       100     0       65500 64524 i\n *  ec   RD: 192.168.100.4:4 mac-ip 10100 aac1.abe8.a87d\n                                 192.168.100.4         -       100     0       65500 64524 i\n</code></pre> <p>This is good, we've learnt a type-2 macip route for all our hosts. But again, something is odd here - one of the leafs is slightly different from the others. Which one?</p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#clue-2","title":"Clue #2","text":"<p>Since we're using ingression replication, we should have a flood list of all our leafs via the type-3 imet route. Let's look at the flood list on leaf1:</p> <pre><code>leaf1#show vxlan flood vtep vlan 100\n          VXLAN Flood VTEP Table\n--------------------------------------------------------------------------------\n\nVLANS                            Ip Address\n-----------------------------   ------------------------------------------------\n100                             192.168.100.2   192.168.100.3\n</code></pre> <p>That's interesting - leaf4 (vQFX) is not listed in the flood list. Can you figure out why?</p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#clue-3","title":"Clue #3","text":"<p>We looked at the type-2 macip routes but we never really saw the MAC address table. Since this is L2 overlay only, we should see the host MAC addresses installed in this table, with a vtep as the next hop. </p> <p>Let's look at leaf1s table now:</p> <pre><code>leaf1#show mac address-table unicast\n          Mac Address Table\n------------------------------------------------------------------\n\nVlan    Mac Address       Type        Ports      Moves   Last Move\n----    -----------       ----        -----      -----   ---------\n 100    aac1.ab48.5bcf    DYNAMIC     Vx1        1       2:36:43 ago\n 100    aac1.ab93.c413    DYNAMIC     Vx1        1       2:37:12 ago\n 100    aac1.abde.327e    DYNAMIC     Et3        1       2:32:54 ago\nTotal Mac Addresses for this criterion: 3\n</code></pre> <p>There are only three addresses, but there should be four. You probably know which one is missing - it is h4, the host behind leaf4. But why? </p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#whats-really-happening","title":"What's really happening","text":"<p>You've probably solved it by now, but I'll break it down anyway. If you go back and check, leaf1 did receive both the type-2 macip route for the host, h4, and the type-3 imet route for ingress replication of BUM traffic. </p> <p>Let's confirm again, and take a detailed look at it this time:</p> <pre><code>leaf1#show bgp evpn route mac-ip aa:c1:ab:e8:a8:7d detail\nBGP routing table information for VRF default\nRouter identifier 192.168.100.1, local AS number 64521\nBGP routing table entry for mac-ip 10100 aac1.abe8.a87d, Route Distinguisher: 192.168.100.4:4\n Paths: 2 available\n  65500 64524\n    192.168.100.4 from 192.168.101.1 (192.168.101.1)\n      Origin IGP, metric -, localpref 100, weight 0, valid, external, ECMP head, ECMP, best, ECMP contributor\n      Extended Community: Route-Target-AS:100:100 TunnelEncap:tunnelTypeVxlan\n      VNI: 10100 ESI: 0000:0000:0000:0000:0000\n  65500 64524\n    192.168.100.4 from 192.168.101.2 (192.168.101.2)\n      Origin IGP, metric -, localpref 100, weight 0, valid, external, ECMP, ECMP contributor\n      Extended Community: Route-Target-AS:100:100 TunnelEncap:tunnelTypeVxlan\n      VNI: 10100 ESI: 0000:0000:0000:0000:0000\n\nleaf1#show bgp evpn route-type imet rd 192.168.100.4:4 detail\nBGP routing table information for VRF default\nRouter identifier 192.168.100.1, local AS number 64521\nBGP routing table entry for imet 10100 192.168.100.4, Route Distinguisher: 192.168.100.4:4\n Paths: 2 available\n  65500 64524\n    192.168.100.4 from 192.168.101.1 (192.168.101.1)\n      Origin IGP, metric -, localpref 100, weight 0, valid, external, ECMP head, ECMP, best, ECMP contributor\n      Extended Community: Route-Target-AS:100:100 TunnelEncap:tunnelTypeVxlan\n      VNI: 10100\n      PMSI Tunnel: Ingress Replication, MPLS Label: 10100, Leaf Information Required: false, Tunnel ID: 192.168.100.4\n  65500 64524\n    192.168.100.4 from 192.168.101.2 (192.168.101.2)\n      Origin IGP, metric -, localpref 100, weight 0, valid, external, ECMP, ECMP contributor\n      Extended Community: Route-Target-AS:100:100 TunnelEncap:tunnelTypeVxlan\n      VNI: 10100\n      PMSI Tunnel: Ingress Replication, MPLS Label: 10100, Leaf Information Required: false, Tunnel ID: 192.168.100.4\n</code></pre> <p>If you look closely enough, you see that there is a value attached to the MAC address for the type-2 route and to the loopback (or vtep) address for the type-3 imet route.</p> <pre><code>* snip *\n\nBGP routing table entry for mac-ip 10100 aac1.abe8.a87d, Route Distinguisher: 192.168.100.4:4\n\n* snip *\n</code></pre> <p>See that '10100' in there? Well, that's the Ethernet Tag ID. Let's take a BGP packet capture and look at this in the BGP update itself to confirm. I'm going to clear my BGP peerings on leaf1 so the updates are sent again for us to capture it. </p> <p>This it the type-3 imet route update:</p> <p></p> <p>And this is the type-2 macip route update:</p> <p></p> <p>In both those cases, we see that the update is tagged with an Ethernet Tag ID of 10100, which corresponds to the VNI. </p> <p>Remember that an Ethernet Tag ID essentially identifies a broadcast domain in an EVPN instance. However, there are multiple ways that such an instance can be represented. The most common ways are:</p> <ol> <li>VLAN based service</li> <li>VLAN bundle service</li> <li>VLAN-aware bundle service</li> </ol> <p>By default, Juniper QFX (and vQFX) runs the VLAN-aware bundle service. This causes an Ethernet Tag ID to be set in the BGP EVPN updates. This is where the interoperability issues come in - my configuration on the Arista vEOS leaf is based on the VLAN based service type. EOS is built to drop updates with a non-zero Ethernet Tag ID when configured with VLAN based service type. This isn't mandated in the RFC, so each vendor has its own implementation. In the same setup, Cumulus VX accepts it (it can only do VLAN based service type) and the N9Kv has a knob which allows it to accept non-zero Ethernet Tags (it can also only do VLAN based service type).</p> <pre><code>// leaf2 - Cumulus VX\n\nroot@6ff6c9ba2374bf95:mgmt:~# net show bridge macs | grep a8:7d\n100       bridge  vni10100   aa:c1:ab:e8:a8:7d                            extern_learn        03:03:07\nuntagged          vni10100   aa:c1:ab:e8:a8:7d  192.168.100.4             self, extern_learn  03:03:07\n\n// leaf3 - N9Kv\n\nleaf3# show mac address-table\nLegend:\n        * - primary entry, G - Gateway MAC, (R) - Routed MAC, O - Overlay MAC\n        age - seconds since last seen,+ - primary entry using vPC Peer-Link,\n        (T) - True, (F) - False, C - ControlPlane MAC, ~ - vsan\n   VLAN     MAC Address      Type      age     Secure NTFY Ports\n---------+-----------------+--------+---------+------+----+------------------\n*  100     aac1.ab48.5bcf   dynamic  0         F      F    Eth1/3\nC  100     aac1.ab93.c413   dynamic  0         F      F    nve1(192.168.100.2)\nC  100     aac1.abde.327e   dynamic  0         F      F    nve1(192.168.100.1)\nC  100     aac1.abe8.a87d   dynamic  0         F      F    nve1(192.168.100.4)\nG    -     5237.8000.1b08   static   -         F      F    sup-eth1(R)\n</code></pre> <p>As you can see, both these devices have accepted the routes, despite a non-zero Ethernet Tag ID. </p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/03/12/multi-vendor-evpn-vxlan-setup-with-containerlab/#final-thoughts-and-looking-forward","title":"Final thoughts and looking forward","text":"<p>Juniper QFX code was eventually enhanced to interoperate with different vendors and their behaviors - with a new routing-instance type of 'mac-vrf', you can now build any type of service you want. </p> <p>If any of you actually deployed this in your labs, you'd see that h1 cannot ping h4:</p> <pre><code>root@h1:~# ping 10.100.100.4\nPING 10.100.100.4 (10.100.100.4) 56(84) bytes of data.\nFrom 10.100.100.1 icmp_seq=1 Destination Host Unreachable\nFrom 10.100.100.1 icmp_seq=2 Destination Host Unreachable\nFrom 10.100.100.1 icmp_seq=3 Destination Host Unreachable\n</code></pre> <p>Until... h4 pings h1, and then suddenly it starts working!</p> <pre><code>root@h4:~# ping 10.100.100.1\nPING 10.100.100.1 (10.100.100.1) 56(84) bytes of data.\n64 bytes from 10.100.100.1: icmp_seq=1 ttl=64 time=167 ms\n64 bytes from 10.100.100.1: icmp_seq=2 ttl=64 time=104 ms\n64 bytes from 10.100.100.1: icmp_seq=3 ttl=64 time=104 ms\n^C\n--- 10.100.100.1 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 5ms\nrtt min/avg/max/mdev = 103.765/124.966/166.897/29.652 ms\n</code></pre> <p>Interesting, interesting. Any guesses why? We'll look at this in detail in part2 of this EVPN multivendor deployment, so stay tuned!</p>","tags":["juniper","junos","containerlab","cumulus","arista","cisco","nxos","bgp","evpn","vxlan"]},{"location":"blog/2022/12/18/ciscos-nxos-evpn-hybrid-mode---interoperability-with-asymmetric-vteps/","title":"Cisco's NXOS EVPN Hybrid mode - interoperability with Asymmetric VTEPs","text":"<p>In this post, we pull back the curtain on this new NXOS EVPN Hybrid mode and understand how it enables interoperability with Asymmetric VTEPs.</p>","tags":["containerlab","junos","bgp","evpn","vxlan","cisco","nxos","multivendor"]},{"location":"blog/2022/12/18/ciscos-nxos-evpn-hybrid-mode---interoperability-with-asymmetric-vteps/#tldr","title":"tl;dr","text":"<p>Cisco gave this a fancy name - EVPN Hybrid mode. Or you know, maybe just call it Asymmetric IRB? Because that's what it is behind the scenes.</p>","tags":["containerlab","junos","bgp","evpn","vxlan","cisco","nxos","multivendor"]},{"location":"blog/2022/12/18/ciscos-nxos-evpn-hybrid-mode---interoperability-with-asymmetric-vteps/#topology","title":"Topology","text":"","tags":["containerlab","junos","bgp","evpn","vxlan","cisco","nxos","multivendor"]},{"location":"blog/2022/12/18/ciscos-nxos-evpn-hybrid-mode---interoperability-with-asymmetric-vteps/#understanding-why-asymmetric-irb-does-not-work-on-nxos","title":"Understanding why Asymmetric IRB does not work on NXOS","text":"<p>For this, let's consider communication between h1 and h4. We'll configure leaf1 and leaf3 for Asymmetric IRB - this implies that all VLANs, VNIs and IRB interfaces exist on both leafs. In our case, this means VLAN 100, VNI 10100, VLAN 200, VNI 10200 and its respective IRB interfaces are configured on both leaf1 and leaf3. </p>","tags":["containerlab","junos","bgp","evpn","vxlan","cisco","nxos","multivendor"]},{"location":"blog/2022/12/18/ciscos-nxos-evpn-hybrid-mode---interoperability-with-asymmetric-vteps/#configuration","title":"Configuration","text":"<p>For leaf1, which is running NXOS, the relevant configuration is as follows:</p> <pre><code>nv overlay evpn\nfeature bgp\nfeature fabric forwarding\nfeature interface-vlan\nfeature vn-segment-vlan-based\nfeature nv overlay\n!\nfabric forwarding anycast-gateway-mac 000a.000a.000a\nvlan 1,100,200,300\nvlan 100\n  vn-segment 10100\nvlan 200\n  vn-segment 10200\n!\nroute-map allow-loopback permit 10\n  match interface loopback0 \n!\ninterface Vlan100\n  no shutdown\n  ip address 10.100.100.254/24\n  fabric forwarding mode anycast-gateway\n!\ninterface Vlan200\n  no shutdown\n  ip address 10.100.200.254/24\n  fabric forwarding mode anycast-gateway\n!\ninterface nve1\n  no shutdown\n  host-reachability protocol bgp\n  advertise virtual-rmac\n  source-interface loopback0\n  member vni 10100\n    ingress-replication protocol bgp\n  member vni 10200\n    ingress-replication protocol bgp\n!\ninterface loopback0\n  ip address 192.0.2.1/32\n!\nrouter bgp 65421\n  router-id 192.0.2.1\n  log-neighbor-changes\n  address-family ipv4 unicast\n    redistribute direct route-map allow-loopback\n    maximum-paths 4\n  address-family l2vpn evpn\n    advertise-pip\n  template peer evpn\n    update-source loopback0\n    ebgp-multihop 2\n    address-family l2vpn evpn\n      send-community\n      send-community extended\n  neighbor 192.0.2.11\n    inherit peer evpn\n    remote-as 65500\n  neighbor 192.0.2.22\n    inherit peer evpn\n    remote-as 65500\n  neighbor 198.51.100.1\n    remote-as 65500\n    address-family ipv4 unicast\n  neighbor 198.51.100.9\n    remote-as 65500\n    address-family ipv4 unicast\nevpn\n  vni 10100 l2\n    rd 192.0.2.1:100\n    route-target import 100:100\n    route-target export 100:100\n  vni 10200 l2\n    rd 192.0.2.1:200\n    route-target import 200:200\n    route-target export 200:200\n</code></pre> <p>On leaf3, which is running Junos OS, we create routing-instances of type mac-vrf - one routing-instance per VLAN since we're enabling it for VLAN-based service type. </p> <pre><code>admin@leaf3# show \n\ninterfaces {\n    irb {\n        unit 100 {\n            virtual-gateway-accept-data;\n            family inet {\n                address 10.100.100.252/24 {\n                    virtual-gateway-address 10.100.100.254;\n                }\n            }\n            virtual-gateway-v4-mac 00:0a:00:0a:00:0a;\n        }\n        unit 200 {\n            virtual-gateway-accept-data;\n            family inet {\n                address 10.100.200.252/24 {\n                    virtual-gateway-address 10.100.200.254;\n                }\n            }\n            virtual-gateway-v4-mac 00:0a:00:0a:00:0a;\n        }\n    }\n    lo0 {\n        unit 0 {\n            family inet {\n                address 192.0.2.3/32;\n            }\n        }\n    }\n}\npolicy-options {\n    policy-statement ECMP {\n        then {\n            load-balance per-flow;\n        }\n    }\n    policy-statement allow-loopback {\n        from interface lo0.0;\n        then accept;\n    }\n}\nrouting-instances {\n    v100_mac_vrf {\n        instance-type mac-vrf;\n        protocols {\n            evpn {\n                encapsulation vxlan;    \n                default-gateway do-not-advertise;\n                extended-vni-list all;\n            }\n        }\n        vtep-source-interface lo0.0;\n        service-type vlan-based;\n        interface xe-0/0/2.0;\n        route-distinguisher 192.0.2.3:100;\n        vrf-target target:100:100;\n        vlans {\n            v100 {\n                vlan-id 100;\n                l3-interface irb.100;\n                vxlan {\n                    vni 10100;\n                }\n            }\n        }\n    }\n    v200_mac_vrf {\n        instance-type mac-vrf;\n        protocols {\n            evpn {\n                encapsulation vxlan;\n                default-gateway do-not-advertise;\n                extended-vni-list all;\n            }\n        }\n        vtep-source-interface lo0.0;\n        service-type vlan-based;\n        interface xe-0/0/3.0;\n        route-distinguisher 192.0.2.3:200;\n        vrf-target target:200:200;\n        vlans {\n            v200 {\n                vlan-id 200;\n                l3-interface irb.200;\n                vxlan {\n                    vni 10200;\n                }\n            }\n        }\n    }\n}\nrouting-options {\n    router-id 192.0.2.3;\n    autonomous-system 65423;            \n    forwarding-table {\n        export ECMP;\n    }\n}\nprotocols {\n    bgp {\n        group underlay {\n            type external;\n            family inet {\n                unicast;\n            }\n            export allow-loopback;\n            peer-as 65500;\n            multipath;\n            neighbor 198.51.100.5;\n            neighbor 198.51.100.13;\n        }\n        group overlay {\n            type external;\n            multihop;\n            local-address 192.0.2.3;\n            family evpn {\n                signaling;\n            }\n            peer-as 65500;\n            neighbor 192.0.2.11;\n            neighbor 192.0.2.22;\n        }\n    }\n}\n</code></pre> <p>Since each leaf is configured with all IRBs, every VLAN is essentially directly connected to each leaf. This implies that both leaf1 and leaf3 can directly ARP for any host on VLAN100 or VLAN200.</p>","tags":["containerlab","junos","bgp","evpn","vxlan","cisco","nxos","multivendor"]},{"location":"blog/2022/12/18/ciscos-nxos-evpn-hybrid-mode---interoperability-with-asymmetric-vteps/#packet-walk-to-visualize-the-problem","title":"Packet walk to visualize the problem","text":"<p>h1 has 10.100.100.254 as its default gateway. When pinging h4, it knows that the destination is in a different subnet and it needs to send the packet to its default gateway. ARP process provides the IP-MAC binding for 10.100.100.254 and h1 sends the ICMP request to leaf1.</p> <p>On leaf1, since the destination MAC address is owned by it, a route lookup is done for 10.100.200.4. This hits the directly connected subnet route:</p> <pre><code>leaf1# show ip route 10.100.200.4\nIP Route Table for VRF \"default\"\n'*' denotes best ucast next-hop\n'**' denotes best mcast next-hop\n'[x/y]' denotes [preference/metric]\n'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\n\n10.100.200.0/24, ubest/mbest: 1/0, attached\n    *via 10.100.200.254, Vlan200, [0/0], 00:29:41, direct\n</code></pre> <p>leaf1 can now ARP for the destination directly. Since this fabric is configured for Ingress Replication (default on Junos OS is Ingress Replication and we have explictly configured Ingress Replication on NXOS), leaf1 uses its flood list for VLAN 200 to package the broadcast ARP into a unicast VXLAN packet and send it to every VTEP (PE) in the flood list. This includes leaf3.</p> <p>When leaf3 receives this, it decapsulates the VXLAN packet and floods the ARP to all local ports in VLAN 200. h4 now receives this ARP packet, it builds its local ARP cache and sends an ARP reply back.</p> <p>Let's take a look at this ARP reply:</p> <p></p> <p>Seems like an ordinary ARP reply, but carefully observe the destination MAC address in the Ethernet header. This is the anycast gateway MAC address. When this ARP reply reaches leaf3, it will consume this packet since it owns the anycast MAC address and leaf1 will never see this ARP reply. Because of this, the ARP entry for 10.100.200.4 will always remain incomplete, and leaf1 has no knowledge of how to forward traffic to this destination.</p> <pre><code>leaf1# show ip arp\n\nFlags: * - Adjacencies learnt on non-active FHRP router\n       + - Adjacencies synced via CFSoE\n       # - Adjacencies Throttled for Glean\n       CP - Added via L2RIB, Control plane Adjacencies\n       PS - Added via L2RIB, Peer Sync\n       RO - Re-Originated Peer Sync Entry\n       D - Static Adjacencies attached to down interface\n\nIP ARP Table for context default\nTotal number of entries: 4\nAddress         Age       MAC Address     Interface       Flags\n198.51.100.1    00:09:48  5295.1e00.1b08  Ethernet1/1              \n198.51.100.9    00:09:29  520a.3600.1b08  Ethernet1/2              \n10.100.100.1    00:02:00  aac1.ab3e.c444  Vlan100                  \n10.100.200.4    00:00:03  INCOMPLETE      Vlan200 \n</code></pre> <p>For Asymmetric IRB to work, the VTEP must build its ARP table from EVPN Type-2 MAC+IP routes. Without this functionality, Asymmetric IRB will fail, as we can clearly see. And it appears that NXOS does not do this, which is why it does not support Asymmetric IRB.</p> <p>The following packet walk provides an easy to understand visual representation of the failure:</p> <p></p>","tags":["containerlab","junos","bgp","evpn","vxlan","cisco","nxos","multivendor"]},{"location":"blog/2022/12/18/ciscos-nxos-evpn-hybrid-mode---interoperability-with-asymmetric-vteps/#enabling-symmetric-irb-between-nxos-leafs","title":"Enabling Symmetric IRB between NXOS leafs","text":"<p>Before we look at how EVPN Hybrid mode works, let's first consider Symmetric IRB between leaf1 and leaf2 (both NXOS). The goal is to get h1 to talk to h2 (communication between VLAN 100 and VLAN 200). On the NXOS leafs, Symmetric IRB requires the following sample configuration (consider leaf1 as an example). First, a VLAN is created and the L3VNI is mapped it. This VNI is configured for the customer VRF, which we'll call 'Tenant1':</p> <pre><code>vlan 300\n  vn-segment 10300\n!\nvrf context Tenant1\n  vni 10300\n  rd auto\n  address-family ipv4 unicast\n    route-target import 65421:300\n    route-target import 65421:300 evpn\n    route-target export 65421:300\n    route-target export 65421:300 evpn\n</code></pre> <p>An anycast MAC address is configured on all the leafs (using 'fabric forwarding anycast-gateway-mac'), and the IRB interfaces are enabled with 'fabric forwarding mode anycast-gateway'. This enables the anycast gateway functionality on the IRB interface. Each leaf is also configured for its respective VLAN/BD under EVPN along with the import/export route-targets (these are the MAC VRF route-targets for the L2 domain).</p> <p>You also need to create an IRB interface for the L3VNI VLAN and it is configured to be in the customer VRF.</p> <pre><code>fabric forwarding anycast-gateway-mac 000a.000a.000a\n!\ninterface Vlan100\n  no shutdown\n  vrf member Tenant1\n  ip address 10.100.100.254/24\n  fabric forwarding mode anycast-gateway\n!\ninterface Vlan300\n  no shutdown\n  vrf member Tenant1\n  ip forward\n</code></pre> <p>The L3VNI is enabled under the NVE interface:</p> <pre><code>interface nve1\n  no shutdown\n  host-reachability protocol bgp\n  advertise virtual-rmac\n  source-interface loopback0\n  member vni 10100\n    ingress-replication protocol bgp\n  member vni 10300 associate-vrf\n</code></pre> <p>By default, NXOS does not advertise an EVPN Type-5 subnet route (we'll talk about the need for this route at a later point in this blog post). This is done using the 'redistribute direct ...' configuration under the VRF IPv4 unicast address family under BGP:</p> <pre><code>router bgp 65421\n  router-id 192.0.2.1\n  log-neighbor-changes\n  address-family ipv4 unicast\n    redistribute direct route-map allow-loopback\n    maximum-paths 4\n  address-family l2vpn evpn\n    advertise-pip\n  template peer evpn\n    update-source loopback0\n    ebgp-multihop 2\n    address-family l2vpn evpn\n      send-community\n      send-community extended\n  neighbor 192.0.2.11\n    inherit peer evpn\n    remote-as 65500\n  neighbor 192.0.2.22\n    inherit peer evpn\n    remote-as 65500\n  neighbor 198.51.100.1\n    remote-as 65500\n    address-family ipv4 unicast\n  neighbor 198.51.100.9\n    remote-as 65500\n    address-family ipv4 unicast\n  vrf Tenant1\n    address-family ipv4 unicast\n      redistribute direct route-map permit-all\n</code></pre> <p>Leaf1 learns h1s MAC address and IPv4 address using ARP/ND. We can initiate a ping from h1 (to its default gateway, 10.100.100.254, which is present on leaf1) to trigger this process.</p> <pre><code>root@h1:~# ping 10.100.100.254\nPING 10.100.100.254 (10.100.100.254) 56(84) bytes of data.\n64 bytes from 10.100.100.254: icmp_seq=1 ttl=255 time=4.43 ms\n64 bytes from 10.100.100.254: icmp_seq=2 ttl=255 time=1.95 ms\n64 bytes from 10.100.100.254: icmp_seq=3 ttl=255 time=2.16 ms\n64 bytes from 10.100.100.254: icmp_seq=4 ttl=255 time=1.86 ms\n^C\n--- 10.100.100.254 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 6ms\nrtt min/avg/max/mdev = 1.856/2.599/4.431/1.064 ms\n</code></pre> <p>Within the VRF, an ARP entry is created for h1 and this is pushed to BGP EVPN as well. This gets advertised as an EVPN Type-2 MAC+IP route to the spines, and from there to leaf2.</p> <pre><code>leaf1# show ip arp vrf Tenant1 \n\nFlags: * - Adjacencies learnt on non-active FHRP router\n       + - Adjacencies synced via CFSoE\n       # - Adjacencies Throttled for Glean\n       CP - Added via L2RIB, Control plane Adjacencies\n       PS - Added via L2RIB, Peer Sync\n       RO - Re-Originated Peer Sync Entry\n       D - Static Adjacencies attached to down interface\n\nIP ARP Table for context Tenant1\nTotal number of entries: 1\nAddress         Age       MAC Address     Interface       Flags\n10.100.100.1    00:01:33  aac1.ab3e.c444  Vlan100 \n\nleaf1# show bgp l2vpn evpn 10.100.100.1\nBGP routing table information for VRF default, address family L2VPN EVPN\nRoute Distinguisher: 192.0.2.1:100    (L2VNI 10100)\nBGP routing table entry for [2]:[0]:[0]:[48]:[aac1.ab3e.c444]:[32]:[10.100.100.1]/272, version 19\nPaths: (1 available, best #1)\nFlags: (0x000102) (high32 00000000) on xmit-list, is not in l2rib/evpn\n\n  Advertised path-id 1\n  Path type: local, path is valid, is best path, no labeled nexthop\n  AS-Path: NONE, path locally originated\n    192.0.2.1 (metric 0) from 0.0.0.0 (192.0.2.1)\n      Origin IGP, MED not set, localpref 100, weight 32768\n      Received label 10100 10300\n      Extcommunity: RT:100:100 RT:65421:300 ENCAP:8 Router MAC:52ca.b100.1b08\n\n  Path-id 1 advertised to peers:\n    192.0.2.11         192.0.2.22 \n</code></pre> <p>The extended communities added to this are important - you see both the L2 route-targets as well as the L3VNI route-target, along with the router MAC address. This router MAC address corresponds to IRB interface MAC address while the L3VNI route-target is used to identify (and import into) the customer VRF:</p> <pre><code>leaf1# show interface vlan100\nVlan100 is up, line protocol is up, autostate enabled\n  Hardware is EtherSVI, address is  52ca.b100.1b08\n  Internet Address is 10.100.100.254/24\n  MTU 1500 bytes, BW 1000000 Kbit, DLY 10 usec,\n   reliability 255/255, txload 1/255, rxload 1/255\n  Encapsulation ARPA, loopback not set\n  Keepalive not supported\n  ARP type: ARPA\n  Last clearing of \"show interface\" counters never\n  L3 in Switched:\n    ucast: 0 pkts, 0 bytes\n  L3 out Switched:\n    ucast: 0 pkts, 0 bytes\n</code></pre> <p>On leaf2, the route gets imported into the VRF table with a matching L3VNI route-target. </p> <pre><code>leaf2# show bgp l2vpn evpn 10.100.100.1\nBGP routing table information for VRF default, address family L2VPN EVPN\nRoute Distinguisher: 192.0.2.1:100\nBGP routing table entry for [2]:[0]:[0]:[48]:[aac1.ab3e.c444]:[32]:[10.100.100.1]/272, version 21\nPaths: (2 available, best #2)\nFlags: (0x000202) (high32 00000000) on xmit-list, is not in l2rib/evpn, is not in HW\n\n  Path type: external, path is valid, not best reason: newer EBGP path, no labeled nexthop\n  AS-Path: 65500 65421 , path sourced external to AS\n    192.0.2.1 (metric 0) from 192.0.2.22 (192.0.2.22)\n      Origin IGP, MED not set, localpref 100, weight 0\n      Received label 10100 10300\n      Extcommunity: RT:100:100 RT:65421:300 ENCAP:8 Router MAC:52ca.b100.1b08\n\n  Advertised path-id 1\n  Path type: external, path is valid, is best path, no labeled nexthop\n             Imported to 2 destination(s)\n             Imported paths list: Tenant1 L3-10300\n  AS-Path: 65500 65421 , path sourced external to AS\n    192.0.2.1 (metric 0) from 192.0.2.11 (192.0.2.11)\n      Origin IGP, MED not set, localpref 100, weight 0\n      Received label 10100 10300\n      Extcommunity: RT:100:100 RT:65421:300 ENCAP:8 Router MAC:52ca.b100.1b08\n\n  Path-id 1 not advertised to any peer\n\nRoute Distinguisher: 192.0.2.2:3    (L3VNI 10300)\nBGP routing table entry for [2]:[0]:[0]:[48]:[aac1.ab3e.c444]:[32]:[10.100.100.1]/272, version 20\nPaths: (1 available, best #1)\nFlags: (0x000202) (high32 00000000) on xmit-list, is not in l2rib/evpn, is not in HW\n\n  Advertised path-id 1\n  Path type: external, path is valid, is best path, no labeled nexthop\n             Imported from 192.0.2.1:100:[2]:[0]:[0]:[48]:[aac1.ab3e.c444]:[32]:[10.100.100.1]/272 \n  AS-Path: 65500 65421 , path sourced external to AS\n    192.0.2.1 (metric 0) from 192.0.2.11 (192.0.2.11)\n      Origin IGP, MED not set, localpref 100, weight 0\n      Received label 10100 10300\n      Extcommunity: RT:100:100 RT:65421:300 ENCAP:8 Router MAC:52ca.b100.1b08\n\n  Path-id 1 not advertised to any peer\n</code></pre> <p>This creates a host route (a /32 route) entry into VRF table:</p> <pre><code>leaf2# show ip route 10.100.100.1 vrf Tenant1 \nIP Route Table for VRF \"Tenant1\"\n'*' denotes best ucast next-hop\n'**' denotes best mcast next-hop\n'[x/y]' denotes [preference/metric]\n'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\n\n10.100.100.1/32, ubest/mbest: 1/0\n    *via 192.0.2.1%default, [20/0], 3d04h, bgp-65422, external, tag 65500, segid: 10300 tunnelid: 0xc0000201 encap: VXLAN\n</code></pre> <p>In addition to the host route, an EVPN Type-5 route (for the IRB subnet) is also advertised in accordance to the RFC (RFC 9135) for silent hosts (to trigger the gleaning process for such hosts). This subnet route is pulled into the VRF as well:</p> <pre><code>leaf2# show bgp l2vpn evpn 10.100.100.0\nBGP routing table information for VRF default, address family L2VPN EVPN\nRoute Distinguisher: 192.0.2.1:3\nBGP routing table entry for [5]:[0]:[0]:[24]:[10.100.100.0]/224, version 14\nPaths: (2 available, best #2)\nFlags: (0x000002) (high32 00000000) on xmit-list, is not in l2rib/evpn, is not in HW\n\n  Path type: external, path is valid, not best reason: newer EBGP path, no labeled nexthop\n  Gateway IP: 0.0.0.0\n  AS-Path: 65500 65421 , path sourced external to AS\n    192.0.2.1 (metric 0) from 192.0.2.22 (192.0.2.22)\n      Origin incomplete, MED not set, localpref 100, weight 0\n      Received label 10300\n      Extcommunity: RT:65421:300 ENCAP:8 Router MAC:52ca.b100.1b08\n\n  Advertised path-id 1\n  Path type: external, path is valid, is best path, no labeled nexthop\n             Imported to 2 destination(s)\n             Imported paths list: Tenant1 L3-10300\n  Gateway IP: 0.0.0.0\n  AS-Path: 65500 65421 , path sourced external to AS\n    192.0.2.1 (metric 0) from 192.0.2.11 (192.0.2.11)\n      Origin incomplete, MED not set, localpref 100, weight 0\n      Received label 10300\n      Extcommunity: RT:65421:300 ENCAP:8 Router MAC:52ca.b100.1b08\n\n  Path-id 1 not advertised to any peer\n\nRoute Distinguisher: 192.0.2.2:3    (L3VNI 10300)\nBGP routing table entry for [5]:[0]:[0]:[24]:[10.100.100.0]/224, version 10\nPaths: (1 available, best #1)\nFlags: (0x000002) (high32 00000000) on xmit-list, is not in l2rib/evpn, is not in HW\n\n  Advertised path-id 1\n  Path type: external, path is valid, is best path, no labeled nexthop\n             Imported from 192.0.2.1:3:[5]:[0]:[0]:[24]:[10.100.100.0]/224 \n  Gateway IP: 0.0.0.0\n  AS-Path: 65500 65421 , path sourced external to AS\n    192.0.2.1 (metric 0) from 192.0.2.11 (192.0.2.11)\n      Origin incomplete, MED not set, localpref 100, weight 0\n      Received label 10300\n      Extcommunity: RT:65421:300 ENCAP:8 Router MAC:52ca.b100.1b08\n\n  Path-id 1 not advertised to any peer\n\nleaf2# show ip route 10.100.100.0/24 vrf Tenant1 \nIP Route Table for VRF \"Tenant1\"\n'*' denotes best ucast next-hop\n'**' denotes best mcast next-hop\n'[x/y]' denotes [preference/metric]\n'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\n\n10.100.100.0/24, ubest/mbest: 1/0\n    *via 192.0.2.1%default, [20/0], 3d04h, bgp-65422, external, tag 65500, segid: 10300 tunnelid: 0xc0000201 encap: VXLAN\n</code></pre> <p>A similar process happens in reverse for h2. On leaf1, h2s exact route and a subnet route should exist in the VRF table:</p> <pre><code>leaf1# show ip route vrf Tenant1 \nIP Route Table for VRF \"Tenant1\"\n'*' denotes best ucast next-hop\n'**' denotes best mcast next-hop\n'[x/y]' denotes [preference/metric]\n'%&lt;string&gt;' in via output denotes VRF &lt;string&gt;\n\n10.100.100.0/24, ubest/mbest: 1/0, attached\n    *via 10.100.100.254, Vlan100, [0/0], 3d04h, direct\n10.100.100.1/32, ubest/mbest: 1/0, attached\n    *via 10.100.100.1, Vlan100, [190/0], 00:31:24, hmm\n10.100.100.254/32, ubest/mbest: 1/0, attached\n    *via 10.100.100.254, Vlan100, [0/0], 3d04h, local\n10.100.200.0/24, ubest/mbest: 1/0\n    *via 192.0.2.2%default, [20/0], 3d04h, bgp-65421, external, tag 65500, segid: 10300 tunnelid: 0xc0000202 encap: VXLAN\n\n10.100.200.2/32, ubest/mbest: 1/0\n    *via 192.0.2.2%default, [20/0], 3d04h, bgp-65421, external, tag 65500, segid: 10300 tunnelid: 0xc0000202 encap: VXLAN\n</code></pre> <p>h1 should now be able to ping h2 and a data-plane captures shows that the VNI in the VXLAN header is the L3VNI:</p> <pre><code>root@h1:~# ping 10.100.200.2\nPING 10.100.200.2 (10.100.200.2) 56(84) bytes of data.\n64 bytes from 10.100.200.2: icmp_seq=1 ttl=62 time=19.5 ms\n64 bytes from 10.100.200.2: icmp_seq=2 ttl=62 time=29.8 ms\n64 bytes from 10.100.200.2: icmp_seq=3 ttl=62 time=12.0 ms\n64 bytes from 10.100.200.2: icmp_seq=4 ttl=62 time=12.0 ms\n64 bytes from 10.100.200.2: icmp_seq=5 ttl=62 time=22.6 ms\n^C\n--- 10.100.200.2 ping statistics ---\n5 packets transmitted, 5 received, 0% packet loss, time 9ms\nrtt min/avg/max/mdev = 11.968/19.167/29.789/6.750 ms\n</code></pre> <p></p>","tags":["containerlab","junos","bgp","evpn","vxlan","cisco","nxos","multivendor"]},{"location":"blog/2022/12/18/ciscos-nxos-evpn-hybrid-mode---interoperability-with-asymmetric-vteps/#configuring-leaf3-for-asymmetric-irb","title":"Configuring leaf3 for Asymmetric IRB","text":"<p>Let's configure leaf3 now for Asymmetric IRB. This implies that the IRB interface for both VLAN100 and VLAN200 exist on leaf3, along with their corresponding L2VNIs. Routing-instances are created for both VLAN100 and VLAN200 with service-type VLAN-based.</p> <pre><code>admin@leaf3# show\ninterfaces {\n\n*snip*\n\n    irb {\n        unit 100 {\n            virtual-gateway-accept-data;\n            family inet {\n                address 10.100.100.252/24 {\n                    virtual-gateway-address 10.100.100.254;\n                }\n            }\n            virtual-gateway-v4-mac 00:0a:00:0a:00:0a;\n        }\n        unit 200 {\n            virtual-gateway-accept-data;\n            family inet {               \n                address 10.100.200.252/24 {\n                    virtual-gateway-address 10.100.200.254;\n                }\n            }\n            virtual-gateway-v4-mac 00:0a:00:0a:00:0a;\n        }\n    }\n    lo0 {\n        unit 0 {\n            family inet {\n                address 192.0.2.3/32;\n            }\n        }\n    }\n}\npolicy-options {\n    policy-statement ECMP {\n        then {\n            load-balance per-flow;\n        }\n    }\n    policy-statement allow-loopback {\n        from interface lo0.0;\n        then accept;\n    }\n}\nrouting-instances {\n    v100_mac_vrf {\n        instance-type mac-vrf;\n        protocols {\n            evpn {\n                encapsulation vxlan;\n                default-gateway do-not-advertise;\n                extended-vni-list all;\n            }\n        }\n        vtep-source-interface lo0.0;\n        service-type vlan-based;\n        interface xe-0/0/2.0;\n        route-distinguisher 192.0.2.3:100;\n        vrf-target target:100:100;\n        vlans {                         \n            v100 {\n                vlan-id 100;\n                l3-interface irb.100;\n                vxlan {\n                    vni 10100;\n                }\n            }\n        }\n    }\n    v200_mac_vrf {\n        instance-type mac-vrf;\n        protocols {\n            evpn {\n                encapsulation vxlan;\n                default-gateway do-not-advertise;\n                extended-vni-list all;\n            }\n        }\n        vtep-source-interface lo0.0;\n        service-type vlan-based;\n        route-distinguisher 192.0.2.3:200;\n        vrf-target target:200:200;\n        vlans {\n            v200 {\n                vlan-id 200;\n                l3-interface irb.200;\n                vxlan {\n                    vni 10200;\n                }\n            }\n        }\n    }\n}\nrouting-options {\n    router-id 192.0.2.3;\n    autonomous-system 65423;\n    forwarding-table {\n        export ECMP;\n    }\n}\nprotocols {\n    bgp {\n        group underlay {\n            type external;\n            family inet {\n                unicast;\n            }                           \n            export allow-loopback;\n            peer-as 65500;\n            multipath;\n            neighbor 198.51.100.5;\n            neighbor 198.51.100.13;\n        }\n        group overlay {\n            type external;\n            multihop;\n            local-address 192.0.2.3;\n            family evpn {\n                signaling;\n            }\n            peer-as 65500;\n            neighbor 192.0.2.11;\n            neighbor 192.0.2.22;\n        }\n    }\n}\n</code></pre> <p>The IRB interfaces come up as long as there is at least one remote VXLAN tunnel that is created (these tunnels can be created by simply exchange EVPN Type-3 IMET routes):</p> <pre><code>admin@leaf3&gt; show ethernet-switching vxlan-tunnel-end-point remote \nLogical System Name       Id  SVTEP-IP         IFL   L3-Idx    SVTEP-Mode    ELP-SVTEP-IP\n&lt;default&gt;                 0   192.0.2.3        lo0.0    0  \n RVTEP-IP         L2-RTT                   IFL-Idx   Interface    NH-Id   RVTEP-Mode  ELP-IP        Flags\n 192.0.2.1        v100_mac_vrf             556       vtep.32770   1788    RNVE      \n    VNID          MC-Group-IP      \n    10100         0.0.0.0         \n RVTEP-IP         L2-RTT                   IFL-Idx   Interface    NH-Id   RVTEP-Mode  ELP-IP        Flags\n 192.0.2.2        v200_mac_vrf             576       vtep.32771   1789    RNVE      \n    VNID          MC-Group-IP      \n    10200         0.0.0.0 \n\nadmin@leaf3&gt; show interfaces irb \nPhysical interface: irb, Enabled, Physical link is Up\n  Interface index: 640, SNMP ifIndex: 506\n  Type: Ethernet, Link-level type: Ethernet, MTU: 1514\n  Device flags   : Present Running\n  Interface flags: SNMP-Traps\n  Link type      : Full-Duplex\n  Link flags     : None\n  Current address: 02:05:86:71:49:00, Hardware address: 02:05:86:71:49:00\n  Last flapped   : Never\n    Input packets : 0\n    Output packets: 0\n\n  Logical interface irb.100 (Index 567) (SNMP ifIndex 540)\n    Flags: Up SNMP-Traps 0x4004000 Encapsulation: ENET2\n    Virtual Gateway V4 MAC: 00:0a:00:0a:00:0a\n    Bandwidth: 1Gbps\n    Routing Instance: v100_mac_vrf Bridging Domain: v100\n    Input packets : 0\n    Output packets: 8\n    Protocol inet, MTU: 1500\n    Max nh cache: 75000, New hold nh limit: 75000, Curr nh cnt: 1, Curr new hold cnt: 0, NH drop cnt: 0\n      Flags: Sendbcast-pkt-to-re\n      Addresses, Flags: Is-Preferred Is-Primary\n        Destination: 10.100.100/24, Local: 10.100.100.252, Broadcast: 10.100.100.255\n        Destination: 10.100.100/24, Local: 10.100.100.254, Broadcast: 10.100.100.255\n\n  Logical interface irb.200 (Index 568) (SNMP ifIndex 541)\n    Flags: Up SNMP-Traps 0x4000 Encapsulation: ENET2\n    Virtual Gateway V4 MAC: 00:0a:00:0a:00:0a\n    Bandwidth: 1Gbps\n    Routing Instance: v200_mac_vrf Bridging Domain: v200\n    Input packets : 0\n    Output packets: 2\n    Protocol inet, MTU: 1514\n    Max nh cache: 75000, New hold nh limit: 75000, Curr nh cnt: 1, Curr new hold cnt: 0, NH drop cnt: 0\n      Flags: Sendbcast-pkt-to-re\n      Addresses, Flags: Is-Preferred Is-Primary\n        Destination: 10.100.200/24, Local: 10.100.200.252, Broadcast: 10.100.200.255\n        Destination: 10.100.200/24, Local: 10.100.200.254, Broadcast: 10.100.200.255\n</code></pre>","tags":["containerlab","junos","bgp","evpn","vxlan","cisco","nxos","multivendor"]},{"location":"blog/2022/12/18/ciscos-nxos-evpn-hybrid-mode---interoperability-with-asymmetric-vteps/#why-symmetric-irb-on-one-leaf-and-asymmetric-irb-on-another-leaf-does-not-work","title":"Why Symmetric IRB on one leaf and Asymmetric IRB on another leaf does not work","text":"<p>Our primary test is to check connectivity between h2 and h3 (since they are in different subnets - h2 is in VLAN200 and h3 is in VLAN100).</p> <p>h3 can ping its default gateway (10.100.100.254, which exists on leaf3 since it is an anycast gateway):</p> <pre><code>root@h3:~# ping 10.100.100.254\nPING 10.100.100.254 (10.100.100.254) 56(84) bytes of data.\n64 bytes from 10.100.100.254: icmp_seq=1 ttl=64 time=119 ms\n64 bytes from 10.100.100.254: icmp_seq=2 ttl=64 time=114 ms\n64 bytes from 10.100.100.254: icmp_seq=3 ttl=64 time=128 ms\n64 bytes from 10.100.100.254: icmp_seq=4 ttl=64 time=149 ms\n^C\n--- 10.100.100.254 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 6ms\nrtt min/avg/max/mdev = 113.884/127.348/149.132/13.538 ms\n</code></pre> <p>Naturally, h3 cannot ping h2:</p> <pre><code>root@h3:~# ping 10.100.200.2\nPING 10.100.200.2 (10.100.200.2) 56(84) bytes of data.\n^C\n--- 10.100.200.2 ping statistics ---\n5 packets transmitted, 0 received, 100% packet loss, time 90ms\n</code></pre> <p>This fails because while leaf3 has the required information for h2 (it has an IRB interface for VLAN200 and is thus able to ARP for h2 directly), leaf2 does not have any information about h3 in its forwarding table. It is receiving the EVPN Type-2 MAC+IP route but it does not know what to do with it since there is no corresponding L2VNI (more importantly, no matching import route-target). It simply drops the prefix, as seen below:</p> <pre><code>*snip*\n\nleaf2# show bgp event-history prefixes \n2022-12-17T13:49:54.428360000+00:00 [M 27] [bgp] E_DEBUG [bgp_af_process_nlri:7447] (default) PFX: [L2VPN EVPN] Dropping prefix [2]:[0]:[0]:[48]:[aac1.abc1.c674]:[32]:[10.100.100.3]/144 from peer 192.0.2.22, due to attribu\nte policy rejected\n2022-12-17T13:49:54.428358000+00:00 [M 27] [bgp] E_DEBUG [bgp_af_process_nlri:7447] (default) PFX: [L2VPN EVPN] Dropping prefix [2]:[0]:[0]:[48]:[aac1.abc1.c674]:[0]:[0.0.0.0]/112 from peer 192.0.2.22, due to attribute pol\nicy rejected\n\n*snip*\n</code></pre>","tags":["containerlab","junos","bgp","evpn","vxlan","cisco","nxos","multivendor"]},{"location":"blog/2022/12/18/ciscos-nxos-evpn-hybrid-mode---interoperability-with-asymmetric-vteps/#enabling-evpn-hybrid-mode-on-nxos","title":"Enabling EVPN Hybrid mode on NXOS","text":"<p>This is where EVPN Hybrid mode comes in. Cisco is purposefully very vague about how this really works, but we're going to break it down. From their documentation (https://www.cisco.com/c/en/us/td/docs/dcn/nx-os/nexus9000/102x/configuration/vxlan/cisco-nexus-9000-series-nx-os-vxlan-configuration-guide-release-102x/m-evpn-hybrid-irb-mode.html), the most important pieces of information are listed below:</p> <p>Statement from Cisco documentation</p> <p>NX-OS symmetric IRB VTEPs must be provisioned with all subnets in an IP VRF that are stretched to asymmetric VTEPs in the fabric. NX-OS symmetric IRB VTEPs must be provisioned with subnets in an IP VRF that are stretched to asymmetric VTEPs in \u201chybrid\u201d mode using \u201cfabric forwarding mode anycast-gateway hybrid\u201d CLI under the subnet SVI interface. All symmetric IRB VTEPs must have the hybrid mode enabled when interoperating with asymmetric VTEPs in each fabric.</p> <p>So, you're really configuring the device for Asymmetric IRB, but with the continued existence of the IP VRF and the corresponding L3VNI. This is the additional configuration that we're going to add on leaf2 now:</p> <pre><code>vlan 100\n  vn-segment 10100\n!\ninterface Vlan100\n  no shutdown\n  vrf member Tenant1\n  ip address 10.100.100.254/24\n  fabric forwarding mode anycast-gateway hybrid\n!\ninterface nve1\n  no shutdown\n  host-reachability protocol bgp\n  advertise virtual-rmac\n  source-interface loopback0\n  member vni 10100\n    ingress-replication protocol bgp\n  member vni 10200\n    ingress-replication protocol bgp\n  member vni 10300 associate-vrf\n!\nevpn\n  vni 10100 l2\n    rd 192.0.2.2:100\n    route-target import 100:100\n    route-target export 100:100\n  vni 10200 l2\n    rd 192.0.2.2:200\n    route-target import 200:200\n    route-target export 200:200\n</code></pre>","tags":["containerlab","junos","bgp","evpn","vxlan","cisco","nxos","multivendor"]},{"location":"blog/2022/12/18/ciscos-nxos-evpn-hybrid-mode---interoperability-with-asymmetric-vteps/#pulling-back-the-curtain","title":"Pulling back the curtain","text":"<p>Because of the existence of the L2VNI, the EVPN Type-2 MAC+IP route is now accepted on leaf2:</p> <pre><code>leaf2# show bgp l2vpn evpn 10.100.100.3\nBGP routing table information for VRF default, address family L2VPN EVPN\nRoute Distinguisher: 192.0.2.2:100    (L2VNI 10100)\nBGP routing table entry for [2]:[0]:[0]:[48]:[aac1.abc1.c674]:[32]:[10.100.100.3]/248, version 222\nPaths: (1 available, best #1)\nFlags: (0x000212) (high32 00000000) on xmit-list, is in l2rib/evpn, is not in HW\n\n  Advertised path-id 1\n  Path type: external, path is valid, is best path, no labeled nexthop, in rib\n             Imported from 192.0.2.3:100:[2]:[0]:[0]:[48]:[aac1.abc1.c674]:[32]:[10.100.100.3]/248 \n  AS-Path: 65500 65423 , path sourced external to AS\n    192.0.2.3 (metric 0) from 192.0.2.22 (192.0.2.22)\n      Origin IGP, MED not set, localpref 100, weight 0\n      Received label 10100\n      Extcommunity: RT:100:100 ENCAP:8\n\n  Path-id 1 not advertised to any peer\n\nRoute Distinguisher: 192.0.2.3:100\nBGP routing table entry for [2]:[0]:[0]:[48]:[aac1.abc1.c674]:[32]:[10.100.100.3]/248, version 215\nPaths: (2 available, best #2)\nFlags: (0x000202) (high32 00000000) on xmit-list, is not in l2rib/evpn, is not in HW\n\n  Path type: external, path is valid, not best reason: newer EBGP path, no labeled nexthop\n  AS-Path: 65500 65423 , path sourced external to AS\n    192.0.2.3 (metric 0) from 192.0.2.11 (192.0.2.11)\n      Origin IGP, MED not set, localpref 100, weight 0\n      Received label 10100\n      Extcommunity: RT:100:100 ENCAP:8\n\n  Advertised path-id 1\n  Path type: external, path is valid, is best path, no labeled nexthop\n             Imported to 1 destination(s)\n             Imported paths list: L2-10100\n  AS-Path: 65500 65423 , path sourced external to AS\n    192.0.2.3 (metric 0) from 192.0.2.22 (192.0.2.22)\n      Origin IGP, MED not set, localpref 100, weight 0\n      Received label 10100\n      Extcommunity: RT:100:100 ENCAP:8\n\n  Path-id 1 not advertised to any peer\n</code></pre> <p>This is installed into the MAC address table:</p> <pre><code>leaf2# show mac address-table address aac1.abc1.c674\nLegend: \n        * - primary entry, G - Gateway MAC, (R) - Routed MAC, O - Overlay MAC\n        age - seconds since last seen,+ - primary entry using vPC Peer-Link,\n        (T) - True, (F) - False, C - ControlPlane MAC, ~ - vsan,\n        (NA)- Not Applicable\n   VLAN     MAC Address      Type      age     Secure NTFY Ports\n---------+-----------------+--------+---------+------+----+------------------\nC  100     aac1.abc1.c674   dynamic  NA         F      F    nve1(192.0.2.3)\n</code></pre> <p>So basically, with all of this new configuration, we have configured leaf2 for Asymmetric IRB (only for destinations behind leafs doing Asymmetric IRB). However, remember why Asymmetric IRB did not work in the first place on NXOS? The ARP process was functionally broken. Is it fixed now? Let's take a look:</p> <pre><code>leaf2# show ip arp vrf Tenant1 \n\nFlags: * - Adjacencies learnt on non-active FHRP router\n       + - Adjacencies synced via CFSoE\n       # - Adjacencies Throttled for Glean\n       CP - Added via L2RIB, Control plane Adjacencies\n       PS - Added via L2RIB, Peer Sync\n       RO - Re-Originated Peer Sync Entry\n       D - Static Adjacencies attached to down interface\n\nIP ARP Table for context Tenant1\nTotal number of entries: 1\nAddress         Age       MAC Address     Interface       Flags\n10.100.200.2    00:00:08  aac1.ab83.9993  Vlan200     \n</code></pre> <p>Well, that's odd. No ARP for 10.100.100.3. Clearly h3 can ping h2 now, so how is it working?</p> <pre><code>root@h3:~# ping 10.100.200.2\nPING 10.100.200.2 (10.100.200.2) 56(84) bytes of data.\n64 bytes from 10.100.200.2: icmp_seq=1 ttl=63 time=129 ms\n64 bytes from 10.100.200.2: icmp_seq=2 ttl=63 time=276 ms\n64 bytes from 10.100.200.2: icmp_seq=3 ttl=63 time=148 ms\n64 bytes from 10.100.200.2: icmp_seq=4 ttl=63 time=127 ms\n^C\n--- 10.100.200.2 ping statistics ---\n5 packets transmitted, 4 received, 20% packet loss, time 11ms\nrtt min/avg/max/mdev = 127.399/169.990/275.513/61.443 ms\n</code></pre> <p>The secret lies behind the 'fabric forwarding mode anycast-gateway hybrid' command - looking at the l2route table, you can see that the 'Asymmetric' flag is set (Asy), and the IP-MAC binding is sent directly to Adjacency Manager (AM) instead of installing it in the ARP table:</p> <pre><code>leaf2# show l2route evpn mac-ip all detail \nFlags -(Rmac):Router MAC (Stt):Static (L):Local (R):Remote (V):vPC link \n(Dup):Duplicate (Spl):Split (Rcv):Recv(D):Del Pending (S):Stale (C):Clear\n(Ps):Peer Sync (Ro):Re-Originated (Orp):Orphan (Asy):Asymmetric (Gw):Gateway\n(Piporp): Directly connected Orphan to PIP based vPC BGW \n(Pipporp): Orphan connected to peer of PIP based vPC BGW \nTopology    Mac Address    Host IP                                 Prod   Flags         Seq No     Next-Hops                              \n----------- -------------- --------------------------------------- ------ ---------- ---------- ---------------------------------------\n100         aac1.ab3e.c444 10.100.100.1                            BGP    --            0         192.0.2.1 (Label: 10100)               \n            Sent To: AM\n            encap-type:1\n100         aac1.abc1.c674 10.100.100.3                            BGP    Asy           0         192.0.2.3 (Label: 10100)               \n            Sent To: AM\n            Peer ID: 2\n            encap-type:1\n100         000a.000a.000a 10.100.100.254                          BGP    Stt,Asy       0         192.0.2.3 (Label: 10100)               \n            Sent To: AM\n            ESI : 0500.00ff.8f00.0027.7400  \n            encap-type:1\n200         aac1.ab83.9993 10.100.200.2                            HMM    L,            0         Local                                  \n            L3-Info: 10300\n            Sent To: BGP\n200         000a.000a.000a 10.100.200.254                          BGP    Stt,Asy       0         192.0.2.3 (Label: 10200)               \n            ESI : 0500.00ff.8f00.0027.d800  \n            encap-type:1\n</code></pre> <p>We can confirm this from the logs below. BGP EVPN sends this to l2rib, which in turn sends it to AM.</p> <pre><code>leaf2# show system internal l2rib event-history mac-ip \n2022-12-17T15:08:59.587921000+00:00 [M 27] [l2rib] E_DEBUG [l2rib_svr_mac_ip_ent_gpb_encode:587] (100,aac1.abc1.c674,10.100.100.3,5): Encoding MAC-IP best route (ADD, client id 0), esi: (F)\n2022-12-17T15:08:59.583040000+00:00 [M 27] [l2rib] E_DEBUG [l2rib_obj_mac_ip_route_create:1434] (100,aac1.abc1.c674,10.100.100.3,5):  ESI: (F), port-channel ifIndex: 0\n2022-12-17T15:08:59.582824000+00:00 [M 27] [l2rib] E_DEBUG [l2rib_obj_mac_ip_route_create:1422] (100,aac1.abc1.c674,10.100.100.3,5):  adminDist: 20, SOO: 0, peerID: 2, peer ifIndex: 1191182338\n2022-12-17T15:08:59.582823000+00:00 [M 27] [l2rib] E_DEBUG [l2rib_obj_mac_ip_route_create:1410] (100,aac1.abc1.c674,10.100.100.3,5): MAC-IP route created with flags: 32, L3 VNI: 0, seqNum: 0\n2022-12-17T15:08:59.582812000+00:00 [M 27] [l2rib] E_DEBUG [l2rib_obj_mac_ip_create:208] (100,aac1.abc1.c674,10.100.100.3): MAC-IP entry created\n2022-12-17T15:08:59.582452000+00:00 [M 27] [l2rib] E_DEBUG [l2rib_client_show_mac_ip_route_msg:1462] NH: 192.0.2.3 (Label: 10100)\n2022-12-17T15:08:59.582450000+00:00 [M 27] [l2rib] E_DEBUG [8381]: Rcvd MAC-IP ROUTE msg: res 0, esi (F), es_type 0, tag 0, ifindex 0, nh_count 1, pc-ifindex 0\n2022-12-17T15:08:59.582447000+00:00 [M 27] [l2rib] E_DEBUG [l2rib_client_show_mac_ip_route_msg:1444] Rcvd MAC-IP ROUTE msg: flags Asy, admin_dist 0, seq 0, soo 0, peerid 0, \n2022-12-17T15:08:59.582445000+00:00 [M 27] [l2rib] E_DEBUG [l2rib_client_show_mac_ip_route_msg:1436] Rcvd MAC-IP ROUTE msg: (100, aac1.abc1.c674, 10.100.100.3), vrf_id 0, encap_type 1, \n2022-12-17T15:08:59.582443000+00:00 [M 27] [l2rib] E_DEBUG [l2rib_client_show_mac_ip_route_msg:1429] Rcvd MAC-IP ROUTE msg: (100, aac1.abc1.c674, 10.100.100.3), l2 vni 0, l3 vni 0\n\n*snip*\n\nleaf2# show system internal adjmgr internal event-history events\n2022-12-17T15:08:59.683335000+00:00 [M 27] [adjmgr] E_DEBUG AM upd do work: Event=sentRouteToURIB, AFI=IPv4, routeCount=1\n2022-12-17T15:08:59.683219000+00:00 [M 27] [adjmgr] E_DEBUG Append IPv4 adj route add to UPD: Result=Success, IP=10.100.100.3, IOD=77, Interface=Vlan100, prot0PhyIod=76, prot0PhyInterface=nve-peer2, prot1PhyIod=0, prot1Phy\nInterface=None, tableId=0x3, adminDistance=250, mobility=0, nhCount=0\n2022-12-17T15:08:59.683212000+00:00 [M 27] [adjmgr] E_DEBUG Append adj prot info to UPD: Result=Success, protAdjIndex=0, phyifIndex=0x47000002, phyIfType=71, Encap=1, tunnelId=0xffffffff\n2022-12-17T15:08:59.614494000+00:00 [M 27] [adjmgr] E_DEBUG Processed L2RIB Msg. dbgStr=##48,5c,1b,1,1b,10,33,35,36,15,7a,c,67,12,68,9,44,b\n2022-12-17T15:08:59.595194000+00:00 [M 27] [adjmgr] E_DEBUG Add to UPD work: Result=Success, IP=10.100.100.3, IOD=77, Interface=Vlan100, AFI=IPv4, workBits=0x1\n2022-12-17T15:08:59.589259000+00:00 [M 27] [adjmgr] E_DEBUG Received MAC-IP update with AFI: 2 IP: 10.100.100.3 VRF: 0x0 l2r_ifIdx: 1191182338 peerId: 0x2 seqNum: 0 flags: 0x7 MAC : aac1.abc1.c674\n2022-12-17T15:08:59.589230000+00:00 [M 27] [adjmgr] E_DEBUG Processed L2RIB Msg. dbgStr=##48,5d\n\n*snip*\n</code></pre> <p>Our final confirmation is that the entry exists in the adjacency table:</p> <pre><code>leaf2# show ip adjacency vrf Tenant1 \n\nFlags: # - Adjacencies Throttled for Glean\n       G - Adjacencies of vPC peer with G/W bit\n       R - Adjacencies learnt remotely\n       CP - Added via L2RIB, Control plane Adjacencies\n       PS - Added via L2RIB, Peer Sync\n       RO - Re-Originated Peer Sync Entry\n       CC - Consistency check pending\n\nIP Adjacency Table for VRF Tenant1\nTotal number of entries: 2\nAddress         MAC Address     Pref Source     Interface         Mobility Flags\n10.100.200.2    aac1.ab83.9993  50   arp        Vlan200                          \n10.100.100.3    aac1.abc1.c674  50   am_l2rib   Vlan100                  0 CP R\n</code></pre> <p>As you can see, the source for 10.100.100.3 is 'am_l2rib' and the 'CP' flag is set, which implies it was learnt via L2RIB. The question that then arises from this is that is Cisco in violation of the RFC? Must we see this entry in the ARP table? No, they are not violating the RFC and it is not necessary to see the entry in the ARP table.</p> <p>Statement from RFC9135</p> <p>For IP-to-MAC bindings learned via EVPN, an implementation may choose to import these bindings directly to the respective forwarding table (such as an adjacency/next-hop table) as opposed to importing them to ARP or ND protocol tables</p> <p>It's a little sneaky, and easy to miss if you don't know where to look. And perhaps the reason it is implemented this way is to ensure that Asymmetric IRB continues to be natively unsupported on NXOS, but it will work (for interoperability reasons) if you have Symmetric IRB with L3VNIs, and the IRBs are configured in this EVPN Hybrid mode.</p>","tags":["containerlab","junos","bgp","evpn","vxlan","cisco","nxos","multivendor"]},{"location":"blog/2021/12/25/cisco-sda-part-i---introduction-to-lisp-and-its-basic-terminology/","title":"Cisco SDA Part I - Introduction to LISP and its basic terminology","text":"<p>This is a new series that will cover Cisco's Software Defined Access architecture/solution over time. There are several moving pieces to this - in this post, we're going to start with a key component, which is LISP.</p>","tags":["sda","lisp"]},{"location":"blog/2021/12/25/cisco-sda-part-i---introduction-to-lisp-and-its-basic-terminology/#traditional-architecture-vs-lisp-architecture","title":"Traditional architecture vs LISP architecture","text":"<p>This is a new series that will cover Cisco's Software Defined Access architecture/solution over time. There are several moving pieces to this - we're going to start with a key component, which is LISP. </p> <p>LISP stands for Locator/ID Separation protocol. Let's quickly revisit how endpoints are/were identified - with a simple IP address (IPv4/IPv6, what have you). The IP address was both the location and the identity of the endpoint. LISP (which serves as a routing architecture), aims to decouple the identity of an endpoint from its location. </p> <p>The IP address continues to be the identity of the endpoint however, its location is now advertised as a separate entity (or address space) as well. </p> <p>A simple visual comparison helps understand this better:</p> <p></p> <p>The location is also known as a routing locator or RLOC in the LISP world. An example of an RLOC would be an edge node that the endpoint connects to. The identity is known as endpoint ID or EID. These two terms (EIDs and RLOCs) are crucial to understanding LISP. </p> <p>LISP, as an architecture, must facilitate bi-directional conversation between EIDs (for example, 10.1.1.1 and 20.1.1.1 in my example above). It does so by exhibiting tunneling like behavior - think of it as a stateless tunnel, whose purpose is purely to get packets from one RLOC to another. To do this, it encapsulates the packet (similar to VXLAN), slaps on headers on top that can carry the packet through the routed core to the destination RLOC.</p> <p>This, in turn, implies that there needs to be a mechanism that allows for RLOCs to advertise EIDs they are locally attached to, so that other RLOCs are aware of the location of remote EIDs. For this, LISP utilizes a mapping system that facilitates the propagation of EID to RLOC mappings. </p> <p>This is similar to how a routing protocol like OSPF advertises networks to its neighbors with one key difference - routing protocols (OSPF/EIGRP/BGP ...) are all based on a push model while LISP is based on a pull model (very important, considering the original goal and design of LISP was to solve the Internet's scalability problems). </p> <p>Think of this visually:</p> <p></p> <p>With your traditional routing models like OSPF, information was always pushed to the neighbors. This forced the peers to evaluate the prefixes, pushed them into RIB/FIB even if there were no active conversations against that prefix. </p> <p>LISP changes this dynamic completely by introducing a pull model. It works on an architecture where the RLOCs advertise their locally attached EIDs to a central system (SW1, in the topological example above) that stores it in some database. It DOES NOT advertise these EIDs to other RLOCs. If a RLOC wants to talk to a remote EID, it must query for information about this EID to the central system. As you can see, there is a quite the parallel here between LISP and DNS. </p>","tags":["sda","lisp"]},{"location":"blog/2021/12/25/cisco-sda-part-i---introduction-to-lisp-and-its-basic-terminology/#lisp-terminology","title":"LISP terminology","text":"<p>Now that we have a basic understanding of this new routing architecture paradigm that LISP introduces, let's talk about some common terminology that you should know:</p> <p>EIDs - endpoint IDs. These can be local (directly attached to a RLOC) or remote (not locally attached and learnt via the LISP mapping process).  RLOCs - routing locators. This is how the location of an endpoint is identified. Typically the first routed hop (the default gateway for your endpoints).  Map Server and Map Resolver (MS/MR) - the LISP mapping system is distributed across the Map Server and the Map Resolver. RLOCs send information about locally attached EIDs to the Map Server to store in its database while the Map Resolver is used to forward queries for EIDs to the Map Server so that it can reply back. Typically, one device acts as both the Map Server and the Map Resolver, especially in the SDA world.  eTR - egress tunnel router. This device registers its locally attached EIDs with the Map Server. It also decapsulates the encapsulated packets it receives to send natively towards the destination.  iTR - ingress tunnel router. This device sends requests to the Map Resolver about EIDs it wants to talk to. It also encapsulates a native packet with the appropriate LISP headers and forwards it on, post encapsulation. xTR - typically, one device will perform both eTR and iTR functions when you consider bi-directional conversation. Such a device is termed as 'xTR'. </p> <p>The following visualization should help you understand the above terminology better:</p> <p></p> <p>In the next post, we'll get to the meat of LISP - its basic configuration and operation.</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/04/cisco-sda-part-x---understanding-l2-handoff/","title":"Cisco SDA Part X - understanding L2 handoff","text":"<p>In this post, we take a detailed look at the L2 handoff feature in Cisco's SD-Access.</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/04/cisco-sda-part-x---understanding-l2-handoff/#introduction-and-topology","title":"Introduction and topology","text":"<p>Fair warning - this is going to be a long, long post. Get yourself some coffee because you're going to be here for a while!</p> <p>We're going to continue working with the following topology for this post, with a legacy network added to the existing infrastructure:</p> <p></p> <p>This is a fairly common scenario that you might run into with customer's migrating to SD-Access. The premise is this - say you have subnet X that you're going to be migrating into SD-Access but it cannot be done in one shot or one window. The requirement is to have this subnet co-exist in the fabric as well as outside the fabric, within a legacy portion of the network. </p> <p>From the perspective of our topology, we have the 192.2.21.0/24 subnet that needs to extend between the legacy network and the SD-Access fabric. For these kind of situations, a feature called L2 handoff was introduced. This post will walk you through the provisioning of L2 handoff via DNAC and help you understand what kind of configuration is pushed to make it work. The biggest question that I always try to answer will also be addressed - HOW does it really work?</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/04/cisco-sda-part-x---understanding-l2-handoff/#l2-handoff-configuration","title":"L2 handoff configuration","text":"<p>The L2 handoff configuration flow is quite similar to L3 handoff. Under 'Provision', go to the 'Fabric' tab. Under 'Fabric Infrastructure', you should see your fabric:</p> <p></p> <p>At this point in time, only one border can be used to do L2 handoff. In my case, I'd like to use Border1 for this purpose, so let's click on Border1. Once you do this, you should see the various details about Border1:</p> <p></p> <p>Within the \"Fabric\" tab here, click \"Configure\":</p> <p></p> <p>In this page, you should see two major options - Layer 3 Handoff and Layer 2 Handoff. For the purposes of this post, we are interested in L2 handoff so that's where our focus will be. As you can see, all VNs that have an IP pool assigned to them will be listed here. </p> <p>In our case, we are interested in extended an IP pool within the Guest_VN, so that's what we are going to select. Click on the name of the VN itself here. Once you do this, the following page will be displayed:</p> <p></p> <p>This is where some input needs to be given - you need to provide the external link that connects to the legacy network (this is a drop down menu and lists all the interfaces on the box - you simply need to choose the relevant interface). Additionally, you need to provide the external VLAN number - this is the legacy VLAN number that is in use in the legacy network for this subnet. </p> <p>In my case, the interface that connects to the legacy switch is Gi1/0/12 and the legacy VLAN is 666. Let's input this into the GUI:</p> <p></p> <p>Click on 'Save' now and you should go one page back. The VN that you chose to do L2 handoff for should now have its checkbox checked. After this, click on 'Add' in the bottom right corner and DNAC should start pushing the relevant configuration to Border1. </p> <p>So, what we've seen so far is the \"magic\" portion of it. The point and click. But what we really want to understand is what happens behind the scenes, don't we? Here's what is pushed on Border1 in terms of L2 handoff configuration:</p> <pre><code>// removing loopback that matched anycast gateway IP for this subnet\n\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:!exec: enable\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:no interface Loopback1027 \n\n//enabling role-based enforcement for this new VLAN\n\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:cts role-based enforcement vlan-list 666\n\n// creating L2 VLAN for this legacy VLAN\n\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:vlan 666\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:name 192_2_21_0-Guest_VN\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:exit \n\n//configuring the L2 handoff link as a trunk link allowing all VLANs\n\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:interface GigabitEthernet1/0/12 \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:no spanning-tree portfast \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:switchport \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:switchport mode trunk \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:switchport trunk allowed vlan all \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:exit \n\n//configuring a dynamic EID for this VN and a database-mapping for the L2 handoff subnet\n\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:router lisp \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:instance-id 4099\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:dynamic-eid 192_2_21_0-Guest_VN-IPV4\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:database-mapping 192.2.21.0/24  locator-set rloc_23bd3133-ba7b-4058-9552-e37e69cbc843 \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:exit-dynamic-eid \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:service ipv4 \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:route-export site-registrations \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:distance site-registrations 250\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:map-cache site-registration\n\n// creating a corresponding SVI for the legacy VLAN and enabling LISP mobility\n// for the new dynamic EID that was created\n\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:interface Vlan666 \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:no lisp mobility liveness test \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:no ip redirects \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:mac-address 0000.0c9f.f45e\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:description Configured from Cisco DNA-Center\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:vrf forwarding Guest_VN \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:ip address 192.2.21.1 255.255.255.0\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:ip helper-address 192.2.201.224\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:ip route-cache same-interface \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:lisp mobility 192_2_21_0-Guest_VN-IPV4\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:exit \n\n// calling the legacy VLAN under a ethernet instance ID\n\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:instance-id 8190\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:service ethernet \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:eid-table vlan 666\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:database-mapping mac locator-set rloc_23bd3133-ba7b-4058-9552-e37e69cbc843 \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:exit-service-ethernet \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:remote-rloc-probe on-route-change \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:exit-instance-id \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:site site_uci \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:description map-server configured from Cisco DNA-Center\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:authentication-key uci\n\n// enabling DHCP snooping for the legacy VLAN\n\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:ip dhcp snooping vlan 666\n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:ip dhcp relay information option \n%PARSER-5-CFGLOG_LOGGEDCMD: User:aninchat  logged command:ip domain lookup \n</code></pre> <p>Okay, we're getting closer to understanding what's really happening behind the scenes but we aren't there yet. What is the significance of all of this configuration? Let's look at some of the more important pieces of configuration.</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/04/cisco-sda-part-x---understanding-l2-handoff/#understanding-the-l2-handoff-configuration","title":"Understanding the L2 handoff configuration","text":"","tags":["sda","lisp"]},{"location":"blog/2022/01/04/cisco-sda-part-x---understanding-l2-handoff/#svi-creation-on-the-border-along-with-the-lisp-configuration","title":"SVI creation on the border, along with the LISP configuration","text":"<p>This is done in conjunction with removing the SVI on the legacy switch (presumably, this legacy switch was acting like the core of your network and was the first L3 hop and the default gateway for the hosts). The intention is to have Border1 as the default gateway for the legacy hosts now. </p> <p>Along with this, the LISP configuration allows for the IP addresses of the legacy hosts to be picked up natively by LISP (via the LISP dynamic EID configuration). This allows for the LISP database to be populated, from where the prefixes are pushed into RIB/FIB as directly connected routes. </p> <p>Let's confirm all of this for Host3, which is a host in the legacy network. </p> <pre><code>Border1#show lisp eid-table vrf Guest_VN ipv4 database \nLISP ETR IPv4 Mapping Database for EID-table vrf Guest_VN (IID 4099), LSBs: 0x1\nEntries total 3, no-route 0, inactive 1\n\n192.2.21.21/32, dynamic-eid SDA_Guest-IPV4, inherited from default locator-set rloc_23bd3133-ba7b-4058-9552-e37e69cbc843, auto-discover-rlocs\n  Locator       Pri/Wgt  Source     State\n  192.2.101.65   10/10   cfg-intf   site-self, reachable\n192.2.21.101/32, Inactive, expires: 23:04:11\n192.2.201.224/32, route-import, inherited from default locator-set rloc_23bd3133-ba7b-4058-9552-e37e69cbc843, auto-discover-rlocs\n  Locator       Pri/Wgt  Source     State\n  192.2.101.65   10/10   cfg-intf   site-self, reachable\n</code></pre> <p>As you can see, the LISP database has picked up this prefix as a /32 entry and the RLOC is Border1 itself. This is also marked as reachable, which implies it can now be registered to the control-plane (again, which is also Border1). Confirm that we see an entry in the site table of Border1:</p> <pre><code>Border1#show lisp eid-table vrf Guest_VN ipv4 server 192.2.21.21\nLISP Site Registration Information\n\nSite name: site_uci\nDescription: map-server configured from Cisco DNA-Center\nAllowed configured locators: any\nRequested EID-prefix:\n\n  EID-prefix: 192.2.21.21/32 instance-id 4099 \n    First registered:     00:58:55\n    Last registered:      00:00:15\n    Routing table tag:    0\n    Origin:               Dynamic, more specific of 192.2.21.0/24\n    Merge active:         Yes\n    Proxy reply:          Yes\n    TTL:                  1d00h\n    State:                complete\n    Registration errors:  \n      Authentication failures:   0\n      Allowed locators mismatch: 0\n    ETR 192.2.101.65, last registered 00:00:15, proxy-reply, map-notify\n                      TTL 1d00h, merge, hash-function sha1, nonce 0x82F1568F-0xA0070DA9\n                      state complete, no security-capability\n                      xTR-ID 0xCEF75710-0x6FF58983-0x14550F97-0xD2031D6E\n                      site-ID unspecified\n      Locator       Local  State      Pri/Wgt  Scope\n      192.2.101.65  yes    up          10/10   IPv4 none\n    Merged locators\n      Locator       Local  State      Pri/Wgt  Scope        Registering ETR\n      192.2.101.65  yes    up          10/10   IPv4 none    192.2.101.65:4342 \n</code></pre> <p>The site table for this VRF has this entry. Perfect! The last thing to confirm is the RIB/FIB, as this is what will eventually be used in the forwarding plane:</p> <pre><code>// RIB entry\n\nBorder1#show ip route vrf Guest_VN 192.2.21.21\n\nRouting Table: Guest_VN\nRouting entry for 192.2.21.21/32\n  Known via \"lisp\", distance 10, metric 1, type unknown\n  Redistributing via bgp 65003\n  Advertised by bgp 65003 metric 10\n  Last update from 192.2.21.21 on Vlan666, 00:56:10 ago\n  Routing Descriptor Blocks:\n  * 192.2.21.21, from 0.0.0.0, 00:56:10 ago, via Vlan666\n      Route metric is 1, traffic share count is 1\n\n// CEF entry\n\nBorder1#show ip cef vrf Guest_VN 192.2.21.21 detail\n192.2.21.21/32, epoch 0, flags [attached, subtree context]\n  SC owned,sourced: LISP local EID - \n  SC inherited: LISP cfg dyn-EID - LISP configured dynamic-EID\n  LISP EID attributes: localEID Yes, c-dynEID Yes, d-dynEID Yes\n  SC owned,sourced: LISP generalised SMR - [disabled, not inheriting, 0xFFA5C97148 locks: 1]\n  Adj source: IP adj out of Vlan666, addr 192.2.21.21 FFA5FE1200\n    Dependent covered prefix type adjfib, cover 192.2.21.0/24\n  2 IPL sources [no flags]\n  nexthop 192.2.21.21 Vlan666\n</code></pre> <p>This looks good too. There's a direct adjacency off of VLAN666 and the next hop is the host IP address itself. Border1 can simply ARP for the host directly and once resolved, it can forward the traffic to it. </p> <pre><code>// ARP entry for Host3\n\nBorder1#show ip arp vrf Guest_VN 192.2.21.21\nProtocol  Address          Age (min)  Hardware Addr   Type   Interface\nInternet  192.2.21.21             0   000c.29f4.3a5e  ARPA   Vlan666\n\n// mac entry for the corresponding mac address of Host3\n\nBorder1#show mac address-table address 000c.29f4.3a5e\n          Mac Address Table\n-------------------------------------------\n\nVlan    Mac Address       Type        Ports\n----    -----------       --------    -----\n 666    000c.29f4.3a5e    DYNAMIC     Gi1/0/12\nTotal Mac Addresses for this criterion: 1\n</code></pre> <p>As you can see, the host is reachable via the L2 handoff link.</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/04/cisco-sda-part-x---understanding-l2-handoff/#fabric-hosts","title":"Fabric hosts","text":"<p>Now that we've established how a legacy host should be learnt via L2 handoff, let's take a look at the fabric hosts. In our topology, we have two fabric hosts - Host1 with an IP address of 192.2.21.19 and and Host2 with an IP address of 192.2.21.20. We're not going to go into too much detail of how LISP registers these; what I want to focus on is what's really happening on Border1 with these hosts. </p> <p>Once LISP has registered these /32 IP addresses of these hosts to the map-server (Border1/Border2), you should see them in the site table:</p> <pre><code>Border1#show lisp eid-table vrf Guest_VN ipv4 server \nLISP Site Registration Information\n* = Some locators are down or unreachable\n# = Some registrations are sourced by reliable transport\n\nSite Name      Last      Up     Who Last             Inst     EID Prefix\n               Register         Registered           ID       \nsite_uci       never     no     --                   4099     0.0.0.0/0\n               never     no     --                   4099     192.2.21.0/24\n               01:45:23  yes#   192.2.101.70:51604   4099     192.2.21.19/32\n               01:23:24  yes#   192.2.101.71:17807   4099     192.2.21.20/32\n               00:00:21  yes    192.2.101.65:4342    4099     192.2.21.21/32\n               00:00:21  yes    192.2.101.65:4342    4099     192.2.201.224/32\n</code></pre> <p>Good - we certainly see those hosts in there. Now, from the site table, a particular LISP configuration pulls these host entries into the RIB, with an AD of 250 and installs them against Null0. </p> <pre><code>Border1#show run | sec router lisp\nrouter lisp\n\n *snip*\n\n !\n instance-id 4099\n  remote-rloc-probe on-route-change\n  dynamic-eid SDA_Guest-IPV4\n   database-mapping 192.2.21.0/24 locator-set rloc_23bd3133-ba7b-4058-9552-e37e69cbc843\n   exit-dynamic-eid\n  !\n  service ipv4\n   eid-table vrf Guest_VN\n   map-cache 0.0.0.0/0 map-request\n   route-import database bgp 65003 route-map DENY-Guest_VN locator-set rloc_23bd3133-ba7b-4058-9552-e37e69cbc843\n   route-export site-registrations\n   distance site-registrations 250\n   map-cache site-registration\n   exit-service-ipv4\n  !\n  exit-instance-id\n !\n\n *snip*\n\n exit-router-lisp\n</code></pre> <p>Look at the RIB now and confirm that these entries are present against Null0:</p> <pre><code>Border1#show ip route vrf Guest_VN \n\nRouting Table: Guest_VN\nCodes: L - local, C - connected, S - static, R - RIP, M - mobile, B - BGP\n       D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area \n       N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2\n       E1 - OSPF external type 1, E2 - OSPF external type 2\n       i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2\n       ia - IS-IS inter area, * - candidate default, U - per-user static route\n       o - ODR, P - periodic downloaded static route, H - NHRP, l - LISP\n       a - application route\n       + - replicated route, % - next hop override, p - overrides from PfR\n\nGateway of last resort is 192.2.100.10 to network 0.0.0.0\n\nB*    0.0.0.0/0 [20/0] via 192.2.100.10, 7w0d\n      192.2.21.0/24 is variably subnetted, 5 subnets, 2 masks\nC        192.2.21.0/24 is directly connected, Vlan666\nL        192.2.21.1/32 is directly connected, Vlan666\nl        192.2.21.19/32 [250/1], 01:41:14, Null0\nl        192.2.21.20/32 [250/1], 01:23:43, Null0\nl        192.2.21.21/32 [10/1] via 192.2.21.21, 01:10:15, Vlan666\n      192.2.100.0/24 is variably subnetted, 2 subnets, 2 masks\nC        192.2.100.8/30 is directly connected, Vlan3016\nL        192.2.100.9/32 is directly connected, Vlan3016\n      192.2.201.0/32 is subnetted, 1 subnets\nB        192.2.201.224 [20/0] via 192.2.100.10, 7w0d\n</code></pre> <p>Perfect! So, now, we have our fabric hosts in the RIB as well. Why are these prefixes against Null0 though? We'll understand this once we take a look at the packet walks.</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/04/cisco-sda-part-x---understanding-l2-handoff/#packet-walk-for-north-to-south-traffic","title":"Packet walk for North to South traffic","text":"<p>Okay, this is going to be two fold.  We're going to take a look at a host beyond the Fusion trying to reach a host within the fabric and as well as a host in the legacy network. Let's use the DNAC as a source host in this case. </p>","tags":["sda","lisp"]},{"location":"blog/2022/01/04/cisco-sda-part-x---understanding-l2-handoff/#dnac-to-fabric-hosts-host1host2-reachability","title":"DNAC to fabric hosts (Host1/Host2) reachability","text":"<p>From DNAC, we're going to do a simple ping to 192.2.21.19 and 192.2.21.20. The packet gets routed through the infrastructure that is above the borders and it eventually reaches Border1. </p> <p></p> <p>On Border1, a forwarding lookup is done and it matches the Null0 entry for these prefixes. </p> <pre><code>Border1#show ip route vrf Guest_VN 192.2.21.19\n\nRouting Table: Guest_VN\nRouting entry for 192.2.21.19/32\n  Known via \"lisp\", distance 250, metric 1, type intra area\n  Redistributing via bgp 65003\n  Advertised by bgp 65003 metric 10\n  Routing Descriptor Blocks:\n  * directly connected, via Null0\n      Route metric is 1, traffic share count is 1\n</code></pre> <p>This is why Null0 is important - one of the rules for triggering the LISP process is that the prefix should point to Null0. So, this implies that when this entry is hit, the LISP process is now invoked. </p> <p>LISP now tries to locate this EID. It goes into its map-cache to determine if there is an entry for this or not. Remember Border1s LISP configuration? It had a dynamic EID created for 192.2.21.0/24 - this also creates a corresponding entry in the map-cache table with an action of 'send-map-request'. </p> <pre><code>Border1#show lisp eid-table vrf Guest_VN ipv4 map-cache            \nLISP IPv4 Mapping Cache for EID-table vrf Guest_VN (IID 4099), 7 entries\n\n0.0.0.0/0, uptime: 7w0d, expires: never, via static-send-map-request\n  Negative cache entry, action: send-map-request\n0.0.0.0/1, uptime: 7w0d, expires: 00:00:41, via map-reply, forward-native\n  Encapsulating to proxy ETR \n192.2.21.0/24, uptime: 7w0d, expires: never, via dynamic-EID, send-map-request\n  Negative cache entry, action: send-map-request\n\n*snip* \n</code></pre> <p>Border1 can now query the map-resolver (it'll just query itself since it is a map-resolver also) and determine where this EID is located. </p> <p>We can see that we should hit the following entry in the site table:</p> <pre><code>Border1#show lisp eid-table vrf Guest_VN ipv4 server 192.2.21.19\nLISP Site Registration Information\n\nSite name: site_uci\nDescription: map-server configured from Cisco DNA-Center\nAllowed configured locators: any\nRequested EID-prefix:\n\n  EID-prefix: 192.2.21.19/32 instance-id 4099 \n    First registered:     22:09:16\n    Last registered:      22:09:16\n    Routing table tag:    0\n    Origin:               Dynamic, more specific of 192.2.21.0/24\n    Merge active:         No\n    Proxy reply:          Yes\n    TTL:                  1d00h\n    State:                complete\n    Registration errors:  \n      Authentication failures:   0\n      Allowed locators mismatch: 0\n    ETR 192.2.101.70:51604, last registered 22:09:16, proxy-reply, map-notify\n                            TTL 1d00h, no merge, hash-function sha1, nonce 0xE204F906-0x7BD3976D\n                            state complete, no security-capability\n                            xTR-ID 0x774DBE71-0x20FB1182-0x5798970B-0x64432A97\n                            site-ID unspecified\n                            sourced by reliable transport\n      Locator       Local  State      Pri/Wgt  Scope\n      192.2.101.70  yes    up          10/10   IPv4 none\n</code></pre> <p></p> <p>Once this map-request/map-reply process is over, the map-cache should be appropriately built with this entry and CEF should be overwritten as well, with Edge1 as the next hop for this host. </p> <pre><code>Border1#show lisp eid-table vrf Guest_VN ipv4 map-cache 192.2.21.19\nLISP IPv4 Mapping Cache for EID-table vrf Guest_VN (IID 4099), 7 entries\n\n192.2.21.19/32, uptime: 00:09:39, expires: 23:50:20, via map-reply, complete\n  Sources: map-reply, site-registration\n  State: complete, last modified: 00:09:39, map-source: 192.2.101.70\n  Exempt, Packets out: 25(14400 bytes) (~ 00:00:42 ago)\n  Configured as EID address space\n  Encapsulating dynamic-EID traffic\n  Locator       Uptime    State      Pri/Wgt     Encap-IID\n  192.2.101.70  00:09:39  up          10/10        -\n    Last up-down state change:         00:09:39, state change count: 1\n    Last route reachability change:    22:28:41, state change count: 1\n    Last priority / weight change:     never/never\n    RLOC-probing loc-status algorithm:\n      Last RLOC-probe sent:            00:09:39 (rtt 2ms)\n</code></pre> <p>Our final confirmation is the CEF entry:</p> <pre><code>Border1#show ip cef vrf Guest_VN 192.2.21.19\n192.2.21.19/32\n  nexthop 192.2.101.70 LISP0.4099\nBorder1#show ip cef vrf Guest_VN 192.2.21.19 detail\n192.2.21.19/32, epoch 0, flags [subtree context, check lisp eligibility]\n  SC owned,sourced: LISP remote EID - locator status bits 0x00000001\n  LISP remote EID: 39 packets 22464 bytes fwd action encap, cfg as EID space, dynamic EID need encap\n  SC inherited: LISP cfg dyn-EID - LISP configured dynamic-EID\n  LISP EID attributes: localEID No, c-dynEID Yes, d-dynEID No\n  LISP source path list\n    nexthop 192.2.101.70 LISP0.4099\n  2 IPL sources [no flags]\n  nexthop 192.2.101.70 LISP0.4099  \n</code></pre> <p>The packet is now encapsulated and sent to Edge1:</p> <p></p> <p>Inline, this packet looks like this:</p> <p></p>","tags":["sda","lisp"]},{"location":"blog/2022/01/04/cisco-sda-part-x---understanding-l2-handoff/#dnac-to-legacy-host","title":"DNAC to legacy host","text":"<p>Let's now take a look at how a packet reaches the legacy host (Host3). This is very straightforward. Again, the packet gets routed towards the borders and reaches Border1. </p> <p></p> <p>Border1 does a lookup in its forwarding table: </p> <pre><code>Border1#show ip cef vrf Guest_VN 192.2.21.21 detail \n192.2.21.21/32, epoch 0, flags [attached, subtree context]\n  SC owned,sourced: LISP local EID - \n  SC inherited: LISP cfg dyn-EID - LISP configured dynamic-EID\n  LISP EID attributes: localEID Yes, c-dynEID Yes, d-dynEID Yes\n  SC owned,sourced: LISP generalised SMR - [disabled, not inheriting, 0xFFA5C97148 locks: 1]\n  Adj source: IP adj out of Vlan666, addr 192.2.21.21 FFA5FE1200\n    Dependent covered prefix type adjfib, cover 192.2.21.0/24\n  2 IPL sources [no flags]\n  nexthop 192.2.21.21 Vlan666\n</code></pre> <p>This is a directly connected host so the ARP/CAM table will lead the packet out of the L2 handoff link. The following EPC capture on the legacy switch proves that the packet is a native IP packet, with only a 802.1Q header with a VLAN ID of 666:</p> <pre><code>Frame 42: 102 bytes on wire (816 bits), 102 bytes captured (816 bits) on interface 0\n    Interface id: 0\n    Encapsulation type: Ethernet (1)\n    Arrival Time: Mar 28, 2020 11:53:31.778445000 UTC\n    [Time shift for this packet: 0.000000000 seconds]\n    Epoch Time: 1585396411.778445000 seconds\n    [Time delta from previous captured frame: 0.044992000 seconds]\n    [Time delta from previous displayed frame: 0.044992000 seconds]\n    [Time since reference or first frame: 6.960737000 seconds]\n    Frame Number: 42\n    Frame Length: 102 bytes (816 bits)\n    Capture Length: 102 bytes (816 bits)\n    [Frame is marked: False]\n    [Frame is ignored: False]\n    [Protocols in frame: eth:vlan:ip:icmp:data]\nEthernet II, Src: Cisco_9f:f4:62 (00:00:0c:9f:f4:62), Dst: Vmware_f4:3a:5e (00:0c:29:f4:3a:5e)\n    Destination: Vmware_f4:3a:5e (00:0c:29:f4:3a:5e)\n        Address: Vmware_f4:3a:5e (00:0c:29:f4:3a:5e)\n        .... ..0. .... .... .... .... = LG bit: Globally unique address (factory default)\n        .... ...0 .... .... .... .... = IG bit: Individual address (unicast)\n    Source: Cisco_9f:f4:62 (00:00:0c:9f:f4:62)\n        Address: Cisco_9f:f4:62 (00:00:0c:9f:f4:62)\n        .... ..0. .... .... .... .... = LG bit: Globally unique address (factory default)\n        .... ...0 .... .... .... .... = IG bit: Individual address (unicast)\n    Type: 802.1Q Virtual LAN (0x8100)\n802.1Q Virtual LAN, PRI: 0, CFI: 0, ID: 666\n    000. .... .... .... = Priority: Best Effort (default) (0)\n    ...0 .... .... .... = CFI: Canonical (0)\n    .... 0010 1001 1010 = ID: 666\n    Type: IP (0x0800)\nInternet Protocol Version 4, Src: 172.17.22.11 (172.17.22.11), Dst: 192.2.21.21 (192.2.21.21)\n    Version: 4\n    Header length: 20 bytes\n    Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))\n        0000 00.. = Differentiated Services Codepoint: Default (0x00)\n        .... ..00 = Explicit Congestion Notification: Not-ECT (Not ECN-Capable Transport) (0x00)\n    Total Length: 84\n    Identification: 0x9a62 (39522)\n    Flags: 0x02 (Don't Fragment)\n        0... .... = Reserved bit: Not set\n        .1.. .... = Don't fragment: Set\n        ..0. .... = More fragments: Not set\n    Fragment offset: 0\n    Time to live: 61\n    Protocol: ICMP (1)\n    Header checksum: 0x0c13 [validation disabled]\n        [Good: False]\n        [Bad: False]\n    Source: 172.17.22.11 (172.17.22.11)\n    Destination: 192.2.21.21 (192.2.21.21)\nInternet Control Message Protocol\n    Type: 8 (Echo (ping) request)\n    Code: 0\n    Checksum: 0x1edf [correct]\n    Identifier (BE): 27426 (0x6b22)\n    Identifier (LE): 8811 (0x226b)\n    Sequence number (BE): 1 (0x0001)\n    Sequence number (LE): 256 (0x0100)\n    Timestamp from icmp data: Mar 28, 2020 12:15:37.000000000 UTC\n    [Timestamp from icmp data (relative): -1325.221555000 seconds]\n    Data (48 bytes) \n</code></pre> <p>This can be visualized like so:</p> <p></p>","tags":["sda","lisp"]},{"location":"blog/2022/01/04/cisco-sda-part-x---understanding-l2-handoff/#packet-walk-for-east-to-west-traffic","title":"Packet walk for East to West traffic","text":"<p>For this, we're going to take a look at a packet walk between Host1 (a fabric host) and Host3 (a legacy host). Since Host1 and Host3 are in the same subnet, when Host1 pings Host3, it will ARP for Host3 directly. </p> <p>ARP is a funny little thing within SDA so I'm not going to go into too much detail. Essentially, when Edge1 gets the ARP for Host3, it will query the map-server for Host3s IP address and learn its mac address. It then queries the mac address and the map-resolver will return Border1 as the RLOC in this case. </p> <p>The ARP is now encapsulated and sent to Border1. Visually, this should look like:</p> <p></p> <p>This is a unicast encapsulation; the destination IP address in the outer header is unicast - let's confirm that via a packet capture.  The following packet capture is taken inbound on Border1 as the packet comes from Edge1:</p> <p></p> <p>Remember, the inner ARP is still a broadcast. The packet is decapsulated by Border1 and this inner ARP is flooded in VLAN 666 (which maps to the VNID we see in the VXLAN header - 8196). </p> <p>This flooded ARP goes out the L2 handoff link and reaches Host3 via regular broadcast flood:</p> <p></p> <p>Similar process happens in reverse - Host3 replies to the ARP, which eventually gets encapsulated by Border1 and sent to Edge1, where it is decapsulated and sent to Host1. Host1 now has Host3s IP address resolved to its mac address so it can generate an ICMP echo and send it to Host3. This gets encapsulated and sent to Border1, since it is the RLOC for the legacy host. </p> <p>On the Border1, the forwarding table will say that this is directly connected:</p> <pre><code>// RIB entry\n\nBorder1#show ip route vrf Guest_VN 192.2.21.21\n\nRouting Table: Guest_VN\nRouting entry for 192.2.21.21/32\n  Known via \"lisp\", distance 10, metric 1, type unknown\n  Redistributing via bgp 65003\n  Advertised by bgp 65003 metric 10\n  Last update from 192.2.21.21 on Vlan666, 00:56:10 ago\n  Routing Descriptor Blocks:\n  * 192.2.21.21, from 0.0.0.0, 00:56:10 ago, via Vlan666\n      Route metric is 1, traffic share count is 1\n\n// CEF entry\n\nBorder1#show ip cef vrf Guest_VN 192.2.21.21 detail\n192.2.21.21/32, epoch 0, flags [attached, subtree context]\n  SC owned,sourced: LISP local EID - \n  SC inherited: LISP cfg dyn-EID - LISP configured dynamic-EID\n  LISP EID attributes: localEID Yes, c-dynEID Yes, d-dynEID Yes\n  SC owned,sourced: LISP generalised SMR - [disabled, not inheriting, 0xFFA5C97148 locks: 1]\n  Adj source: IP adj out of Vlan666, addr 192.2.21.21 FFA5FE1200\n    Dependent covered prefix type adjfib, cover 192.2.21.0/24\n  2 IPL sources [no flags]\n  nexthop 192.2.21.21 Vlan666\n</code></pre> <p>The packet is decap'd and sent out natively towards Host3. A similar process happens in the reverse direction - the native packet reaches Border1. Because the destination mac address is of Border1 itself, a routing lookup is done which results in either the Null0 entry for 192.2.21.19 or a LISP resolved entry:</p> <pre><code>Border1#show ip cef vrf Guest_VN 192.2.21.19\n192.2.21.19/32\n  nexthop 192.2.101.70 LISP0.4099\nBorder1#show ip cef vrf Guest_VN 192.2.21.19 detail\n192.2.21.19/32, epoch 0, flags [subtree context, check lisp eligibility]\n  SC owned,sourced: LISP remote EID - locator status bits 0x00000001\n  LISP remote EID: 39 packets 22464 bytes fwd action encap, cfg as EID space, dynamic EID need encap\n  SC inherited: LISP cfg dyn-EID - LISP configured dynamic-EID\n  LISP EID attributes: localEID No, c-dynEID Yes, d-dynEID No\n  LISP source path list\n    nexthop 192.2.101.70 LISP0.4099\n  2 IPL sources [no flags]\n  nexthop 192.2.101.70 LISP0.4099 \n</code></pre> <p>Visually, the entire forwarding path is like so in the direction of Host1-&gt;Host3:</p> <p></p> <p>And like so in the direction of Host3-&gt;Host1:</p> <p></p>","tags":["sda","lisp"]},{"location":"blog/2022/01/05/cisco-sda-part-xi---understanding-arp-in-sda/","title":"Cisco SDA Part XI - understanding ARP in SDA","text":"<p>In this post, we look at how an ARP packet flows within a SD-Access fabric.</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/05/cisco-sda-part-xi---understanding-arp-in-sda/#introduction-and-topology","title":"Introduction and topology","text":"<p>This probably should have been one of my first posts for SDA but here we are. I've recently come to realize that there is a lot of misconception about how ARP works within the SDA solution - the defacto answer appears to be that it is a part of BUM traffic (broadcast, unknown unicast, multicast), and thus, it will be flooded (implying that there is a dependency on some form of replication, either head-end or via an underlay multicast infrastructure). </p> <p>This is not true and it's time to bust that myth! We will continue to use the same topology, only this time, Host1 and Host2 are part of the same subnet (192.2.11.0/24) and the same VN - Corp_VN. </p> <p></p>","tags":["sda","lisp"]},{"location":"blog/2022/01/05/cisco-sda-part-xi---understanding-arp-in-sda/#baselining-the-network","title":"Baselining the network","text":"<p>First things first - let's gather some preliminary data to establish a baseline before we start testing. Both hosts have generated some minimal traffic, due to which their IP addresses and macs are present in the IPDT (IP device tracking) tables and the LISP database. </p> <pre><code>// Edge1s IPDT IP table\n\nEdge1#show device-tracking database interface gig1/0/23\nportDB has 2 entries for interface Gi1/0/23, 2 dynamic \nCodes: L - Local, S - Static, ND - Neighbor Discovery, ARP - Address Resolution Protocol, DH4 - IPv4 DHCP, DH6 - IPv6 DHCP, PKT - Other Packet, API - API created\nPreflevel flags (prlvl):\n0001:MAC and LLA match     0002:Orig trunk            0004:Orig access           \n0008:Orig trusted trunk    0010:Orig trusted access   0020:DHCP assigned         \n0040:Cga authenticated     0080:Cert authenticated    0100:Statically assigned   \n\n\n    Network Layer Address               Link Layer Address Interface        vlan prlvl  age   state     Time left        \nAPI 192.2.11.100                            000c.2993.006c  Gi1/0/23       1026  0005    5mn REACHABLE  1536 ms try 0    \nND  FE80::1D22:B4A3:26A2:B064               000c.2993.006c  Gi1/0/23       1026  0005    3mn REACHABLE  130 s try 0   \n\n\n// Edge2s IPDT IP table\n\nEdge2#show device-tracking database interface gig1/0/6\nportDB has 2 entries for interface Gi1/0/6, 2 dynamic \nCodes: L - Local, S - Static, ND - Neighbor Discovery, ARP - Address Resolution Protocol, DH4 - IPv4 DHCP, DH6 - IPv6 DHCP, PKT - Other Packet, API - API created\nPreflevel flags (prlvl):\n0001:MAC and LLA match     0002:Orig trunk            0004:Orig access           \n0008:Orig trusted trunk    0010:Orig trusted access   0020:DHCP assigned         \n0040:Cga authenticated     0080:Cert authenticated    0100:Statically assigned   \n\n\n    Network Layer Address               Link Layer Address Interface        vlan prlvl  age   state     Time left        \nARP 192.2.11.200                            000c.29f2.c674  Gi1/0/6        1026  0005   20s  REACHABLE  287 s try 0      \nND  FE80::4CEA:EE0:9E2D:B08                 000c.29f2.c674  Gi1/0/6        1026  0005  175s  REACHABLE  125 s try 0  \n\n\n// Edge1s IPDT mac table\n\nEdge1#show device-tracking database mac                    \n MAC              Interface      vlan prlvl      state          time left policy\n 7872.5dab.d2e6   Gi1/0/24       2045 NO TRUST   MAC-REACHABLE  294 s     LISP-DT-GUARD-VLAN \n 7486.0b05.1fff   Vl2045         2045 TRUSTED    MAC-STALE      N/A       LISP-DT-GUARD-VLAN \n 7486.0b05.1ff8   Vl1026         1026 TRUSTED    MAC-STALE      N/A       LISP-DT-GUARD-VLAN \n 7486.0b05.1fee   Vl1028         1028 TRUSTED    MAC-DOWN       N/A       LISP-DT-GUARD-VLAN \n 7486.0b05.1fdc   Vl1027         1027 TRUSTED    MAC-DOWN       N/A       LISP-DT-GUARD-VLAN \n 000c.2993.006c   Gi1/0/23       1026 NO TRUST   MAC-REACHABLE  269 s     IPDT_MAX_10 \n 0000.0c9f.f85c   Vl2045         2045 TRUSTED    MAC-REACHABLE  N/A       LISP-DT-GUARD-VLAN \n 0000.0c9f.f463   Vl1028         1028 TRUSTED    MAC-DOWN       N/A       LISP-DT-GUARD-VLAN \n 0000.0c9f.f462   Vl1027         1027 TRUSTED    MAC-DOWN       N/A       LISP-DT-GUARD-VLAN \n 0000.0c9f.f461   Vl1026         1026 TRUSTED    MAC-REACHABLE  N/A       LISP-DT-GUARD-VLAN\n\n// Edge2s IPDT mac table\n\nEdge2#show device-tracking database mac                \n MAC              Interface      vlan prlvl      state          time left policy\n 7486.0b05.7e7f   Vl2045         2045 TRUSTED    MAC-STALE      N/A       LISP-DT-GUARD-VLAN \n 7486.0b05.7e78   Vl1026         1026 TRUSTED    MAC-STALE      N/A       LISP-DT-GUARD-VLAN \n 7486.0b05.7e6e   Vl1028         1028 TRUSTED    MAC-DOWN       N/A       LISP-DT-GUARD-VLAN \n 7486.0b05.7e5c   Vl1027         1027 TRUSTED    MAC-DOWN       N/A       LISP-DT-GUARD-VLAN \n 380e.4d5b.0d40   Gi1/0/24       2045 NO TRUST   MAC-REACHABLE  290 s     LISP-DT-GUARD-VLAN \n 000c.29f2.c674   Gi1/0/6        1026 NO TRUST   MAC-REACHABLE  275 s     IPDT_MAX_10 \n 0000.0c9f.f85c   Vl2045         2045 TRUSTED    MAC-REACHABLE  N/A       LISP-DT-GUARD-VLAN \n 0000.0c9f.f463   Vl1028         1028 TRUSTED    MAC-DOWN       N/A       LISP-DT-GUARD-VLAN \n 0000.0c9f.f462   Vl1027         1027 TRUSTED    MAC-DOWN       N/A       LISP-DT-GUARD-VLAN \n 0000.0c9f.f461   Vl1026         1026 TRUSTED    MAC-REACHABLE  N/A       LISP-DT-GUARD-VLAN\n</code></pre> <p>LISP should have picked these up dynamically as well, so we should see these addresses in the LISP database as well. The allocated instance-ID for IPv4 is 4100 and for Ethernet, it is 8195.</p> <pre><code>// Edge1 and Edge2s IPv4 LISP database\n\nEdge1#show lisp instance-id 4100 ipv4 database \nLISP ETR IPv4 Mapping Database for EID-table vrf Corp_VN (IID 4100), LSBs: 0x1\nEntries total 2, no-route 0, inactive 1\n\n192.2.11.100/32, dynamic-eid SDA_Prod-IPV4, inherited from default locator-set rloc_70ac0729-86f6-44d0-a1c9-254b98507c24\n  Locator       Pri/Wgt  Source     State\n  192.2.101.70   10/10   cfg-intf   site-self, reachable \n\nEdge2#show lisp instance-id 4100 ipv4 database \nLISP ETR IPv4 Mapping Database for EID-table vrf Corp_VN (IID 4100), LSBs: 0x1\nEntries total 2, no-route 0, inactive 1\n\n192.2.11.200/32, dynamic-eid SDA_Prod-IPV4, inherited from default locator-set rloc_842654a1-cb84-42ec-9187-2e3a46790cc1\n  Locator       Pri/Wgt  Source     State\n  192.2.101.71   10/10   cfg-intf   site-self, reachable\n\n// Edge1 and Edge2s Ethernet LISP database\n\nEdge1#show lisp instance-id 8195 ethernet database \nLISP ETR MAC Mapping Database for EID-table Vlan 1026 (IID 8195), LSBs: 0x1\nEntries total 1, no-route 0, inactive 0\n\n000c.2993.006c/48, dynamic-eid Auto-L2-group-8195, inherited from default locator-set rloc_70ac0729-86f6-44d0-a1c9-254b98507c24\n  Locator       Pri/Wgt  Source     State\n  192.2.101.70   10/10   cfg-intf   site-self, reachable\n\nEdge2#show lisp instance-id 8195 ethernet database\nLISP ETR MAC Mapping Database for EID-table Vlan 1026 (IID 8195), LSBs: 0x1\nEntries total 3, no-route 0, inactive 0\n\n000c.29f2.c674/48, dynamic-eid Auto-L2-group-8195, inherited from default locator-set rloc_842654a1-cb84-42ec-9187-2e3a46790cc1\n  Locator       Pri/Wgt  Source     State\n  192.2.101.71   10/10   cfg-intf   site-self, reachable\n</code></pre> <p>Perfect. Since these are registered in the LISP database, we should see them in the site table on the control-plane as well. Let's quickly check Border1 for this:</p> <pre><code>// site table for IPv4\n\nBorder1#show lisp instance-id 4100 ipv4 server \nLISP Site Registration Information\n* = Some locators are down or unreachable\n# = Some registrations are sourced by reliable transport\n\nSite Name      Last      Up     Who Last             Inst     EID Prefix\n               Register         Registered           ID       \nsite_uci       never     no     --                   4100     0.0.0.0/0\n               never     no     --                   4100     192.2.11.0/24\n               15:12:30  yes#   192.2.101.70:14450   4100     192.2.11.100/32\n               15:12:30  yes#   192.2.101.71:17807   4100     192.2.11.200/32\n\n// site table for Ethernet\n\nBorder1#show lisp instance-id 8195 ethernet server \nLISP Site Registration Information\n* = Some locators are down or unreachable\n# = Some registrations are sourced by reliable transport\n\nSite Name      Last      Up     Who Last             Inst     EID Prefix\n               Register         Registered           ID       \nsite_uci       never     no     --                   8195     any-mac\n               15:12:39  yes#   192.2.101.71:17807   8195     0000.0c9f.f461/48\n               00:26:37  yes#   192.2.101.70:14450   8195     000c.2993.006c/48\n               01:50:40  yes#   192.2.101.71:17807   8195     000c.29f2.c674/48\n               15:12:39  yes#   192.2.101.71:17807   8195     7486.0b05.7e78/48\n</code></pre> <p>One last thing to verify before we start our testing - let's make sure the hosts do not have each others IP address resolved already. </p> <pre><code>// Host1s ARP table\n\nC:\\Users\\Host1&gt;arp -a\n\nInterface: 192.2.11.100 --- 0xb\n  Internet Address      Physical Address      Type\n  192.2.11.1            00-00-0c-9f-f4-61     dynamic\n  192.2.11.255          ff-ff-ff-ff-ff-ff     static\n  224.0.0.22            01-00-5e-00-00-16     static\n  224.0.0.252           01-00-5e-00-00-fc     static\n\n// Host2s ARP table\n\nC:\\Users\\Host2&gt;arp -a\n\nInterface: 192.2.11.200 --- 0xb\n  Internet Address      Physical Address      Type\n  192.2.11.1            00-00-0c-9f-f4-61     dynamic\n  192.2.11.255          ff-ff-ff-ff-ff-ff     static\n  224.0.0.22            01-00-5e-00-00-16     static\n  224.0.0.252           01-00-5e-00-00-fc     static\n</code></pre> <p>Only the gateway IPs are resolved and the hosts have no knowledge of each other. </p>","tags":["sda","lisp"]},{"location":"blog/2022/01/05/cisco-sda-part-xi---understanding-arp-in-sda/#arp-packet-flow","title":"ARP packet flow","text":"<p>Naturally, our test is very simple - we will ping from Host1 to Host2. </p> <pre><code>C:\\Users\\admin-PC1&gt;ping 192.2.11.200\n\nPinging 192.2.11.200 with 32 bytes of data:\nReply from 192.2.11.200: bytes=32 time=843ms TTL=128\nReply from 192.2.11.200: bytes=32 time=1ms TTL=128\nReply from 192.2.11.200: bytes=32 time=1ms TTL=128\nReply from 192.2.11.200: bytes=32 time=1ms TTL=128\n\nPing statistics for 192.2.11.200:\n    Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),\nApproximate round trip times in milli-seconds:\n    Minimum = 0ms, Maximum = 5ms, Average = 1ms\n</code></pre> <p>Pings work too. Let's try to understand what happened here. </p> <p>Host1 generates an ARP request for Host2s IP address, since it knows it is in the same subnet. This hits Edge1. </p> <p></p> <p>A packet capture on Gi1/0/23 on Edge1 confirms that the ARP is a broadcast as it comes into Edge1:</p> <p></p> <p>Here's where things get interesting. The ARP will not be flooded immediately. Edge1 will first send out a map-request for the IP address that is present in the \"Target IP address\" of the ARP packet:</p> <p></p> <p>The control-plane nodes maintain an address-resolution table which has IP-mac mapping entries for all hosts in the fabric. This is similar to how BGP EVPN, in a VXLAN fabric, maintains an EVPN ARP suppression table. </p> <p>You can view this table using the following command:</p> <pre><code>Border1#show lisp instance-id 8195 ethernet server address-resolution \n\nAddress-resolution data for router lisp 0 instance-id 8195\n\nL3 InstID    Host Address                                Hardware Address\n     4100    192.2.11.100/32                             000c.2993.006c\n     4100    192.2.11.200/32                             000c.29f2.c674\n     4100    FE80::1D22:B4A3:26A2:B064/128               000c.2993.006c\n     4100    FE80::4CEA:EE0:9E2D:B08/128                 000c.29f2.c674\n</code></pre> <p>When this map-request (generated from an ARP request) comes to the control-plane (Border1, in this case), it looks at this table for a hit. If there is a hit, it sends a map-reply with the mac address of the host. </p> <p></p> <p>Wireshark doesn't have the proper dissectors to  understand this packet fully but here's the map-reply anyway:</p> <p></p> <p>When Edge1 gets this, it builds another map-request - this time, for the mac address it just got from the control-plane. This leads to a second round of map-request/map-reply. When Border1 gets the map-request, it looks at the Ethernet site table (since the request is for a mac address) and responds back with the RLOC associated with this mac (which is Edge2). </p> <p></p> <p>Inline, this looks like so:</p> <p></p> <p>Border1 replies back with the RLOC:</p> <p></p> <p>Inline, this looks like so:</p> <p></p> <p>At this point, Edge1 should have its LISP map-cache built as well:</p> <pre><code>Edge1#show lisp instance-id 8195 ethernet map-cache detail \nLISP MAC Mapping Cache for EID-table Vlan 1026 (IID 8195), 1 entries\n\n000c.29f2.c674/48, uptime: 03:57:51, expires: 20:02:08, via map-reply, complete\n  Sources: map-reply\n  State: complete, last modified: 03:57:51, map-source: 192.2.101.71\n  Idle, Packets out: 0(0 bytes)\n  Encapsulating dynamic-EID traffic\n  Locator       Uptime    State      Pri/Wgt     Encap-IID\n  192.2.101.71  03:57:51  up          10/10        -\n    Last up-down state change:         03:57:51, state change count: 1\n    Last route reachability change:    03:57:51, state change count: 1\n    Last priority / weight change:     never/never\n    RLOC-probing loc-status algorithm:\n      Last RLOC-probe sent:            03:57:51 (rtt 2ms)\n</code></pre> <p>Remember to look at the Ethernet instance-ID and not the IPv4 one. What happens now? The ARP is converted into a unicast message on Edge1 and is encapsulated with a destination IP of Edge2:</p> <p></p> <p>Inline, this looks like this:</p> <p></p> <p>Notice how the ARP is no longer a broadcast - it has been converted into a unicast ARP on Edge1 and encapsulated with a VXLAN header with a VNID of the Ethernet service (for that VLAN). The destination IP is of Loopback0 of Edge2. </p> <p>Once Edge2 gets this, it decapsulates the packet and unicasts it to Host2. This entire process now happens in reverse when Host2 sends an ARP reply in response to the ARP request. </p> <p>So, one final question to close this post out - when do we really use some form of replication for ARP? Naturally, when there is no hit in the address-resolution table of the control-plane. This also implies that this is perfect for silent hosts and this is exactly where L2 flooding comes into the picture. </p> <p>With L2 flooding (and an underlay multicast infrastructure), we can flood ARPs through the fabric and force a silent host to respond, thus bringing it alive in the fabric. L2 flooding should be another post on its own so we'll get to it some day!</p> <p>Thanks for reading, I hope this was informative! Till the next one, hopefully in better times (yes, this quarantine/social-distancing sucks but it is necessary). </p>","tags":["sda","lisp"]},{"location":"blog/2022/01/06/cisco-sda-part-xii---bsr-auto-rp-for-the-fabric-and-a-catch-22/","title":"Cisco SDA part XII - BSR, Auto-RP for the fabric and a catch-22","text":"<p>In this post, we look at why BSR and Auto-RP do not work with a SD-Access fabric.</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/06/cisco-sda-part-xii---bsr-auto-rp-for-the-fabric-and-a-catch-22/#introduction-and-topology","title":"Introduction and topology","text":"<p>This is the topology that we'll be working with for this blog post:</p> <p></p> <p>Nothing too fancy - two edges, two co-located control-plane and border devices and a fusion that sits outside of the fabric for inter-VRF (inter-VN) communication. </p> <p>A lot of customers have their RPs outside of the fabric and want to use one of two protocols that allow for discovery of RP without static assignment - Bootstrap Router (BSR) or Auto-RP (Cisco Proprietary). </p>","tags":["sda","lisp"]},{"location":"blog/2022/01/06/cisco-sda-part-xii---bsr-auto-rp-for-the-fabric-and-a-catch-22/#the-problem-with-bsr-and-a-sda-fabric","title":"The problem with BSR and a SDA fabric","text":"<p>Off the bat, I can tell you that BSR does not (and will not) work with a SDA fabric today - if you closely look at how we form PIM neighbors for the fabric (we don't, really), it is a static neighbor, enabled by appropriate code based on the RP and the RPF neighbor for it. We're not really exchanging PIM HELLOs in the overlay and thus, no active neighbor discovery. Don't believe me? Let's confirm:</p> <p>The fusion sees the two borders as active PIM neighbors:</p> <pre><code>Fusion#show ip pim vrf Corporate neighbor \nPIM Neighbor Table\nMode: B - Bidir Capable, DR - Designated Router, N - Default DR Priority,\n      P - Proxy Capable, S - State Refresh Capable, G - GenID Capable,\n      L - DR Load-balancing Capable\nNeighbor          Interface                Uptime/Expires    Ver   DR\nAddress                                                            Prio/Mode\n192.168.1.1       Te0/0/1.3001             03:01:10/00:01:25 v2    1 / S P G\n192.168.1.17      Gi0/0/0.3005             03:01:07/00:01:15 v2    1 / S P G\n</code></pre> <p>The borders only see the fusion and nothing else:</p> <pre><code>Border1#show ip pim vrf Corporate neighbor\nPIM Neighbor Table\nMode: B - Bidir Capable, DR - Designated Router, N - Default DR Priority,\n      P - Proxy Capable, S - State Refresh Capable, G - GenID Capable,\n      L - DR Load-balancing Capable\nNeighbor          Interface                Uptime/Expires    Ver   DR\nAddress                                                            Prio/Mode\n192.168.1.18      Vlan3005                 03:49:59/00:01:36 v2    1 / DR S P G\n</code></pre> <pre><code>Border1#show ip pim vrf Corporate neighbor\nPIM Neighbor Table\nMode: B - Bidir Capable, DR - Designated Router, N - Default DR Priority,\n      P - Proxy Capable, S - State Refresh Capable, G - GenID Capable,\n      L - DR Load-balancing Capable\nNeighbor          Interface                Uptime/Expires    Ver   DR\nAddress                                                            Prio/Mode\n192.168.1.18      Vlan3005                 03:49:59/00:01:36 v2    1 / DR S P G\n</code></pre> <p>The edge (showing just Edge1 as an example here), as expected, sees no neighbor:</p> <pre><code>Edge1#show ip pim vrf Corporate neighbor\nPIM Neighbor Table\nMode: B - Bidir Capable, DR - Designated Router, N - Default DR Priority,\n      P - Proxy Capable, S - State Refresh Capable, G - GenID Capable,\n      L - DR Load-balancing Capable\nNeighbor          Interface                Uptime/Expires    Ver   DR\nAddress                                                            Prio/Mode\n</code></pre> <p>As I said earlier, a neighbor is injected into the PIM neighbor list  based on who the RPF neighbor is for the RP.  So, let's test that out - I'll add a static RP address on Edge1 now. </p> <pre><code>Edge1(config)#ip pim vrf Corporate rp-address 192.168.250.1\n</code></pre> <p>The RPF neighbor for this RP address is:</p> <pre><code>Edge1#show ip rpf vrf Corporate 192.168.250.1\nRPF information for ? (192.168.250.1)\n  RPF interface: LISP0.4101\n  RPF neighbor: ? (192.168.10.68)\n  RPF route/mask: 192.168.250.1/32\n  RPF type: unicast ()\n  Doing distance-preferred lookups across tables\n  RPF topology: ipv4 multicast base\n</code></pre> <p>This neighbor now gets inserted into the PIM neighbor list:</p> <pre><code>Edge1#show ip pim vrf Corporate neighbor     \nPIM Neighbor Table\nMode: B - Bidir Capable, DR - Designated Router, N - Default DR Priority,\n      P - Proxy Capable, S - State Refresh Capable, G - GenID Capable,\n      L - DR Load-balancing Capable\nNeighbor          Interface                Uptime/Expires    Ver   DR\nAddress                                                            Prio/Mode\n192.168.10.68     LISP0.4101               00:03:53/00:01:55 v2    0 /\n</code></pre> <p>This logic, however, breaks BSR functionality with the RP outside of the fabric. Why? Well, BSR uses a flood to all PIM neighbors mechanism (it floods to the multicast destination 224.0.0.13) to forward the candidate RP(s) information - assuming the fusion (in our case) sends this to the borders, which are forming active PIM neighbors with it, the borders, in turn, cannot send this to the edges because the edges never show up as PIM neighbors for it. </p> <p>Here's a quote from the BSR RFC to confirm how BSR messages are forwarded:</p> <p>When a Bootstrap message is forwarded, it is forwarded out of every multicast-capable interface that has PIM neighbors (including the one over which the message was received)</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/06/cisco-sda-part-xii---bsr-auto-rp-for-the-fabric-and-a-catch-22/#auto-rp-and-a-catch-22","title":"Auto-RP and a catch-22","text":"<p>Auto-RP, however, does work but there's a catch-22 going on here and I want to make sure everyone understands WHY it works - let's pull back the curtain, shall we? </p> <p>Before we get into the fabric specific details, let's recall how Auto-RP works. Here's my one minute crash course - Auto-RP uses two multicast groups to distribute RP information, 224.0.1.39 and 224.0.1.40, and includes two new roles for its functionality - candidate RPs and Mapping Agents. The candidate RPs announce their RP address to the mapping agent(s) using 224.0.1.39 and the mapping agent elects one of these as the RP address and announces that to the network using 224.0.1.40. </p> <p>So, if you want to learn of an RP using Auto-RP, you need to be subscribed to 224.0.1.40, which all Cisco multicast enabled devices do, by default. But, how do you build a multicast tree for a group that you don't know the RP for? Thus, for the Auto-RP groups, the expectation is to fall back to dense mode to flood these messages. There are several ways to do this, the simplest and easiest is to enable the 'autorp listener' feature using the CLI 'ip pim autorp listener' or for a specific VRF, using 'ip pim vrf  autorp listener'. <p>In our topology, I'm going to configure a loopback on the fusion, enable PIM SM on it and put it in the appropriate VRF. </p> <pre><code>Fusion#show run int loopback1\nBuilding configuration...\n\nCurrent configuration : 115 bytes\n!\ninterface Loopback1\n vrf forwarding Corporate\n ip address 192.168.250.1 255.255.255.255\n ip pim sparse-mode\nend\n</code></pre> <p>Next, I'll advertise this loopback as the candidate RP. The same fusion will also be a mapping agent, just to keep things simple. </p> <pre><code>Fusion(config)#ip pim vrf Corporate send-rp-announce Loopback1 scope 255 interval 5\nFusion(config)#ip pim vrf Corporate send-rp-discovery scope 255 interval 5\n</code></pre> <p>An important point to note - our devices are configured to be autorp listeners by default (not sure from what release for the IOS/IOS-XE platforms). You can confirm this with the following:</p> <pre><code>Fusion#show run all | in autorp\nip pim autorp\nip pim vrf Corporate autorp\n</code></pre> <p>This is only seen with the 'all' argument against the running-config, which implies it is a default configuration present on the device. This is important because this allows the device to use PIM dense mode for 224.0.1.39 and 224.0.1.40, which enables the packet to be flooded on all interfaces (dense-mode forwarding rules apply). </p> <p>The borders get this and are able to derive the RP information from it. </p> <pre><code>Border1#show ip pim vrf Corporate rp mapping \nPIM Group-to-RP Mappings\n\nGroup(s) 224.0.0.0/4\n  RP 192.168.250.1 (?), v2v1\n    Info source: 192.168.1.18 (?), elected via Auto-RP\n         Uptime: 12:52:44, expires: 00:00:11\n</code></pre> <pre><code>Border2#show ip pim vrf Corporate rp mapping \nPIM Group-to-RP Mappings\n\nGroup(s) 224.0.0.0/4\n  RP 192.168.250.1 (?), v2v1\n    Info source: 192.168.1.2 (?), elected via Auto-RP\n         Uptime: 09:31:01, expires: 00:00:13\n</code></pre> <p>Let's look at the edge now (taking Edge1 as an example). </p> <pre><code>Edge1#show ip pim vrf Corporate rp mapping\nPIM Group-to-RP Mappings\n</code></pre> <p>Edge1 has not received this information. This is because 224.0.1.40 is treated as any other ASM group in the fabric and therein lies the flaw and the catch-22. If you're using native multicast for the underlay (as an example), this implies that an underlay SSM tree should be built for the packets to be correctly delivered. </p> <p>How would the underlay SSM tree built if the RP is not known for the group? We use the RP information, find out the RPF neighbor for the RP and build our underlay SSM tree against that neighbor by sending PIM (S,G) joins to it. As you can see, the mroute table for this group on Edge1 has a null incoming interface and the RP is not known.</p> <pre><code>Edge1#show ip mroute vrf Corporate 224.0.1.40 verbose\nIP Multicast Routing Table\nFlags: D - Dense, S - Sparse, B - Bidir Group, s - SSM Group, C - Connected,\n       L - Local, P - Pruned, R - RP-bit set, F - Register flag,\n       T - SPT-bit set, J - Join SPT, M - MSDP created entry, E - Extranet,\n       X - Proxy Join Timer Running, A - Candidate for MSDP Advertisement,\n       U - URD, I - Received Source Specific Host Report, \n       Z - Multicast Tunnel, z - MDT-data group sender, \n       Y - Joined MDT-data group, y - Sending to MDT-data group, \n       G - Received BGP C-Mroute, g - Sent BGP C-Mroute, \n       N - Received BGP Shared-Tree Prune, n - BGP C-Mroute suppressed, \n       Q - Received BGP S-A Route, q - Sent BGP S-A Route, \n       V - RD &amp; Vector, v - Vector, p - PIM Joins on route, \n       x - VxLAN group, c - PFP-SA cache created entry\nOutgoing interface flags: H - Hardware switched, A - Assert winner, p - PIM Join\n Timers: Uptime/Expires\n Interface state: Interface, Next-Hop or VCD, State/Mode\n\n(*, 224.0.1.40), 00:00:28/00:02:36, RP 0.0.0.0, flags: DCL\n  Incoming interface: Null, RPF nbr 0.0.0.0\n  Outgoing interface list:\n    Loopback4101, Forward/Sparse, 00:00:28/00:02:36\n</code></pre> <p>Thus, if I manually add the RP on Edge1, I should now see Auto-RP packets as well.</p> <pre><code>Edge1(config)#ip pim vrf Corporate rp-address 192.168.250.1\n</code></pre> <p>Now, I see the mroute table built correctly as well:</p> <pre><code>Edge1#show ip mroute vrf Corporate 224.0.1.40 verbose\nIP Multicast Routing Table\nFlags: D - Dense, S - Sparse, B - Bidir Group, s - SSM Group, C - Connected,\n       L - Local, P - Pruned, R - RP-bit set, F - Register flag,\n       T - SPT-bit set, J - Join SPT, M - MSDP created entry, E - Extranet,\n       X - Proxy Join Timer Running, A - Candidate for MSDP Advertisement,\n       U - URD, I - Received Source Specific Host Report, \n       Z - Multicast Tunnel, z - MDT-data group sender, \n       Y - Joined MDT-data group, y - Sending to MDT-data group, \n       G - Received BGP C-Mroute, g - Sent BGP C-Mroute, \n       N - Received BGP Shared-Tree Prune, n - BGP C-Mroute suppressed, \n       Q - Received BGP S-A Route, q - Sent BGP S-A Route, \n       V - RD &amp; Vector, v - Vector, p - PIM Joins on route, \n       x - VxLAN group, c - PFP-SA cache created entry\nOutgoing interface flags: H - Hardware switched, A - Assert winner, p - PIM Join\n Timers: Uptime/Expires\n Interface state: Interface, Next-Hop or VCD, State/Mode\n\n(*, 224.0.1.40), 00:02:09/00:02:57, RP 192.168.250.1, flags: SJCL\n  Incoming interface: LISP0.4101, RPF nbr 192.168.10.68, LISP: [192.168.10.68, 232.0.1.180]\n  Outgoing interface list:\n    Loopback4101, Forward/Sparse, 00:02:09/00:02:57\n\n(192.168.1.18, 224.0.1.40), 00:00:02/00:02:57, flags: LJT\n  Incoming interface: LISP0.4101, RPF nbr 192.168.10.68, LISP: [192.168.10.68, 232.0.1.180]\n  Outgoing interface list:\n    Loopback4101, Forward/Sparse, 00:00:02/00:02:57\n</code></pre> <p>The RPF neighbor points to Border1 and Edge1 (and Edge2) sends a (S,G) join in the underlay towards Border1, building this SSM tree. On Border1, we see the correct state, allowing for the auto-RP messages to flow to the edges. </p> <pre><code>Border1#show ip mroute 232.0.1.180 verbose\nIP Multicast Routing Table\nFlags: D - Dense, S - Sparse, B - Bidir Group, s - SSM Group, C - Connected,\n       L - Local, P - Pruned, R - RP-bit set, F - Register flag,\n       T - SPT-bit set, J - Join SPT, M - MSDP created entry, E - Extranet,\n       X - Proxy Join Timer Running, A - Candidate for MSDP Advertisement,\n       U - URD, I - Received Source Specific Host Report, \n       Z - Multicast Tunnel, z - MDT-data group sender, \n       Y - Joined MDT-data group, y - Sending to MDT-data group, \n       G - Received BGP C-Mroute, g - Sent BGP C-Mroute, \n       N - Received BGP Shared-Tree Prune, n - BGP C-Mroute suppressed, \n       Q - Received BGP S-A Route, q - Sent BGP S-A Route, \n       V - RD &amp; Vector, v - Vector, p - PIM Joins on route, \n       x - VxLAN group, c - PFP-SA cache created entry, \n       * - determined by Assert\nOutgoing interface flags: H - Hardware switched, A - Assert winner, p - PIM Join\n Timers: Uptime/Expires\n Interface state: Interface, Next-Hop or VCD, State/Mode\n\n(192.168.10.68, 232.0.1.180), 13:28:09/00:03:08, flags: sTp\n  Incoming interface: Null0, RPF nbr 0.0.0.0\n  Outgoing interface list:\n    TenGigabitEthernet1/1/1, Forward/Sparse, 00:02:32/00:03:06, Pkts:0, p\n    TenGigabitEthernet1/1/2, Forward/Sparse, 13:15:29/00:03:08, Pkts:0, p\n</code></pre> <p>A packet capture confirms that the Auto-RP messages are now flowing via the overlay, encapsulated with a VXLAN header:</p> <p></p> <p>There you have it - Auto-RP in SDA isn't really going to work unless you statically configure the RP as well (which defeats the purpose of Auto-RP, doesn't it?). I do feel that the implementation for BSR and Auto-RP groups should be changed - 224.0.1.13, 224.0.1.39 and 224.0.1.40 should be a \"special\" case and we should use L2 Flooding to flood these packets in the underlay ASM group, pre-built for L2 Flooding (which will get these messages to all edges). </p> <p>I hope this was informative and as always, I'd like to thank you for reading, if you've come this far. </p>","tags":["sda","lisp"]},{"location":"blog/2021/12/26/cisco-sda-part-ii---basic-lisp-configuration-and-operation/","title":"Cisco SDA Part II - basic LISP configuration and operation","text":"<p>In this post, we introduce basic LISP configuration and operation using packet captures and packet walks.</p>","tags":["sda","lisp"]},{"location":"blog/2021/12/26/cisco-sda-part-ii---basic-lisp-configuration-and-operation/#introduction-and-topology","title":"Introduction and topology","text":"<p>Now that we've covered this new routing paradigm that LISP introduces in the previous post and understood some commonly used terms, we will move on to basic LISP operations.  </p> <p>The topology that we will be using is the following:</p> <p></p> <p>Some generic details about the setup - R1 and R5 have a loopback each (loopback0) with IP addresses of 1.1.1.1/24 and 5.5.5.5/24 respectively. There is an OSPF adjacency between R1 and xTR2 as well as R5 and xTR4. Loopback0 is advertised into OSPF by both R1 and R5. xTR2 and xTR4 learn these respective loopbacks.</p> <pre><code>// R1\n\nR1#show run int lo0\nBuilding configuration...\n\nCurrent configuration : 93 bytes\n!\ninterface Loopback0\n ip address 1.1.1.1 255.255.255.0\n ip ospf network point-to-point\nend\n\nR1#show ip ospf neighbor \n\nNeighbor ID     Pri   State           Dead Time   Address         Interface\n2.2.2.2           0   FULL/  -        00:00:37    10.1.12.2       GigabitEthernet0/0\n\n\n// R5\n\nR5#show run int lo0\nBuilding configuration...\n\nCurrent configuration : 93 bytes\n!\ninterface Loopback0\n ip address 5.5.5.5 255.255.255.0\n ip ospf network point-to-point\nend\n\nR5#show ip ospf neighbor \n\nNeighbor ID     Pri   State           Dead Time   Address         Interface\n4.4.4.4           0   FULL/  -        00:00:36    10.1.45.4       GigabitEthernet0/0\n</code></pre> <p>xTR2, xTR4 and the MS/MR have a /32 loopback each:</p> <pre><code>// xTR2\n\nxTR2#show run int lo0\nBuilding configuration...\n\nCurrent configuration : 63 bytes\n!\ninterface Loopback0\n ip address 2.2.2.2 255.255.255.255\nend\n\n// MS/MR\n\nMS_MR#show run int l0\nBuilding configuration...\n\nCurrent configuration : 63 bytes\n!\ninterface Loopback0\n ip address 3.3.3.3 255.255.255.255\nend\n\n// xTR4\n\nxTR4#show run int lo0\nBuilding configuration...\n\nCurrent configuration : 63 bytes\n!\ninterface Loopback0\n ip address 4.4.4.4 255.255.255.255\nend\n</code></pre> <p>The LISP cloud has some form of underlay (we don't need to go into details of that) that allows for these loopbacks to talk to each other and have complete reachability between them.  </p> <pre><code>// reachability to MS/MRs loopback    \n\nxTR2#ping 3.3.3.3 source 2.2.2.2\nType escape sequence to abort.\nSending 5, 100-byte ICMP Echos to 3.3.3.3, timeout is 2 seconds:\nPacket sent with a source address of 2.2.2.2 \n!!!!!\nSuccess rate is 100 percent (5/5), round-trip min/avg/max = 3/4/6 ms\n\n// reachability to R4s loopback   \n\nxTR2#ping 4.4.4.4 source 2.2.2.2\nType escape sequence to abort.\nSending 5, 100-byte ICMP Echos to 4.4.4.4, timeout is 2 seconds:\nPacket sent with a source address of 2.2.2.2 \n!!!!!\nSuccess rate is 100 percent (5/5), round-trip min/avg/max = 4/4/5 ms\n</code></pre> <p>Apart from basic IP addressing between each device (and details described earlier), there is nothing else that has been pre-configured on these devices.</p>","tags":["sda","lisp"]},{"location":"blog/2021/12/26/cisco-sda-part-ii---basic-lisp-configuration-and-operation/#lisp-msmr-configuration","title":"LISP MS/MR configuration","text":"<p>Let's go ahead and start configuring each device role in the topology now. We are going to start with the MS/MR. </p> <pre><code>// instantiate the LISP process \n\nMS_MR(config)#router lisp\n\n// mapping sites to prefixes that can be accepted for that site\n\nMS_MR(config-router-lisp)#site SITE_A\nMS_MR(config-router-lisp-site)#authentication-key cisco\nMS_MR(config-router-lisp-site)#eid-prefix 1.1.1.0/24\nMS_MR(config-router-lisp-site)#exit\n\nMS_MR(config-router-lisp)#site SITE_B \nMS_MR(config-router-lisp-site)#authentication-key cisco\nMS_MR(config-router-lisp-site)#eid-prefix 5.5.5.0/24\nMS_MR(config-router-lisp-site)#exit\n\n// configuring this device to be a MS/MR\n\nMS_MR(config-router-lisp)#ipv4 map-server \nMS_MR(config-router-lisp)#ipv4 map-resolver \n</code></pre> <p>On the MS/MR, outside of configuring it as a Map Server and Map Resolver, you also need to define what prefixes it is allowed to accept and store in its EID to RLOC mapping database and against what site. Next, configure the xTRs. This is where you setup the EID advertisement and point to the MS/MR. </p> <pre><code>// instantiate LISP\n// define table and instance ID\n\nxTR2(config)#router lisp\nxTR2(config-router-lisp)#eid-table default instance-id 0\n\n// advertise EIDs\n\nxTR2(config-router-lisp-eid-table)#database-mapping 1.1.1.0/24 2.2.2.2 priority 1 weight 50       \nxTR2(config-router-lisp-eid-table)#exit\n\n// configure device role to be xTR (both iTR and eTR)\n// additionally, specify IP address of MS/MR\n\nxTR2(config-router-lisp)# ipv4 itr map-resolver 3.3.3.3\nxTR2(config-router-lisp)# ipv4 itr\nxTR2(config-router-lisp)# ipv4 etr map-server 3.3.3.3 key cisco\nxTR2(config-router-lisp)# ipv4 etr\nxTR2(config-router-lisp)# exit\n</code></pre> <p>Similar configuration for xTR4, only the database-mapping line changes a bit:</p> <pre><code>// instantiate LISP\n// define table and instance ID\n\nxTR4(config)#router lisp\nxTR4(config-router-lisp)#eid-table default instance-id 0\n\n// advertise EIDs\n\nxTR4(config-router-lisp-eid-table)#database-mapping 5.5.5.0/24 4.4.4.4 priority 1 weight 50       \nxTR4(config-router-lisp-eid-table)#exit\n\n// configure device role to be xTR (both iTR and eTR)\n// additionally, specify IP address of MS/MR\n\nxTR4(config-router-lisp)# ipv4 itr map-resolver 3.3.3.3\nxTR4(config-router-lisp)# ipv4 itr\nxTR4(config-router-lisp)# ipv4 etr map-server 3.3.3.3 key cisco\nxTR4(config-router-lisp)# ipv4 etr\nxTR4(config-router-lisp)# exit\n</code></pre> <p>Take a look at the database-mapping statement closely - what it does is advertise the EID (which is in red) along with the RLOC (which is in yellow) to the MS/MR (we are setting the RLOCs to be the loopback0 IP address). The MS/MR now stores this in its EID to RLOC mapping database.  </p> <p>Now that we've configured the xTRs and the MS/MR, let's take a look at what is really going on behind the scenes. When you configure the xTRs and advertise an EID to RLOC mapping, it sends a Map Register to the MS/MR. </p> <p></p> <p>The Map Register packet looks like this:</p> <p></p> <p>From the Map Register, the MS/MR pulls out relevant information to populate its EID to RLOC mapping database. Additionally, MS/MR sends a Map Notify back (only if the 'M' bit is set in the Map Register which essentially means it expects to get a Map Notify back). </p> <p></p> <p>The Map Notify packet looks like this:</p> <p></p> <p>This process ends here. The MS/MR does not 'push' this information to anyone else. From a xTR perspective, there are two important tables that you should be looking at - the LISP database table and the LISP map-cache. The LISP database table is where it stores the locally attached EIDs that are being advertised to the MS/MR and the LISP map-cache is what is actively used to build the LISP forwarding/data-plane. </p>","tags":["sda","lisp"]},{"location":"blog/2021/12/26/cisco-sda-part-ii---basic-lisp-configuration-and-operation/#lisp-database-table","title":"LISP database table","text":"<pre><code>// xTR2s LISP database table\n\nxTR2#show ip lisp database \nLISP ETR IPv4 Mapping Database for EID-table default (IID 0), LSBs: 0x1, 1 entries\n\n1.1.1.0/24\n  Locator  Pri/Wgt  Source     State\n  2.2.2.2    1/50   cfg-addr   site-self, reachable\n\n// xTR4s LISP database table\n\nxTR4#show ip lisp database \nLISP ETR IPv4 Mapping Database for EID-table default (IID 0), LSBs: 0x1, 1 entries\n\n5.5.5.0/24\n  Locator  Pri/Wgt  Source     State\n  4.4.4.4    1/50   cfg-addr   site-self, reachable\n</code></pre> <p>It is important to understand how the database determines if an EID is 'reachable' or not - this is not a data packet driven derivative. There is no active reachability check that is done against this EID. It is purely from the existence of the prefix in RIB. </p> <p>For example, I shut down R1s interface to xTR2. With an intermediate dumb device between R1 and xTR2, xTR2s interface stays up despite R1 being down. Eventually, this causes OSPF peering to go down and 1.1.1.0/24 to be pulled from RIB. At this point, the state of this EID in the LISP database is moved to unreachable: </p> <pre><code>// OSPF peering goes down\n\n%OSPF-5-ADJCHG: Process 1, Nbr 1.1.1.1 on GigabitEthernet0/0 from FULL to DOWN, Neighbor Down: Dead timer expired \n\n// prefix pulled from RIB\n\nxTR2#show ip route 1.1.1.1\n% Network not in table \n\n// EID marked as unreachable\n// Map-Register sent to MS/MR with EID as unreachable\n\nxTR2#show ip lisp database \nLISP ETR IPv4 Mapping Database for EID-table default (IID 0), LSBs: 0x0, 1 entries\n\n*** ALL CONFIGURED LOCAL EID PREFIXES HAVE NO ROUTE ***\n***      REPORTING LOCAL RLOCS AS UNREACHABLE       ***\n\n1.1.1.0/24 *** NO ROUTE TO EID PREFIX ***\n  Locator  Pri/Wgt  Source     State\n  2.2.2.2    1/50   cfg-addr   site-self, unreachable \n</code></pre> <p>I can now configure a static route towards that subnet and it becomes reachable again, thus proving what we had theorized earlier:</p> <pre><code>// configuring a static route to 1.1.1.0/24 on xTR2\n\nxTR2(config)#ip route 1.1.1.0 255.255.255.0 10.1.12.1\nxTR2(config)#end\n\n// xTR2s LISP database\n\nxTR2#show ip lisp database \nLISP ETR IPv4 Mapping Database for EID-table default (IID 0), LSBs: 0x1, 1 entries\n\n1.1.1.0/24\n  Locator  Pri/Wgt  Source     State\n  2.2.2.2    1/50   cfg-addr   site-self, reachable\n</code></pre>","tags":["sda","lisp"]},{"location":"blog/2021/12/26/cisco-sda-part-ii---basic-lisp-configuration-and-operation/#lisp-map-cache","title":"LISP map-cache","text":"<p>Remember how we talked about LISP being a pull model? Well, at this point, with just the EID registrations being done with the MS/MR, the xTRs have no knowledge of each others EIDs in their map-caches (which is used for LISP data-plane). </p> <p>Thus, at time=0 (where time=0 is the point when xTRs have just registered EIDs with the MS/MR and no data-plane traffic is flowing right now) the map-cache is empty except for one, very important, default entry.</p> <pre><code>// XTR2s LISP map-cache\n\nxTR2#show ip lisp map-cache       \nLISP IPv4 Mapping Cache for EID-table default (IID 0), 1 entries\n\n0.0.0.0/0, uptime: 00:10:02, expires: never, via static send map-request\n  Negative cache entry, action: send-map-request  \n\n// xTR4s LISP map-cache\n\nxTR4#show ip lisp map-cache \nLISP IPv4 Mapping Cache for EID-table default (IID 0), 1 entries\n\n0.0.0.0/0, uptime: 00:05:45, expires: never, via static send map-request\n  Negative cache entry, action: send-map-request\n</code></pre> <p>This map-cache entry triggers a 'Map Request' to the MS/MR (this is the 'pull' that forms the basis of the LISP model), essentially requesting for information about the EID subnet it is trying to reach. The MS/MR looks at its site database for the EID. If it finds it, it forwards the Map Request to the RLOC that registered the EID. This RLOC now responds to the source RLOC and the source RLOC installs the EID to RLOC mapping (information gleaned from the Map Reply) in its map-cache. This new entry is now used for data-plane forwarding.  </p>","tags":["sda","lisp"]},{"location":"blog/2021/12/26/cisco-sda-part-ii---basic-lisp-configuration-and-operation/#lisp-process-end-to-end","title":"LISP process - end to end","text":"<p>Now that we have a general understanding of the tables in use, let's look at an entire flow, end to end by having R1 ping R5s loopback (5.5.5.5) sourcing its own loopback (1.1.1.1). R1s default gateway is xTR2, so after determining that the destination is not in its subnet, R1 ARPs for its default gateway (xTR2), resolves that and sends the packet to xTR2.</p> <p></p> <p>xTR2 looks at the destination mac, realize it owns it and strips the L2 header. It then does a lookup against the destination IP address in the IP header. This is where some of the magic begins. While RIB shows no route to the destination, look closely at the CEF entry:</p> <pre><code>// RIB lookup on xTR2\n\nxTR2#show ip route 5.5.5.5\n% Network not in table\n\n// CEF lookup on xTR2\n\nxTR2#show ip cef 5.5.5.5 detail\n0.0.0.0/0, epoch 0, flags [default route handler, cover dependents, check lisp eligibility, default route]\n  LISP remote EID: 0 packets 0 bytes fwd action signal, cfg as EID space\n  LISP source path list\n    attached to LISP0\n  Covered dependent prefixes: 1\n    notify cover updated: 1\n  1 IPL source [unresolved]\n  no route\n</code></pre> <p>While the route is still a 'no route' entry, it has appropriate actions set to leak the packet to the CPU and into the LISP process. LISP now processes the packet by looking at its map-cache table:</p> <pre><code>xTR2#show ip lisp map-cache 5.5.5.5\nLISP IPv4 Mapping Cache for EID-table default (IID 0), 1 entries\n\n0.0.0.0/0, uptime: 00:47:34, expires: never, via static send map-request\n  Sources: static send map-request\n  State: send-map-request, last modified: 00:47:34, map-source: local\n  Idle, Packets out: 0(0 bytes)\n  Configured as EID address space\n  Negative cache entry, action: send-map-request \n</code></pre> <p>A lookup in the LISP map-caches results in a hit against the 0.0.0.0/0 entry that LISP installs by default. This is a catch-all entry that never expires and its sole purpose is to trigger a Map Request to the MS/MR. </p> <p>Having hit this entry, the LISP process on xTR2 now generates a Map Request for EID 5.5.5.5 and sends that to the MS/MR. </p> <p></p> <p>The Map Request packet looks like this:</p> <p></p> <p>The MS/MR looks at its site database to determine if any RLOC has registered this EID:</p> <pre><code>MS_MR#show lisp site 5.5.5.5\nLISP Site Registration Information\n\nSite name: SITE_B\nAllowed configured locators: any\nRequested EID-prefix:\n  EID-prefix: 5.5.5.0/24 \n    First registered:     00:01:09\n    Last registered:      00:00:09\n    Routing table tag:    0\n    Origin:               Configuration\n    Merge active:         No\n    Proxy reply:          No\n    TTL:                  1d00h\n    State:                complete\n    Registration errors:  \n      Authentication failures:   0\n      Allowed locators mismatch: 0\n    ETR 4.4.4.4, last registered 00:00:09, no proxy-reply, map-notify\n                 TTL 1d00h, no merge, hash-function sha1, nonce 0x7BE12FC9-0xAF5DF791\n                 state complete, no security-capability\n                 xTR-ID 0xDB225608-0x437E7B02-0xCC1C81E3-0x1E9A0224\n                 site-ID unspecified\n      Locator  Local  State      Pri/Wgt  Scope\n      4.4.4.4  yes    up           1/50   IPv4 none\n</code></pre> <p>It finds an active entry for this against SITE_B, with a RLOC of 4.4.4.4. However, notice one of the flags that is set against the eTR - no proxy-reply. This means that the MS/MR cannot proxy for this EID and it must forward the Map Request to the RLOC that registered this EID - 4.4.4.4 in this case. </p> <p>That is what it does next:</p> <p></p> <p>When xTR4 gets this, it looks at its database. It is the authoritative owner for this EID so it sends a Map Reply back to the iTR RLOC, 2.2.2.2 in this case.</p> <p></p> <p>The Map Reply packet looks like this:</p> <p></p> <p>xTR2 processes the Map Reply and inserts a new, more specific (when compared to 0.0.0.0/0) entry for the EID in its LISP map-cache:</p> <pre><code>xTR2#show ip lisp map-cache \nLISP IPv4 Mapping Cache for EID-table default (IID 0), 2 entries\n\n0.0.0.0/0, uptime: 01:30:31, expires: never, via static send map-request\n  Negative cache entry, action: send-map-request\n5.5.5.0/24, uptime: 00:00:10, expires: 23:59:49, via map-reply, complete\n  Locator  Uptime    State      Pri/Wgt\n  4.4.4.4  00:00:10  up           1/50 \n\n\nxTR2#show ip lisp map-cache 5.5.5.5\nLISP IPv4 Mapping Cache for EID-table default (IID 0), 2 entries\n\n5.5.5.0/24, uptime: 00:00:14, expires: 23:59:45, via map-reply, complete\n  Sources: map-reply\n  State: complete, last modified: 00:00:14, map-source: 4.4.4.4\n  Active, Packets out: 0(0 bytes)\n  Locator  Uptime    State      Pri/Wgt\n  4.4.4.4  00:00:14  up           1/50 \n    Last up-down state change:         00:00:14, state change count: 1\n    Last route reachability change:    00:00:14, state change count: 1\n    Last priority / weight change:     never/never\n    RLOC-probing loc-status algorithm:\n      Last RLOC-probe sent:            never\n</code></pre> <p>A lot of good information here - you can see the EID that has been added to the map-cache, along with its RLOC. Outside of this, the source of this was a Map Reply and this entry is valid for the next 23 hours, 59 minutes and 45 seconds. </p> <p>Where does this expiry time come from? This is actually specified in the Map Reply itself - it is the 'Record TTL' field. Look closely at the packet - this field has a value of 1440 (in minutes), which is 24 hours.</p> <p>We're not done yet - once the map-cache has been built, it is pushed down to CEF as well:</p> <pre><code>xTR2#show ip cef 5.5.5.5 detail\n5.5.5.0/24, epoch 0, flags [default route handler, subtree context, check lisp eligibility, default route]\n  SC owned,sourced: LISP remote EID - locator status bits 0x00000001\n  LISP remote EID: 4 packets 400 bytes fwd action encap\n  LISP source path list\n    nexthop 4.4.4.4 LISP0\n  2 IPL sources [unresolved, active source]\n    Dependent covered prefix type inherit, cover 0.0.0.0/0\n  recursive via 0.0.0.0/0\n    no route\n</code></pre> <p>Remember that earlier, we were hitting the 0.0.0.0/0 entry for the same prefix. Now, a more specific entry has been added for 5.5.5.0/24; subsequent packets hit this instead and do not require any further map requests to be generated. This entry forces a LISP encapsulation of the packet with a next-hop of 4.4.4.4 as you can see from the output above. </p> <p>This encapsulated packet is sent towards the MS/MR, since recursively, that is the next hop for 4.4.4.4.</p> <p></p> <p>The encapsulated packet looks like this:</p> <p></p> <p>Note</p> <p>Look at the source IP address in the outer IP header. It is not 2.2.2.2. This is because the LISP process populates the outer source IP address with the IP address of the egress interface. To change this behavior (and make it more consistent/predictable), configure the command 'ip lisp source-locator ' on each egress LISP interface ** <p>At this point, regular routing takes over (the destination IP address in the outer IP header carries the packet to the destination RLOC) and the packet is forwarded to xTR4. </p> <p></p> <p>The destination mac and the destination IP address in the outer IP header is owned by xTR4 itself (4.4.4.4), so it strips off the outer L2 and L3 headers (decapsulates the packet). It parses through the UDP and LISP headers and then does a route lookup against the inner destination IP address. The packet is then forwarded based on this lookup, towards R5.</p> <pre><code>xTR4#show ip route 5.5.5.5\nRouting entry for 5.5.5.0/24\n  Known via \"ospf 1\", distance 110, metric 2, type intra area\n  Last update from 10.1.45.5 on GigabitEthernet0/0, 02:16:52 ago\n  Routing Descriptor Blocks:\n  * 10.1.45.5, from 5.5.5.5, 02:16:52 ago, via GigabitEthernet0/0\n      Route metric is 2, traffic share count is 1\n\nxTR4#show ip cef 5.5.5.5\n5.5.5.0/24\n  nexthop 10.1.45.5 GigabitEthernet0/0\n</code></pre> <p></p> <p>This entire process now happens in the reverse direction, starting from R5. Result? We have reachability between R1s and R5s loopbacks.</p> <pre><code>R1#ping 5.5.5.5 source 1.1.1.1\nType escape sequence to abort.\nSending 5, 100-byte ICMP Echos to 5.5.5.5, timeout is 2 seconds:\nPacket sent with a source address of 1.1.1.1 \n..!!!\nSuccess rate is 60 percent (3/5), round-trip min/avg/max = 7/7/9 ms\n</code></pre> <p>Why were two pings lost? One ICMP timeout is how long it took for one map resolution to be completed - this needs to be done in both directions, thus two ICMP requests did not get any responses back.</p> <p>This concludes a very long post. See you in the next one!</p>","tags":["sda","lisp"]},{"location":"blog/2021/12/27/cisco-sda-part-iii---lisp-and-non-lisp-sites/","title":"Cisco SDA Part III - LISP and non-LISP sites","text":"<p>In this post, we look at how a LISP site talks to non-LISP sites.</p>","tags":["sda","lisp"]},{"location":"blog/2021/12/27/cisco-sda-part-iii---lisp-and-non-lisp-sites/#introduction-and-topology","title":"Introduction and topology","text":"<p>Understanding how a LISP site talks to a non-LISP site (and vice versa) is very crucial to LISP and the bigger picture that we're building towards - SDA. </p> <p>The topology that we'll work with is a slightly modified version of what we had before - another router has been added that will facilitate conversation between LISP and non-LISP:</p> <p></p> <p>This new router, xTR6, connects to another router, R7 which in turn can reach the Internet, where 4.2.2.2 is. R7 is sending a default route to xTR6. The focus of this post is to understand how R1s loopback, 1.1.1.1 (which is in the LISP space) can talk to 4.2.2.2 (which is in the non-LISP space).</p> <p>To facilitate such a conversation, two additional functionalities were added into LISP - proxy iTR (PiTR) and proxy eTR (PeTR). When a device performs both functions, it is called as a PxTR. Thus, xTR6 in this topology, is a PxTR.</p> <p>PiTR - this box is responsible for being a proxy for EIDs in the LISP space and advertising these EID spaces (via some form of aggregation) into the non-LISP space. The intention of this is to draw traffic for the LISP space towards itself.  PeTR - this box connects the LISP space to non LISP space. It receives encapsulated traffic, decapsulates it and forwards it natively towards non-LISP sites.</p> <p>To understand how this works, we will look at each direction individually - LISP to non-LISP and non-LISP to LISP.</p>","tags":["sda","lisp"]},{"location":"blog/2021/12/27/cisco-sda-part-iii---lisp-and-non-lisp-sites/#lisp-to-non-lisp","title":"LISP to non-LISP","text":"<p>Configuration for both of these functionalities (PiTR and PeTR) is fairly simple. Let's start off by having xTR6 be a normal xTR. It has the following configuration to begin with:</p> <pre><code>xTR6#show run | sec router lisp\nrouter lisp\n eid-table default instance-id 0\n  exit\n !\n ipv4 itr map-resolver 3.3.3.3\n ipv4 itr\n ipv4 etr map-server 3.3.3.3 key cisco\n ipv4 etr\n exit\n</code></pre> <p>To enable PeTR functionality, simply use 'ipv4 proxy-etr'.</p> <pre><code>xTR6(config)#router lisp\nxTR6(config-router-lisp)#ipv4 proxy-etr\nxTR6(config-router-lisp)#end\n</code></pre> <p>xTR6 is now configured to decapsulate LISP encapsulated packets and forward them natively. But how would it receive traffic in the first place, when our destination (4.2.2.2) is not even advertised in the LISP space (there are no database mappings configured for this)? This is where LISP starts to become really cool!</p> <p>Let's go step by step - R1 sends an ICMP request to xTR2. It hits the 0.0.0.0/0 entry in CEF and subsequently leaks to the LISP process, where it hits the 0.0.0.0/0 entry again (which we should be very familiar with by now). This triggers LISP to generate a map request for 4.2.2.2.</p> <pre><code>// xTR2s 0.0.0.0/0 CEF entry hit\n\nxTR2#show ip cef 4.2.2.2 detail\n0.0.0.0/0, epoch 0, flags [default route handler, cover dependents, check lisp eligibility, default route]\n  LISP remote EID: 6 packets 600 bytes fwd action signal, cfg as EID space\n  LISP source path list\n    attached to LISP0\n  Covered dependent prefixes: 2\n    notify cover updated: 2\n  1 IPL source [unresolved]\n  no route\n\n// xTR2s 0.0.0.0/0 LISP map-cache entry hit\n\nxTR2#show ip lisp map-cache 4.2.2.2\nLISP IPv4 Mapping Cache for EID-table default (IID 0), 1 entries\n\n0.0.0.0/0, uptime: 00:02:17, expires: never, via static send map-request\n  Sources: static send map-request\n  State: send-map-request, last modified: 00:02:17, map-source: local\n  Idle, Packets out: 0(0 bytes)\n  Configured as EID address space\n  Negative cache entry, action: send-map-request\n</code></pre> <p></p> <p>When the MS/MR receives a map request, it looks at its site database to determine if this has been configured or not and if there are any active RLOCs for this EID.</p> <pre><code>MS_MR#show lisp site 4.2.2.2\n% Could not find instance-id 0 EID 4.2.2.2 in site database.\n</code></pre> <p>If there is an EID miss (which is what happens in this case, as you can see above), it is required to send a negative map reply back with certain action bits set (remember, there are 3 action bits available). One of these action codes is natively-forward, which is what the MS/MR uses here.</p> <p></p> <p>The negative map reply looks like this:</p> <p></p> <p>Notice how the EID in the map reply is not 4.2.2.2/32. Per RFC (RFC 6833), the map resolver \"should return the least-specific prefix that both matches the original query and does not match any EID-Prefix known to exist in the LISP-capable infrastructure\" in order to minimize the number of negative cache entries needed. </p> <p>When xTR2 processes this map reply, it installs this into the LISP map-cache. From here, it gets pushed into the CEF table as well.</p> <pre><code>xTR2#show ip lisp map-cache \nLISP IPv4 Mapping Cache for EID-table default (IID 0), 3 entries\n\n0.0.0.0/0, uptime: 12:04:44, expires: never, via static send map-request\n  Negative cache entry, action: send-map-request\n4.0.0.0/8, uptime: 00:14:57, expires: 00:14:54, via map-reply, forward-native\n  Encapsulating to proxy ETR\n5.5.5.0/24, uptime: 11:20:41, expires: 12:41:55, via map-reply, complete\n  Locator  Uptime    State      Pri/Wgt\n  4.4.4.4  11:18:04  up           1/50\n\nxTR2#show ip cef 4.2.2.2 detail\n4.0.0.0/8, epoch 0, flags [default route handler, subtree context, check lisp eligibility, default route]\n  SC owned,sourced: LISP remote EID - locator status bits 0x00000000\n  LISP remote EID: 4 packets 400 bytes fwd action fwd native via petr\n  LISP source path list\n    nexthop 6.6.6.6 LISP0\n  2 IPL sources [unresolved, active source]\n    Dependent covered prefix type inherit, cover 0.0.0.0/0\n  recursive via 0.0.0.0/0\n    no route \n</code></pre> <p>Whoa - why does this entry say that it needs to be encapsulated to a 'proxy eTR'? I need to pull back the curtain a little bit now - prior to this, I had configured an additional command on xTR2 which wasn't show earlier:</p> <pre><code>xTR2(config)#router lisp\nxTR2(config-router-lisp)#ipv4 use-petr 6.6.6.6\nxTR2(config-router-lisp)#end\n</code></pre> <p>This allows all negative cache entries with forward-native set to be sent to a particular IP address (as configured in the command). In our case, this IP address is 6.6.6.6, which connects to the non-LISP site. This IP address, in general, is typically set to point to your PeTR.</p> <p>So, xTR2 can now encapsulate subsequent packets with a destination IP address of 6.6.6.6 (in the outer IP header) and forward it on.</p> <p></p> <p>xTR6 receives this, decapsulates it and does another CEF lookup on the inner IP header. This hits the 0.0.0.0/0 entry which leaks the packet to LISP, where it hits the 0.0.0.0/0 entry again, causing a map request to be sent to the MS/MR for the destination IP address, 4.2.2.2:</p> <pre><code>// xTR6s 0.0.0.0/0 CEF entry hit\n\nxTR6#show ip cef 4.2.2.2 detail\n0.0.0.0/0, epoch 0, flags [cover dependents, check lisp eligibility, default route]\n  LISP remote EID: 1 packets 0 bytes fwd action signal, cfg as EID space\n  LISP source path list\n    attached to LISP0\n  Covered dependent prefixes: 1\n    notify cover updated: 1\n  1 IPL source [no flags]\n  attached to LISP0\n\n// xTR6s 0.0.0.0/0 LISP map-cache entry hit\n\nxTR6#show ip lisp map-cache 4.2.2.2\nLISP IPv4 Mapping Cache for EID-table default (IID 0), 2 entries\n\n0.0.0.0/0, uptime: 00:31:20, expires: never, via static send map-request\n  Sources: static send map-request\n  State: send-map-request, last modified: 00:31:20, map-source: local\n  Idle, Packets out: 1(100 bytes) (~ 00:20:19 ago)\n  Configured as EID address space\n  Negative cache entry, action: send-map-request\n</code></pre> <p>Note</p> <p>I did have to enable PiTR functionality for this part to work. Without this, we run into other problems (such as routing loops) which I get to later in the post **</p> <p>This can be visualized as below:</p> <p></p> <p>Again, the MS/MR responds back with a negative map reply and the native forward bit set, since it does not have any EIDs that match 4.2.2.2 in its site database:</p> <p></p> <p>xTR6 processes this map reply (remember that the MS/MR responds back with the least specific prefix, which in this case was 4.0.0.0/8) and gleans that this prefix needs to be natively forwarded. The next hop now gets picked up from the best match in RIB/FIB (which is the next hop for the default route received from R7). </p> <pre><code>// xTR6 sends a map request to the MS/MR for 4.2.2.2/32\n\n*Jun  7 00:57:19.419: LISP: Send map request for EID prefix IID 0 4.2.2.2/32\n*Jun  7 00:57:19.419: LISP-0: Remote EID IID 0 prefix 4.2.2.2/32, Send map request (1) (sources: &lt;signal&gt;, state: incomplete, rlocs: 0).\n*Jun  7 00:57:19.419: LISP-0: EID-AF IPv4, Sending map-request from 4.2.2.2 to 4.2.2.2 for EID 4.2.2.2/32, ITR-RLOCs 1, nonce 0x7798929C-0x4F5ECE13 (encap src 10.1.36.6, dst 3.3.3.3), FromPITR.\n\n// xTR6 starts processing map reply\n\n*Jun  7 00:57:19.424: LISP: Processing received Map-Reply(2) message on GigabitEthernet0/0 from 10.1.36.3:4342 to 6.6.6.6:4342\n*Jun  7 00:57:19.425: LISP: Received map reply nonce 0x7798929C-0x4F5ECE13, records 1\n*Jun  7 00:57:19.425: LISP: Processing Map-Reply mapping record for IID 0 4.0.0.0/8, ttl 15, action forward, authoritative, 0 locators\n*Jun  7 00:57:19.425: LISP-0: Map Request IID 0 prefix 4.2.2.2/32 remote EID prefix[LL], Received reply with rtt 6ms.\n*Jun  7 00:57:19.425: LISP: Processing mapping information for EID prefix IID 0 4.0.0.0/8\n*Jun  7 00:57:19.426: LISP-0: AF IID 0 IPv4, Persistent db: ignore writing request, disabled.\n*Jun  7 00:57:19.426: LISP-0: Remote EID IID 0 prefix 4.0.0.0/8, Change state to forward-native (sources: map-rep, state: unknown, rlocs: 0).\n*Jun  7 00:57:19.426: LISP-0: Remote EID IID 0 prefix 4.0.0.0/8, Starting idle timer (delay 00:02:30) (sources: map-rep, state: forward-native, rlocs: 0).\n*Jun  7 00:57:19.426: LISP-0: Remote EID IID 0 prefix 4.2.2.2/32, Change state to deleted (sources: &lt;&gt;, state: incomplete, rlocs: 0).\n*Jun  7 00:57:19.427: LISP-0: Remote EID IID 0 prefix 4.2.2.2/32, Map-reply from 10.1.36.3 returned less specific 4.0.0.0/8 (sources: &lt;&gt;, state: deleted, rlocs: 0).\n\n// next-hop pulled from an internal CEF walk\n\n*Jun  7 00:57:19.427: LISPreid: Default:4.0.0.0/8 Add, action fwd native, lsb 0x0\n*Jun  7 00:57:19.427: LISPreid: Default:0.0.0.0/0 Null modify of pco 0xF296E08 linked to glean for LISP0\n*Jun  7 00:57:19.428: LISPreid: Default:4.0.0.0/8 Added LISP IPL src, ok\n*Jun  7 00:57:19.428: LISPreid: Default:4.0.0.0/8 Created pco 0x1064C550 linked to IP adj out of GigabitEthernet0/1, addr 10.1.67.7 0F8B85F8\n*Jun  7 00:57:19.429: LISPreid: Default:0.0.0.0/0 Null modify of pco 0xF296E08 linked to glean for LISP0\n*Jun  7 00:57:19.429: LISPreid: Default:4.0.0.0/8 Added LISP_FWD_NATIVE src, success\n*Jun  7 00:57:19.430: LISPreid: Default:4.2.2.2/32 Remove, action not specd, lsb 0x0, match found\n*Jun  7 00:57:19.430: LISPreid: Default:4.0.0.0/8 Null modify of pco 0x1064C550 linked to IP adj out of GigabitEthernet0/1, addr 10.1.67.7 0F8B85F8\n*Jun  7 00:57:19.430: LISPreid: Default:4.2.2.2/32 Removed LISP src, success\n*Jun  7 00:57:19.431: LISPreid: Default:4.2.2.2/32 Removing LISP IPL src\n*Jun  7 00:57:19.438: LISPreid: Default:4.0.0.0/8 Null modify of pco 0x1064C550 linked to IP adj out of GigabitEthernet0/1, addr 10.1.67.7 0F8B85F8\n*Jun  7 00:57:19.439: LISPreid: Default:4.2.2.2/32 Removed LISP subtree, success\n</code></pre> <p>The end result?</p> <pre><code>xTR6#show ip cef 4.2.2.2 detail\n4.0.0.0/8, epoch 0, flags [check lisp eligibility]\n  LISP remote EID: 3 packets 300 bytes fwd action fwd native\n  LISP fwd-native source\n    Dependent covered prefix type LISP-FWD, cover 0.0.0.0/0\n  1 IPL source [no flags]\n  nexthop 10.1.67.7 GigabitEthernet0/1\n</code></pre> <p>Subsequent packets for 4.2.2.2 now hit the above entry and get forwarded to R7: </p> <p></p> <p>This concludes our discussion on how packets get forwarded from a LISP site to a non-LISP site. </p>","tags":["sda","lisp"]},{"location":"blog/2021/12/27/cisco-sda-part-iii---lisp-and-non-lisp-sites/#non-lisp-to-lisp","title":"Non-LISP to LISP","text":"<p>What about the reverse direction though (non-LISP to LISP)? This is where things get even more interesting and a very important rule of LISP is revealed. </p> <p>So, in the reverse path, R7 forwards the ICMP reply from 4.2.2.2 to xTR6. Just like before, xTR6 does a CEF lookup for the destination (1.1.1.1). However, whenever a box is enabled for iTR functionality, it also does a lookup in the local LISP database to check whether the source is a locally known EID or not. If not, the LISP process is not invoked at all. Instead, traditional forwarding would happen without leaking any packets to the LISP process.</p> <p>Currently, xTR6 is not configured to be a PiTR (remember, we had only configured the command for PeTR) - it behaves as an iTR as of now. So, technically, we can potentially create a routing loop here to prove what I just said. What is our setup like? xTR6 is getting a default route from R7 (just like a typical enterprise design):</p> <pre><code>xTR6#show ip cef 1.1.1.1 detail\n0.0.0.0/0, epoch 0, flags [check lisp eligibility, default route]\n  LISP remote EID: 0 packets 0 bytes fwd action signal, cfg as EID space\n  LISP source path list\n    attached to LISP0\n  1 IPL source [no flags]\n  recursive via 10.1.67.7\n    attached to GigabitEthernet0/1\n</code></pre> <p>Does xTR6 have the source in its LISP database as a local EID? No, it does not.</p> <pre><code>xTR6#show ip lisp forwarding eid local 4.2.2.2\nPrefix\n% No local EID prefix in IPv4:Default matching 4.2.2.2\n</code></pre> <p>This means that even though we're hitting the 0.0.0.0/0 entry in CEF and it is associated to LISP0 (so that packets can leak to the LISP process), it will only consider the next-hop from RIB/FIB and not leak packets to LISP. This causes xTR6 to send the packet back to R7. R7 sends it back to xTR6 again and a routing loop ensues. </p> <p></p> <p>To bypass this check and have a device be a proxy for EIDs in the LISP space, you configure it as a PiTR. Additionally, you must remember that as a PiTR, there is no default 0.0.0.0/0 entry in the LISP map-cache (unlike a iTR where this is present by default). </p> <p>This implies that you must create some manual map cache entries. This is done using the 'map-cache' commands. </p> <pre><code>// configure a default catch-all map-cache entry\n\nxTR6(config)#router lisp\nxTR6(config-router-lisp)#eid-table default instance-id 0\nxTR6(config-router-lisp-eid-table)#map-cache 0.0.0.0/0 map-request\nxTR6(config-router-lisp-eid-table)#exit\n\n// note that PiTR cannot be configured until iTR is disabled\n// a box cannot have both roles at the same time\n\nxTR6(config-router-lisp)#ipv4 proxy-itr 6.6.6.6\n% Disable ITR functionality first.\n\nxTR6(config-router-lisp)#no ipv4 itr\n\n// the IP address below acts as a source-locator\n// packets encap'd by this PiTR have a source of 6.6.6.6\n\nxTR6(config-router-lisp)#ipv4 proxy-itr 6.6.6.6\nxTR6(config-router-lisp)#end\n</code></pre> <p>Now, we should be able to reach 4.2.2.2 from R1s loopback, 1.1.1.1:</p> <pre><code>R1#ping 4.2.2.2 source 1.1.1.1\nType escape sequence to abort.\nSending 5, 100-byte ICMP Echos to 4.2.2.2, timeout is 2 seconds:\nPacket sent with a source address of 1.1.1.1 \n.!!!!\nSuccess rate is 80 percent (4/5), round-trip min/avg/max = 5/8/12 ms\n</code></pre> <p>Let's quickly take a look at the entire end to end process in this direction of traffic flow (non-LISP to LISP).</p> <p>R7 sends the ICMP reply from 4.2.2.2 to xTR6 (typically, the PiTR is configured to aggregate its EID spaces and send that to its peer so as to draw traffic towards itself for the LISP space. However, for the sake of simplicity, I have configured a static route on R7 for 1.1.1.0/24 that points to xTR6). </p> <p></p> <p>This reaches xTR6, which is now configured to be a PiTR. It bypasses the source EID lookup, does a CEF lookup, which leaks the packet to LISP, where it hits the 0.0.0.0/0 entry we had manually configured:</p> <pre><code>// xTR6s CEF lookup for 1.1.1.1\n\nxTR6#show ip cef 1.1.1.1 detail\n0.0.0.0/0, epoch 0, flags [check lisp eligibility, default route]\n  LISP remote EID: 0 packets 0 bytes fwd action signal, cfg as EID space\n  LISP source path list\n    attached to LISP0\n  1 IPL source [no flags]\n  attached to LISP0\n\n// xTR6s LISP map-cache lookup for 1.1.1.1  \n\nxTR6#show ip lisp map-cache 1.1.1.1\nLISP IPv4 Mapping Cache for EID-table default (IID 0), 1 entries\n\n0.0.0.0/0, uptime: 00:05:50, expires: never, via static send map-request\n  Sources: static send map-request\n  State: send-map-request, last modified: 00:05:50, map-source: local\n  Idle, Packets out: 0(0 bytes)\n  Configured as EID address space\n  Negative cache entry, action: send-map-request\n</code></pre> <p>This triggers a map request for 1.1.1.1 that is sent to the MS/MR:</p> <p></p> <p>The MS/MR looks at its site database to see if this EID is configured and if any RLOC has registered this. It finds a hit with 2.2.2.2 as the RLOC:</p> <pre><code>MS_MR#show lisp site 1.1.1.1\nLISP Site Registration Information\n\nSite name: SITE_A\nAllowed configured locators: any\nRequested EID-prefix:\n  EID-prefix: 1.1.1.0/24 \n    First registered:     00:03:04\n    Last registered:      00:00:06\n    Routing table tag:    0\n    Origin:               Configuration\n    Merge active:         No\n    Proxy reply:          No\n    TTL:                  1d00h\n    State:                complete\n    Registration errors:  \n      Authentication failures:   0\n      Allowed locators mismatch: 0\n    ETR 2.2.2.2, last registered 00:00:06, no proxy-reply, map-notify\n                 TTL 1d00h, no merge, hash-function sha1, nonce 0xD799F064-0x5BB5D363\n                 state complete, no security-capability\n                 xTR-ID 0xF1FF77A5-0x014680A5-0xA290EB87-0x388BA5CE\n                 site-ID unspecified\n      Locator  Local  State      Pri/Wgt  Scope\n      2.2.2.2  yes    up           1/50   IPv4 none\n</code></pre> <p>It forwards this request to xTR2. </p> <p></p> <p>xTR2, in turn, sends a map reply back to xTR6 after processing it.</p> <p></p> <p>xTR6 processes this map reply and inserts the prefix in the LISP map-cache and pushes that to the CEF table as well:</p> <pre><code>// xTR6s LISP map-cache for 1.1.1.1\n\nxTR6#show ip lisp map-cache 1.1.1.1\nLISP IPv4 Mapping Cache for EID-table default (IID 0), 3 entries\n\n1.1.1.0/24, uptime: 00:06:41, expires: 23:53:18, via map-reply, complete\n  Sources: map-reply\n  State: complete, last modified: 00:06:41, map-source: 2.2.2.2\n  Idle, Packets out: 2(200 bytes) (~ 00:06:38 ago)\n  Locator  Uptime    State      Pri/Wgt\n  2.2.2.2  00:06:41  up           1/50 \n    Last up-down state change:         00:06:41, state change count: 1\n    Last route reachability change:    00:06:41, state change count: 1\n    Last priority / weight change:     never/never\n    RLOC-probing loc-status algorithm:\n      Last RLOC-probe sent:            never\n\n// xTR6s CEF table for 1.1.1.1      \n\nxTR6#show ip cef 1.1.1.1 detail\n1.1.1.0/24, epoch 0, flags [subtree context, check lisp eligibility]\n  SC owned,sourced: LISP remote EID - locator status bits 0x00000001\n  LISP remote EID: 2 packets 200 bytes fwd action encap\n  LISP source path list\n    nexthop 2.2.2.2 LISP0\n  2 IPL sources [no flags]\n  nexthop 2.2.2.2 LISP0\n</code></pre> <p>xTR6 can now encapsulate subsequent packets and send them to xTR2.</p> <p></p> <p>xTR2 decapsulates them and sends them to R1. </p> <p></p>","tags":["sda","lisp"]},{"location":"blog/2021/12/28/cisco-sda-part-iv---lisp-mobility---dynamic-eids/","title":"Cisco SDA Part IV - LISP mobility - dynamic EIDs","text":"<p>In this post, we look at LISP dynamic EID - a core construct of LISP host mobility.</p>","tags":["sda","lisp"]},{"location":"blog/2021/12/28/cisco-sda-part-iv---lisp-mobility---dynamic-eids/#introduction-and-topology","title":"Introduction and topology","text":"<p>One of the most important characteristics of LISP is the mobility it offers - the next few posts aim at helping understand how this functionality is achieved, starting with dynamic EIDs.</p> <p>We will continue using the same topology as before, with some minor changes to the xTRs. xTR6 is now another xTR and not a PxTR.</p> <p></p> <p>The goal is this post is to understand how dynamic EIDs are configured and how does it really function behind the scenes. Our final test is to have complete connectivity between 1.1.1.1 and 5.5.5.5. Our initial (and relevant LISP configuration) is pretty standard:</p> <pre><code>// xTR2\n\nxTR2#show run | sec router lisp\nrouter lisp\n eid-table default instance-id 0\n  exit\n !\n ipv4 itr map-resolver 3.3.3.3\n ipv4 itr\n ipv4 etr map-server 3.3.3.3 key cisco\n ipv4 etr\n exit\n\n// xTR4 \n\nxTR4#show run | sec router lisp\nrouter lisp\n eid-table default instance-id 0\n  exit\n !\n ipv4 itr map-resolver 3.3.3.3\n ipv4 itr\n ipv4 etr map-server 3.3.3.3 key cisco\n ipv4 etr\n exit\n\n// xTR6\n\nxTR6#show run | sec router lisp\nrouter lisp\n eid-table default instance-id 0\n  exit\n !\n ipv4 itr map-resolver 3.3.3.3\n ipv4 itr\n ipv4 etr map-server 3.3.3.3 key cisco\n ipv4 etr\n exit\n</code></pre>","tags":["sda","lisp"]},{"location":"blog/2021/12/28/cisco-sda-part-iv---lisp-mobility---dynamic-eids/#dynamic-eids","title":"Dynamic EIDs","text":"","tags":["sda","lisp"]},{"location":"blog/2021/12/28/cisco-sda-part-iv---lisp-mobility---dynamic-eids/#configuring-dynamic-eids","title":"Configuring dynamic EIDs","text":"<p>With dynamic EIDs, the first thing you realize is that you have to use locator sets. Think of locator sets as a parallel to peer groups in BGP - you can use one locator set, that uniquely identifies a RLOC and its attributes, and apply that to several instances of LISP. </p> <pre><code>xTR2(config-router-lisp)#locator-set xTR2 \nxTR2(config-router-lisp-locator-set)#ipv4-interface lo0 priority 1 weight 50\nxTR2(config-router-lisp-locator-set)#end\n</code></pre> <p>Before we create the dynamic EID, take a quick look at the LISP map-cache:</p> <pre><code>xTR2#show ip lisp map-cache\nLISP IPv4 Mapping Cache for EID-table default (IID 0), 1 entries\n\n0.0.0.0/0, uptime: 00:05:24, expires: never, via static send map-request\n  Negative cache entry, action: send-map-request\n</code></pre> <p>As expected, there's just a single, default entry in the LISP cache. Let's configure a dynamic EID now:</p> <pre><code>xTR2(config-router-lisp)#eid-table default instance-id 0\nxTR2(config-router-lisp-eid-table)#dynamic-eid 1.1.1.0/24_EID\nxTR2(config-router-lisp-eid-table-dynamic-eid)#database-mapping 1.1.1.0/24 locator-set xTR2 \nxTR2(config-router-lisp-eid-table-dynamic-eid)#end\n</code></pre> <p>You create a dynamic EID using the 'dynamic-eid' CLI and give it a name that can be referenced later. Within this dynamic EID, you need to create your database mappings - this dictates the prefix range that is assumed to be dynamic (or 'roaming') in nature.</p> <p>This creates a new entry in the LISP map-cache:</p> <pre><code>xTR2#show ip lisp map-cache\nLISP IPv4 Mapping Cache for EID-table default (IID 0), 2 entries\n\n0.0.0.0/0, uptime: 00:09:04, expires: never, via static send map-request\n  Negative cache entry, action: send-map-request\n1.1.1.0/24, uptime: 00:01:32, expires: never, via dynamic-EID, send-map-request\n  Negative cache entry, action: send-map-request\n</code></pre> <p>But notice, as of now, the MS/MR has no registrations against this EID:</p> <pre><code>MS_MR#show lisp site 1.1.1.0/24\nLISP Site Registration Information\n\nSite name: SITE_A\nAllowed configured locators: any\nRequested EID-prefix:\n  EID-prefix: 1.1.1.0/24 \n    First registered:     00:55:01\n    Last registered:      00:10:22\n    Routing table tag:    0\n    Origin:               Configuration\n    Merge active:         No\n    Proxy reply:          No\n    TTL:                  00:00:00\n    State:                unknown\n    Registration errors:  \n      Authentication failures:   0\n      Allowed locators mismatch: 0\n    No registrations.\n</code></pre> <p>At this point, if I try to ping from R1 to xTR2 (sourcing an IP address of 1.1.1.1), nothing works and we get this error message:</p> <pre><code>LISP: Processing data signal for EID prefix IID 0 1.1.1.1/32\nLISP-0: Remote EID IID 0 prefix 1.1.1.0/24, Process data signal, matching remote dynamic EID entry (sources: &lt;dynEID&gt;, state: send-map-request, rlocs: 0).\nLISP-0: Remote EID IID 0 prefix 1.1.1.1/32, Change state to incomplete (sources: &lt;signal&gt;, state: unknown, rlocs: 0).\nLISP-0: Remote EID IID 0 prefix 1.1.1.1/32, [incomplete] Scheduling map requests delay 00:00:00 min_elapsed 00:00:01 (sources: &lt;signal&gt;, state: incomplete, rlocs: 0).\nLISP-0: IID 0 No ITR RLOCs available, do not process remote EID prefix map requests.\n</code></pre> <p>So, what is missing? With dynamic EIDs, you need to configure the host facing L3 interfaces as LISP mobility interfaces as well for this to work:</p> <pre><code>xTR2(config)#int gig0/0\nxTR2(config-if)#lisp mobility 1.1.1.0/24_EID\n</code></pre> <p>This is where you call the dynamic EID that was defined at the beginning. And that's all there is to it! That is the complete configuration you need to enable dynamic EIDs in LISP on xTRs. There is a small catch though, which we'll get to in a bit. </p>","tags":["sda","lisp"]},{"location":"blog/2021/12/28/cisco-sda-part-iv---lisp-mobility---dynamic-eids/#learning-eids-dynamically","title":"Learning EIDs dynamically","text":"<p>Now that we understand how to configure dynamic EIDs, the big question remains - how are EIDs learnt dynamically? The answer is simple - via traffic. This can either be control-plane traffic (like ARP) or data-plane traffic (like an ICMP request). </p> <p>When said traffic hits a xTR that is enabled for dynamic EIDs and it falls within that prefix range, the dynamic registration process starts. This includes sending a Map Register to the MS/MR as well as adding an exact match entry in the LISP database and RIB/FIB.</p> <p></p> <p>The Map Register process is the same as before - nothing special here. xTR2 generates a Map Register packet and sends that to the configured Map Server (MS/MR, in our case). The MS/MR looks at its site database and confirms that an EID exists in its database that matches the EID in the register and then sends a Map Notify back. </p> <p>Interestingly enough though, we see no Map Notify coming back from the MS/MR right now:</p> <p></p> <p>Any guesses on why? Let's walk through the entire flow and understand why this happened. xTR2 gets a data packet (or an ARP) from R1 which has a source IP address of 1.1.1.1 and a destination IP address of 10.1.12.2. This triggers the LISP dynamic EID learn, which invokes an exact route installation in the LISP database and RIB/FIB. </p> <p>We can confirm that both of these occurred correctly:</p> <pre><code>// RIB table\n\nxTR2#show ip route 1.1.1.1\nRouting entry for 1.1.1.1/32\n  Known via \"lisp\", distance 10, metric 1, type unknown\n  Last update from 1.1.1.1 on GigabitEthernet0/0, 00:09:36 ago\n  Routing Descriptor Blocks:\n  * 1.1.1.1, from 0.0.0.0, 00:09:36 ago, via GigabitEthernet0/0\n      Route metric is 1, traffic share count is 1\n\n// LISP database\n\nxTR2#show ip lisp database \nLISP ETR IPv4 Mapping Database for EID-table default (IID 0), LSBs: 0x1, 1 entries\n\n1.1.1.1/32, dynamic-eid 1.1.1.0/24_EID, locator-set xTR2\n  Locator  Pri/Wgt  Source     State\n  2.2.2.2    0/0    cfg-intf   site-self, reachable \n</code></pre> <p>Additionally, a Map Register is sent to the MS/MR. Does the MS/MR have a match for this EID in its site database?</p> <pre><code>MS_MR#show lisp site 1.1.1.1   \nLISP Site Registration Information\n\nSite name: SITE_A\nAllowed configured locators: any\nRequested EID-prefix:\n  EID-prefix: 1.1.1.0/24 \n    First registered:     01:19:16\n    Last registered:      00:34:37\n    Routing table tag:    0\n    Origin:               Configuration\n    Merge active:         No\n    Proxy reply:          No\n    TTL:                  00:00:00\n    State:                unknown\n    Registration errors:  \n      Authentication failures:   0\n      Allowed locators mismatch: 0\n    No registrations.\n</code></pre> <p>Yes, it does. However, pay close attention to the output - we are hitting the 1.1.1.0/24 entry and naturally, there are no registrations against this EID so it cannot send a Map Notify back. </p> <p>This is one of the biggest gotchas with dynamic EIDs which leads me to one of the most important changes needed to make dynamic EIDs work - you must allow for more specific prefixes to be accepted on the MS/MR. </p> <p>It is a very simple, but crucial change:</p> <pre><code>MS_MR(config)#router lisp\nMS_MR(config-router-lisp)#site SITE_A\nMS_MR(config-router-lisp-site)#eid-prefix 1.1.1.0/24 accept-more-specifics\nMS_MR(config-router-lisp-site)#end\n</code></pre> <p>Now, when the Map Register is sent again, the MS/MR is willing to accept a more specific prefix under the range of 1.1.1.0/24 and it sends the corresponding Map Notify back:</p> <p></p> <p>The MS/MR also has this entry in its site database now:</p> <pre><code>MS_MR#show lisp site 1.1.1.1\nLISP Site Registration Information\n\nSite name: SITE_A\nAllowed configured locators: any\nRequested EID-prefix:\n  EID-prefix: 1.1.1.1/32 \n    First registered:     00:04:23\n    Last registered:      00:00:24\n    Routing table tag:    0\n    Origin:               Dynamic, more specific of 1.1.1.0/24\n    Merge active:         No\n    Proxy reply:          No\n    TTL:                  1d00h\n    State:                complete\n    Registration errors:  \n      Authentication failures:   0\n      Allowed locators mismatch: 0\n    ETR 2.2.2.2, last registered 00:00:24, no proxy-reply, map-notify\n                 TTL 1d00h, no merge, hash-function sha1, nonce 0x3737C5D8-0x6F651C53\n                 state complete, no security-capability\n                 xTR-ID 0xA55F2BD8-0xA072A7CF-0xDCD31112-0xC826C03F\n                 site-ID unspecified\n      Locator  Local  State      Pri/Wgt  Scope\n      2.2.2.2  yes    up           0/0    IPv4 none\n</code></pre> <p>As you can see, this is an entirely new entry that was dynamically added to the site database of the MS/MR because we allowed it to learn more specific prefixes within the scope of 1.1.1.0/24. </p>","tags":["sda","lisp"]},{"location":"blog/2021/12/28/cisco-sda-part-iv---lisp-mobility---dynamic-eids/#completing-our-configuration","title":"Completing our configuration","text":"<p>Let's complete our configuration for the 5.5.5.0/24 EID space as well. This requires three changes - first, on xTR4, configure this EID as a dynamic EID range. Second, on the MS/MR, allow for more specific prefixes to be learnt for this EID space. Lastly, on the interface facing the host, enable LISP mobility against the configured dynamic EID. </p> <pre><code>// configure a locator-set for xTR4\n\nxTR4(config-router-lisp)#locator-set xTR4\nxTR4(config-router-lisp-locator-set)#ipv4-interface lo0\n\n// configure dynamic EID for 5.5.5.0/24\n\nxTR4(config-router-lisp)#eid-table default instance-id 0\nxTR4(config-router-lisp-eid-table)#dynamic-eid 5.5.5.0/24_EID\nxTR4(config-router-lisp-eid-table-dynamic-eid)#database-mapping 5.5.5.0/24 locator-set xTR4\n\n// configure interface gig0/0 for LISP mobility\n\nxTR4(config-router-lisp-eid-table)#int gig0/0\nxTR4(config-if)#lisp mobility 5.5.5.0/24_EID  \n\n// configure MS/MR to allow more specifics for 5.5.5.0/24 range\n\nMS_MR(config)#router lisp\nMS_MR(config-router-lisp)#site SITE_B\nMS_MR(config-router-lisp-site)#eid-prefix 5.5.5.0/24 accept-more-specifics\nMS_MR(config-router-lisp-site)#end\n</code></pre> <p>Let's trigger a dynamic EID learn for the 5.5.5.0/24 range and ensure that an entry gets added into the MS/MR site database. </p> <pre><code>R5#ping 10.1.45.4 source 5.5.5.5\nType escape sequence to abort.\nSending 5, 100-byte ICMP Echos to 10.1.45.4, timeout is 2 seconds:\nPacket sent with a source address of 5.5.5.5 \n.!!!!\nSuccess rate is 80 percent (4/5), round-trip min/avg/max = 4/4/5 ms\n</code></pre> <p>A look at xTR4s LISP database and the MS/MRs site database to confirm the expected entries were added:</p> <pre><code>// xTR4s LISP database\n\nxTR4#show ip lisp database  \nLISP ETR IPv4 Mapping Database for EID-table default (IID 0), LSBs: 0x1, 1 entries\n\n5.5.5.5/32, dynamic-eid 5.5.5.0/24_EID, locator-set xTR4\n  Locator  Pri/Wgt  Source     State\n  4.4.4.4    0/0    cfg-intf   site-self, reachable\n\n// MS/MRs site database  \n\nMS_MR#show lisp site 5.5.5.5\nLISP Site Registration Information\n\nSite name: SITE_B\nAllowed configured locators: any\nRequested EID-prefix:\n  EID-prefix: 5.5.5.5/32 \n    First registered:     00:00:24\n    Last registered:      00:00:24\n    Routing table tag:    0\n    Origin:               Dynamic, more specific of 5.5.5.0/24\n    Merge active:         No\n    Proxy reply:          No\n    TTL:                  1d00h\n    State:                complete\n    Registration errors:  \n      Authentication failures:   0\n      Allowed locators mismatch: 0\n    ETR 4.4.4.4, last registered 00:00:24, no proxy-reply, map-notify\n                 TTL 1d00h, no merge, hash-function sha1, nonce 0xC5BB43A9-0x4B85B722\n                 state complete, no security-capability\n                 xTR-ID 0xF1E43ADA-0x5BB5F6AE-0xAE8DC820-0x4AE98A3E\n                 site-ID unspecified\n      Locator  Local  State      Pri/Wgt  Scope\n      4.4.4.4  yes    up           0/0    IPv4 none\n</code></pre> <p>At this point, we should have complete reachability from 1.1.1.1 to 5.5.5.5, which we do. The data-plane process is the same as normal EIDs - nothing new there.</p> <pre><code>R1#ping 5.5.5.5 source 1.1.1.1\nType escape sequence to abort.\nSending 5, 100-byte ICMP Echos to 5.5.5.5, timeout is 2 seconds:\nPacket sent with a source address of 1.1.1.1 \n..!!!\nSuccess rate is 60 percent (3/5), round-trip min/avg/max = 7/10/14 ms\n</code></pre> <p>This completes our section of understanding dynamic EIDs. In the next post, we will look at host mobility and what happens when a host moves from one RLOC to another. See you in the next one!</p>","tags":["sda","lisp"]},{"location":"blog/2021/12/29/cisco-sda-part-v---lisp-mobility---roaming-hosts/","title":"Cisco SDA Part V - LISP mobility - roaming hosts","text":"<p>In this post, we look at an actual LISP host mobility event and what happens behind the scenes to make this work.</p>","tags":["sda","lisp"]},{"location":"blog/2021/12/29/cisco-sda-part-v---lisp-mobility---roaming-hosts/#introduction-and-topology","title":"Introduction and topology","text":"<p>Continuing on from the previous post, we take a look at actual host mobility events and how the LISP infrastructure facilitates this. Our goal for this post is to have the simulated host (1.1.1.1) move from behind xTR2 to behind xTR4 (simulated via R10). A working assumption used in the post is that there is no active traffic destined for the host that is moving (we will look at this in the SMR post).</p> <p>The topology is a slightly modified version of what we used in the last post:</p> <p></p> <p>There are certain baselines we need to establish from the get go - 1.1.1.1/32 has been learnt dynamically and installed in the site database of the MS/MR, with a RLOC of xTR2 (2.2.2.2). Let's confirm this:</p> <pre><code>MS_MR#show lisp site 1.1.1.1\nLISP Site Registration Information\n\nSite name: SITE_A\nAllowed configured locators: any\nRequested EID-prefix:\n  EID-prefix: 1.1.1.1/32 \n    First registered:     00:19:38\n    Last registered:      00:00:56\n    Routing table tag:    0\n    Origin:               Dynamic, more specific of 1.1.1.0/24\n    Merge active:         No\n    Proxy reply:          No\n    TTL:                  1d00h\n    State:                complete\n    Registration errors:  \n      Authentication failures:   0\n      Allowed locators mismatch: 0\n    ETR 2.2.2.2, last registered 00:00:56, no proxy-reply, map-notify\n                 TTL 1d00h, no merge, hash-function sha1, nonce 0xC6805C87-0xF287CBA7\n                 state complete, no security-capability\n                 xTR-ID 0x8527C2E3-0xB036F499-0x0C2893BB-0x56D43540\n                 site-ID unspecified\n      Locator  Local  State      Pri/Wgt  Scope\n      2.2.2.2  yes    up           0/0    IPv4 none\n</code></pre> <p>On xTR2, this entry is also present in its LISP database:</p> <pre><code>xTR2#show ip lisp database \nLISP ETR IPv4 Mapping Database for EID-table default (IID 0), LSBs: 0x1, 1 entries\n\n1.1.1.1/32, dynamic-eid 1.1.1.0/24_EID, locator-set xTR2\n  Locator  Pri/Wgt  Source     State\n  2.2.2.2    0/0    cfg-intf   site-self, reachable\n</code></pre> <p>Additionally, xTR4 needs to be provisioned for 1.1.1.0/24 as a dynamic EID:</p> <pre><code>xTR4(config)#router lisp\nxTR4(config-router-lisp)#eid-table default instance-id 0\nxTR4(config-router-lisp-eid-table)#dynamic-eid 1.1.1.0/24_EID\nxTR4(config-router-lisp-eid-table-dynamic-eid)#database-mapping 1.1.1.0/24 locator-set xTR4\n</code></pre>","tags":["sda","lisp"]},{"location":"blog/2021/12/29/cisco-sda-part-v---lisp-mobility---roaming-hosts/#lisp-mobility-event","title":"LISP mobility event","text":"<p>So, now that we have these baselines established, let's move 1.1.1.1 to R10. </p> <p></p> <p>When this happens, some traffic (can be control-plane or data-plane) is generated from 1.1.1.1 and it hits xTR4. xTR4 dynamically learns this, installs it in its LISP database and sends a Map Register to the MS/MR. The MS/MR accepts this and installs it in its site database. </p> <p></p> <p>At the same time, the MS/MR knew that there was a previously registered RLOC against this specific /32 entry. It informs the original RLOC (xTR2, in this case) of this new RLOC learn by sending it a Map Notify as well. </p> <p></p> <p>This Map Notify is as follows:</p> <p></p> <p>When xTR2 receives this Map Notify, it realizes that the host has \"moved away\". It deletes the /32 entry from its LISP database and sends a Map Register back to the MS/MR with an action of 'Drop'. </p> <p></p> <p>This packet looks like so:</p> <p></p> <p>On receiving this Map Register, the MS/MR removes xTR2 as a RLOC for 1.1.1.1/32. This action completes the host move. </p>","tags":["sda","lisp"]},{"location":"blog/2021/12/30/cisco-sda-part-vi---lisp-mobility---solicit-map-requests-smrs/","title":"Cisco SDA Part VI - LISP mobility - Solicit Map Requests (SMRs)","text":"<p>In this post, we look at SMRs and how these are essential for a host mobility event, within the LISP architecture.</p>","tags":["sda","lisp"]},{"location":"blog/2021/12/30/cisco-sda-part-vi---lisp-mobility---solicit-map-requests-smrs/#introduction-and-topology","title":"Introduction and topology","text":"<p>We start this post with the assumption that a host mobility event has occurred (see previous post for details on host mobility) and that the EID 1.1.1.1/24 is moved from behind xTR2 to behind xTR6. </p> <p>The state of the topology is like so:</p> <p></p> <p>When a mobility event like this occurs, only the MS/MR is notified of this, along with the original RLOC. Remember that LISP works using a pull based model. This implies that the actual datapath remains unchanged on the xTRs even after a mobility event. What this means is that if 5.5.5.5/24 tries to reach 1.1.1.1/24, the data-plane programming still points to xTR2. </p> <p>Confirm the same by looking at the LISP map-cache and the CEF table on xTR4:</p> <pre><code>// xTR4s LISP map-cache\n\nxTR4#show ip lisp map-cache 1.1.1.1\nLISP IPv4 Mapping Cache for EID-table default (IID 0), 4 entries\n\n1.1.1.1/32, uptime: 00:00:37, expires: 23:59:22, via map-reply, complete\n  Sources: map-reply\n  State: complete, last modified: 00:00:37, map-source: 2.2.2.2\n  Active, Packets out: 0(0 bytes)\n  Encapsulating dynamic-EID traffic\n  Locator  Uptime    State      Pri/Wgt\n  2.2.2.2  00:00:37  up           0/0  \n    Last up-down state change:         00:00:37, state change count: 1\n    Last route reachability change:    00:00:37, state change count: 1\n    Last priority / weight change:     never/never\n    RLOC-probing loc-status algorithm:\n      Last RLOC-probe sent:            never\n\n// xTR4s CEF table\n\nxTR4#show ip cef 1.1.1.1 detail \n1.1.1.1/32, epoch 0, flags [subtree context, check lisp eligibility]\n  SC owned,sourced: LISP remote EID - locator status bits 0x00000001\n  LISP remote EID: 19 packets 1900 bytes fwd action encap, dynamic EID need encap\n  SC inherited: LISP cfg dyn-EID - LISP configured dynamic-EID\n  LISP EID attributes: localEID No, c-dynEID Yes, d-dynEID No\n  LISP source path list\n    nexthop 2.2.2.2 LISP0\n  2 IPL sources [no flags]\n  nexthop 2.2.2.2 LISP0\n</code></pre>","tags":["sda","lisp"]},{"location":"blog/2021/12/30/cisco-sda-part-vi---lisp-mobility---solicit-map-requests-smrs/#solicit-map-requests","title":"Solicit Map Requests","text":"<p>So, how does the LISP infrastructure inform xTR4 that the EID 1.1.1.1/24 has actually moved and it no longer exists behind xTR2? This is where SMRs (solicit map requests) come into play. </p> <p>After the host mobility event, consider the next packet that hits xTR4, destined for 1.1.1.1. Based on the CEF table seen above, xTR4 encapsulates this and sends it to xTR2. </p> <p></p> <p>When xTR2 receives this, it checks its LISP database, where it finds no entry for this EID. It generates a map request (with the 'S' bit set) and sends that to the source RLOC, xTR4, in this case. This map request is called a SMR (solicit map request). </p> <p></p> <p>Essentially, what xTR2 is trying to convey is that the destination EID is no longer behind it and the originating RLOC should ask the MS/MR again for information on where this EID is now. </p> <p>The SMR packet looks like this (notice that the 'S' bit is set):</p> <p></p> <p>When xTR4 gets this SMR, it generates an encapsulated map request (with the 's' bit set) and sends that to the MS/MR. The MS/MR forwards this to xTR6, since that is the RLOC for 1.1.1.1/32 as registered in its site database. xTR6 sends a Map Reply back to xTR4.</p> <p></p> <p>The Map Request generated on receipt of a SMR looks like this (notice the 's' bit is set; the purpose of the 's' bit is simply to signify that this map request was invoked by the receipt of a SMR):</p> <p></p> <p>Once this map reply is received by xTR4, it now updates its LISP map-cache table (and the corresponding entry in the CEF table as well) to point to xTR6 as the RLOC for this EID. </p> <pre><code>// xTR4s LISP map-cache\n\nxTR4#show ip lisp map-cache \nLISP IPv4 Mapping Cache for EID-table default (IID 0), 4 entries\n\n0.0.0.0/0, uptime: 00:52:46, expires: never, via static send map-request\n  Negative cache entry, action: send-map-request\n1.1.1.0/24, uptime: 00:46:52, expires: never, via dynamic-EID, send-map-request\n  Negative cache entry, action: send-map-request\n1.1.1.1/32, uptime: 00:35:38, expires: 23:58:39, via map-reply, complete\n  Locator  Uptime    State      Pri/Wgt\n  6.6.6.6  00:01:20  up           0/0  \n5.5.5.0/24, uptime: 00:52:46, expires: never, via dynamic-EID, send-map-request\n\n// xTR4s CEF table\n\nxTR4#show ip cef 1.1.1.1 detail\n1.1.1.1/32, epoch 0, flags [subtree context, check lisp eligibility]\n  SC owned,sourced: LISP remote EID - locator status bits 0x00000001\n  LISP remote EID: 147331 packets 14733100 bytes fwd action encap, dynamic EID need encap\n  SC inherited: LISP cfg dyn-EID - LISP configured dynamic-EID\n  LISP EID attributes: localEID No, c-dynEID Yes, d-dynEID No\n  LISP source path list\n    nexthop 6.6.6.6 LISP0\n  2 IPL sources [no flags]\n  nexthop 6.6.6.6 LISP0\n</code></pre> <p>Post this, traffic destined to 1.1.1.1 now gets directed to xTR6.</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/01/cisco-sda-part-vii---multi-instance-lisp/","title":"Cisco SDA Part VII - multi-instance LISP","text":"<p>In this post, we look at multi-instance LISP, which is another core construct for Cisco's SD-Access.</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/01/cisco-sda-part-vii---multi-instance-lisp/#introduction-and-topology","title":"Introduction and topology","text":"<p>We're slowly getting closer to the true implementation of LISP in Cisco's SD-Access. LISP has the capability of being VRF-aware - this is achieved via multi-instance LISP. </p> <p>The idea is fairly simple - you have multiple instances of LISP (mapped to corresponding VRFs) - all your LISP tables are now maintained per instance. </p> <p>We will be using the following topology for this:</p> <p></p>","tags":["sda","lisp"]},{"location":"blog/2022/01/01/cisco-sda-part-vii---multi-instance-lisp/#vrf-aware-configuration","title":"VRF aware configuration","text":"<p>We have moved R1 and R5 into a VRF called 'TEST'. This is done by assigning the port facing R1 on xTR2 to this VRF:</p> <pre><code>xTR2#show run int gig0/0\nBuilding configuration...\n\nCurrent configuration : 246 bytes\n!\ninterface GigabitEthernet0/0\n vrf forwarding TEST\n ip address 10.1.12.2 255.255.255.0\n no ip redirects\n ip ospf network point-to-point\n duplex auto\n speed auto\n media-type rj45\n no lisp mobility liveness test\n lisp mobility 1.1.1.0/24_EID\nend\n</code></pre> <p>Similar configuration is done on xTR4 as well. To make LISP VRF aware, you need to create instances of LISP and map the instance to a particular VRF. All your database mappings will now come under this instance-ID.</p> <p>For this example, we will create an instance-ID of 100 and map the VRF 'TEST' to this instance-ID.</p> <pre><code>xTR2(config)#router lisp\nxTR2(config-router-lisp)#eid-table vrf TEST instance-id 100\nxTR2(config-router-lisp-eid-table)#database-mapping 1.1.1.0/24 locator-set xTR2\n</code></pre> <p>Similar configuration is done on xTR4 as well:</p> <pre><code>xTR4(config)#router lisp\nxTR4(config-router-lisp)# eid-table vrf TEST instance-id 100\nxTR4(config-router-lisp-eid-table)#database-mapping 5.5.5.0/24 locator-set xTR4\n</code></pre> <p>All of these database mappings will now be per instance-ID. They will no longer show up in the global LISP database.</p> <pre><code>// no entries found in xTR2s global LISP database\n\nxTR2#show ip lisp database \n% Could not find EID table in configuration.\n\n// xTR2s LISP database for instance-ID 100\n\nxTR2#show ip lisp instance-id 100 database \nLISP ETR IPv4 Mapping Database for EID-table vrf TEST (IID 100), LSBs: 0x1, 1 entries\n\n1.1.1.0/24, locator-set xTR2\n  Locator  Pri/Wgt  Source     State\n  2.2.2.2    0/0    cfg-intf   site-self, reachable\n\n// no entries found in xTR4s global LISP database\n\nxTR4#show ip lisp database \n% Could not find EID table in configuration.\n\n// xTR4s LISP database for instance-ID 100\n\nxTR4#show ip lisp instance-id 100 database \nLISP ETR IPv4 Mapping Database for EID-table vrf TEST (IID 100), LSBs: 0x1, 1 entries\n\n5.5.5.0/24, locator-set xTR4\n  Locator  Pri/Wgt  Source     State\n  4.4.4.4    0/0    cfg-intf   site-self, reachable\n</code></pre> <p>From the MS/MR perspective, we simply map the instance-IDs to the EIDs within the sites:</p> <pre><code>MS_MR#show run | sec router lisp\nrouter lisp\n eid-table default instance-id 0\n  exit\n !\n site SITE_A\n  authentication-key cisco\n  eid-prefix instance-id 100 1.1.1.0/24\n  exit\n !\n site SITE_B\n  authentication-key cisco\n  eid-prefix instance-id 100 5.5.5.0/24\n  exit\n !\n ipv4 map-server\n ipv4 map-resolver\n ipv4 map-request-source 3.3.3.3\n exit\n</code></pre>","tags":["sda","lisp"]},{"location":"blog/2022/01/01/cisco-sda-part-vii---multi-instance-lisp/#packet-walks","title":"Packet walks","text":"<p>From this point on, the control-plane and data-plane packet walks are the same - the only difference is that the lookups happen against the specific instance-IDs that are defined. Let's consider both possible scenarios in our above topology. </p> <p>If the packet comes from R1, it comes in the VRF called TEST. This can be visualized like so:</p> <p></p> <p>If the packet comes from R8, it comes in another VRF we created called TEST_2. This can be visualized like so:</p> <p></p> <p>How does the LISP process know which instance-ID to consider? This is populated in the LISP packets themselves. For example, consider the following 'Encapsulated Map Request' packet:</p> <p></p> <p>The LISP packet format has the provision to carry the instance-ID as an attribute within the source EID (as you can see from the above packet). This determines which instance-ID the lookups are done in. The same instance-ID is carried in actual data-plane packets as well. Consider the following ICMP packet as an example:</p> <p></p> <p>This is the truest form of LISP implementation in SDA and this is exactly how SDA achieves macro segmentation. The terminology is 'Virtual Network' or VN in the SDA world however this is nothing but VRFs. </p>","tags":["sda","lisp"]},{"location":"blog/2022/01/02/cisco-sda-part-viii---dhcp-challenges-in-sda/","title":"Cisco SDA Part VIII - DHCP challenges in SDA","text":"<p>In this post, we look at various DHCP challenges in Cisco's SD-Access fabric and how it is solved.</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/02/cisco-sda-part-viii---dhcp-challenges-in-sda/#introduction-and-topology","title":"Introduction and topology","text":"<p>Remember that in SD-Access, we do not use vanilla LISP. To achieve macro segmentation, multi-instance LISP (VRF-aware LISP) is used. However, this poses a problem for DHCP. Consider the following topology for this (this topology is also a simple example of SD-Access design):</p> <p></p>","tags":["sda","lisp"]},{"location":"blog/2022/01/02/cisco-sda-part-viii---dhcp-challenges-in-sda/#tracing-the-dhcp-discover","title":"Tracing the DHCP Discover","text":"<p>Host1 is in VLAN 1029 and it sends a DHCP discover. This hits interface VLAN 1029, which has an IP helper-address that points to 172.168.1.1 (which is the DHCP server). The full configuration of this interface VLAN is:</p> <pre><code>Edge1#show run int vlan 1029\nBuilding configuration...\n\nCurrent configuration : 314 bytes\n!\ninterface Vlan1029\n description Configured from Cisco DNA-Center\n mac-address 0000.0c9f.f464\n vrf forwarding Campus_VN\n ip address 176.169.71.1 255.255.255.0\n ip helper-address 172.168.1.1\n no ip redirects\n ip route-cache same-interface\n no lisp mobility liveness test\n lisp mobility 176_169_71_0-Campus_VN\nend\n</code></pre> <p>However, this interface VLAN is in VRF 'Campus_VN'. Inside the routing table for this VRF, does a route to the DHCP server exist? </p> <pre><code>Edge1#show ip route vrf Campus_VN 172.168.1.1 \n\nRouting Table: Campus_VN\n% Network not in table\n</code></pre> <p>So, there's no route to the DHCP server. Why would we ever look at the VRF routing table and not invoke LISP in the first place?  Think back to what a DHCP discover is like. The source IP address of the DHCP discover is 0.0.0.0 while the destination IP address is 255.255.255.255. </p> <p>Remember the major LISP rules? Well, we fail the source EID check right here because 0.0.0.0 will never match a known source EID. This means that we cannot invoke LISP and fall back to normal routing, which implies a lookup in the respective routing table (in this case, for VRF 'Campus_VN'). With no route to the DHCP server in this RIB, the packet is dropped and the host will never get an IP address. </p> <p>Do you see the problem that multi-instance LISP presents now? The fix is very, very simple but it is one that perplexed me for a while - I could see the fix (or the configuration rather) but I did not know what it fixes or why that configuration existed in the first place. In hindsight of course, everything makes perfect sense.</p> <p>For those who want to take a stab at the configuration that fixes this issue, stop now! For those who are already tired of reading this old man's drivel, the answer is to configure the edges as proxy-iTRs. Why? Because proxy-iTRs do not perform source EID checks. That means the DHCP discover gets elevated into the overlay since LISP will get invoked:</p> <pre><code>// DHCP Discover hits this entry in CEF\n\nEdge1#show ip cef vrf Campus_VN 172.168.1.1 \n0.0.0.0/0\n  attached to LISP0.4100\n\n// detailed output of the same CEF entry  \n\nEdge1#show ip cef vrf Campus_VN 172.168.1.1 detail\n0.0.0.0/0, epoch 0, flags [cover dependents, check lisp eligibility, default route]\n  LISP remote EID: 2 packets 903 bytes fwd action signal, cfg as EID space\n  LISP source path list\n    attached to LISP0.4100\n  Covered dependent prefixes: 2\n    notify cover updated: 2\n  1 IPL source [no flags]\n  attached to LISP0.4100\n\n// since LISP is invoked, we look at the map-cache\n// no specific entry here, so it hits the default 0.0.0.0/0 entry  \n\nEdge1#show ip lisp eid-table vrf Campus_VN map-cache 172.168.1.1\nLISP IPv4 Mapping Cache for EID-table vrf Campus_VN (IID 4100), 3 entries\n\n0.0.0.0/0, uptime: 00:35:21, expires: never, via static-send-map-request\n  Sources: static-send-map-request\n  State: send-map-request, last modified: 00:35:21, map-source: local\n  Exempt, Packets out: 2(903 bytes) (~ 00:29:50 ago)\n  Configured as EID address space\n  Negative cache entry, action: send-map-request \n</code></pre>","tags":["sda","lisp"]},{"location":"blog/2022/01/02/cisco-sda-part-viii---dhcp-challenges-in-sda/#dhcp-server-known-to-lisp","title":"DHCP server known to LISP","text":"<p>At this point, one of two things can happen: </p> <ol> <li>The route to the DHCP server is redistributed into the LISP site database and the Border knows of this. If that is the case, the Border replies back to the map request with its own RLOC information and the packet is encapsulated to it. </li> </ol> <p></p>","tags":["sda","lisp"]},{"location":"blog/2022/01/02/cisco-sda-part-viii---dhcp-challenges-in-sda/#dhcp-server-unknown-to-lisp","title":"DHCP server unknown to LISP","text":"<ol> <li>The Border has no idea of the EID and there is a miss in the site database. In that case, it responds with a negative map reply and the Edge will encapsulate the packet and send it to the Border anyway (since 'use-petr' will be configured as the Border). </li> </ol> <p>Both are valid designs and are okay to run with. Either of them gets the packet to the border from where it is routed to the DHCP server.</p> <p></p>","tags":["sda","lisp"]},{"location":"blog/2022/01/02/cisco-sda-part-viii---dhcp-challenges-in-sda/#tracing-the-dhcp-offer-and-the-problem-it-presents","title":"Tracing the DHCP Offer and the problem it presents","text":"<p>The DHCP server gets this and uses the 'giaddr' to determine which pool to allocate the address from (your traditional DHCP functionality, nothing magical here). It sends an offer back - the offer will have a source IP address of the DHCP server and the destination IP address of the 'giaddr' address.</p> <p>And here's the second major problem that DHCP needs to deal with in SD-Access - this 'giaddr' address is essentially the interface VLANs IP address, which (as you might have guessed it) is the anycast gateway IP address across all Edge's. Essentially, the same VLAN and IP address mapping will exist across all Edge's. </p> <p></p> <p>And here is the problem - how can the DHCP offer be directed to the Edge that actually sourced the DHCP discover? </p> <p></p>","tags":["sda","lisp"]},{"location":"blog/2022/01/02/cisco-sda-part-viii---dhcp-challenges-in-sda/#dhcp-option82","title":"DHCP option82","text":"<p>This is where option82 comes in. When the DHCP discover is sourced by the Edge, it fills in the option82 field as well. This contains two important things - its RLOC and the VXLAN-ID (VNID) to be used. Take a look at the following packet capture:</p> <p></p> <p>This is a DHCP discover sourced by our Edge in this topology (Edge1). The discover is built with the relevant options (including option82). The inner IP header has a source IP address of interface VLAN 1029s and a destination IP address of the DHCP server. </p> <p>The VXLAN header has a VNI that maps to the instance ID for this VRF. The outer IP header has a source IP address of loopback0 of this Edge and a destination IP address of loopback0 of the Border - essentially the source RLOC and the destination RLOC.</p> <p>Let's try decoding agent remote ID of option 82:</p> <p></p> <p>So, when this offer hits the Border, it is leaked to the CPU and this RLOC + VNI information is extracted from option82. The border then directs this towards the specified RLOC - 192.168.70.8, which is the Edge that sourced the discover in the first place. </p> <p>This concludes a very important aspect of SD-Access - one that should work seamlessly and without notice but something that needs to be very carefully understood.</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/03/cisco-sda-part-ix---need-for-duplicate-ips-on-fabric-borders/","title":"Cisco SDA Part IX - need for duplicate IPs on fabric borders","text":"<p>In this post, we look at why SD-Access borders have the anycast IP addresses configured as loopback addresses.</p>","tags":["sda","lisp"]},{"location":"blog/2022/01/03/cisco-sda-part-ix---need-for-duplicate-ips-on-fabric-borders/#introduction-and-topology","title":"Introduction and topology","text":"<p>Looking at the some of the configuration that is automatically pushed from DNAC, you should spot some very interesting things in there. This post aims to demystify these and help the reader understand why these were needed in the first place, hopefully giving you a better understanding of how the SDA fabric is built. </p> <p>Let's consider the following topology for this:</p> <p></p> <p>Since a SDA fabric implements an anycast gateway type SVI across all edges, you would see the same IP address to SVI mappings created on all your edges in the fabric. Interestingly enough, the same IP address is also mapped to a loopback interface on the borders of the fabric (one loopback on the border for every SVI created on the edge). Look at the following configuration snippets from an edge and a border:</p> <pre><code>// Border1\n\nBorder1#show run int lo1022\nBuilding configuration...\n\nCurrent configuration : 123 bytes\n!\ninterface Loopback1022\n description Loopback Border\n vrf forwarding Guest_VN\n ip address 192.2.21.1 255.255.255.255\nend\n\n// Border2\n\nBorder2#show run int lo1022\nBuilding configuration...\n\nCurrent configuration : 123 bytes\n!\ninterface Loopback1022\n description Loopback Border\n vrf forwarding Guest_VN\n ip address 192.2.21.1 255.255.255.255\nend\n\n// Edge1\n\nEdge1#show run int vlan 1022\nBuilding configuration...\n\nCurrent configuration : 310 bytes\n!\ninterface Vlan1022\n description Configured from Cisco DNA-Center\n mac-address 0000.0c9f.f45d\n vrf forwarding Guest_VN\n ip address 192.2.21.1 255.255.255.0\n ip helper-address 192.2.201.224\n no ip redirects\n ip route-cache same-interface\n no lisp mobility liveness test\n lisp mobility 192_2_21_0-Guest_VN\nend\n\n// Edge2\n\nEdge2#show run int vlan 1022\nBuilding configuration...\n\nCurrent configuration : 310 bytes\n!\ninterface Vlan1022\n description Configured from Cisco DNA-Center\n mac-address 0000.0c9f.f45d\n vrf forwarding Guest_VN\n ip address 192.2.21.1 255.255.255.0\n ip helper-address 192.2.201.224\n no ip redirects\n ip route-cache same-interface\n no lisp mobility liveness test\n lisp mobility 192_2_21_0-Guest_VN\nend\n</code></pre>","tags":["sda","lisp"]},{"location":"blog/2022/01/03/cisco-sda-part-ix---need-for-duplicate-ips-on-fabric-borders/#need-for-loopback-ips-on-the-border","title":"Need for loopback IPs on the border","text":"<p>Why do we do this? Remember when we talked about how DHCP works with SDA and the need for option82? Well, this ties directly into that. In that post, I stated:</p> <p>And here's the second major problem that DHCP needs to deal with in  SD-Access - this 'giaddr' address is essentially the interface VLANs IP address, which (as you might have guessed it) is the anycast gateway IP address across all Edge's. Essentially, the same VLAN and IP address  mapping will exist across all Edge's. How can the DHCP offer be directed to the Edge that actually sourced the DHCP discover?</p> <p>The answer to this was given slightly later in the same post. For ease of reading, here is a direct quote from the post regarding how this works:</p> <p>So, when this offer hits the Border, it is leaked to the CPU and this RLOC + VNI information is extracted from option82.</p> <p>However, I did not explain the little configuration \"hack\" that was done to make it leak to the CPU. When the DHCP offer comes back from the DHCP server, the destination IP address is the anycast gateway IP address. If this is not created on the border, how else would it leak the packet to the CPU and allow for DHCP snooping to process this and interpret the option82 data? </p> <p>So, there you have it - the reason DNAC pushes /32 loopbacks to the border with the same IP address as your anycast gateway IP addresses is to allow for the DHCP packets to leak to the CPU for processing of option82. The packet is then rebuilt and sent towards the appropriate edge switch. </p> <p>A second, equally interesting bit of configuration lies within the LISP and BGP sections of the border.</p> <p>On the border, apart from configuring these /32 loopbacks with the same IPs as the anycast gateway IPs, we also advertise this into BGP:</p> <pre><code>// Border1\n\nBorder1#show run | sec router bgp\nrouter bgp 65003\n bgp router-id interface Loopback0\n bgp log-neighbor-changes\n bgp graceful-restart\n\n *snip*\n\n address-family ipv4 vrf Guest_VN\n  bgp aggregate-timer 0\n  network 192.2.21.1 mask 255.255.255.255\n  aggregate-address 192.2.21.0 255.255.255.0 summary-only\n  redistribute lisp metric 10\n  neighbor 192.2.100.6 remote-as 65002\n  neighbor 192.2.100.6 update-source Vlan3004\n  neighbor 192.2.100.6 activate\n  neighbor 192.2.100.6 weight 65535\n exit-address-family\n\n// Border2\n\nBorder2#show run | sec router bgp\nrouter bgp 65003\n bgp router-id interface Loopback0\n bgp log-neighbor-changes\n bgp graceful-restart\n\n *snip*\n\n address-family ipv4 vrf Guest_VN\n  bgp aggregate-timer 0\n  network 192.2.21.1 mask 255.255.255.255\n  aggregate-address 192.2.21.0 255.255.255.0 summary-only\n  redistribute lisp metric 10\n  neighbor 192.2.100.18 remote-as 65002\n  neighbor 192.2.100.18 update-source Vlan3012\n  neighbor 192.2.100.18 activate\n  neighbor 192.2.100.18 weight 65535\n exit-address-family \n</code></pre> <p>What is odd about this is that we're redistributing LISP into BGP already. This implies that my /32 LISP host entries in the server cache will be pushed into the BGP table and aggregated out to the peer anyway. So, what purpose does this very specific network statement serve? </p> <p>Well, consider a scenario where there are no hosts in the fabric at all. The first host connects to the network and tries to get an IP address via DHCP. At this point in time, will there be any entry in the LISP database? No - because the host doesn't even have a valid IP address yet. </p> <p>Naturally, the redistribution of LISP into BGP is useless at this point in time - there is nothing in the LISP server cache to redistribute. Because of this, the packets that come back from the DHCP server will get blackholed eventually - the upstream router (from the perspective of the border) will have no route to the anycast gateway IP address. </p> <p>This is why a manual network statement is needed to advertise this anycast gateway IP to the outside world. </p> <p>Interesting little tricks, innit? </p>","tags":["sda","lisp"]},{"location":"blog/2024/10/09/asymmetric-routing-with-sr-linux-in-evpn-vxlan-fabrics/","title":"Asymmetric routing with SR Linux in EVPN VXLAN fabrics","text":"<p>This post dives deeper into the asymmetric routing model on SR Linux. The topology in use is a 3-stage Clos fabric with BGP EVPN and VXLAN, with host h1 single-homed to leaf1, h2 dual-homed to leaf2 and leaf3 and h3 single-homed to leaf4. Hosts h1 and h2 are in the same subnet, 172.16.10.0/24 while h3 is in a different subnet, 172.16.20.0/24. Thus, this post demonstrates Layer 2 extension over a routed fabric as well as how Layer 3 services are deployed over the same fabric, with an asymmetric routing model. </p> <p>The physical topology is shown below:</p> <p></p> <p>The Containerlab file used for this is shown below:</p> <pre><code>name: srlinux-asymmetric-routing\n\ntopology:\n  nodes:\n    spine1:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n    spine2:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n    leaf1:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n    leaf2:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n    leaf3:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n    leaf4:\n      kind: nokia_srlinux\n      image: ghcr.io/nokia/srlinux\n    h1:\n      kind: linux\n      image: ghcr.io/srl-labs/network-multitool\n      exec:\n        - ip addr add 172.16.10.1/24 dev eth1\n        - ip route add 172.16.20.0/24 via 172.16.10.254\n    h2:\n      kind: linux\n      image: ghcr.io/srl-labs/network-multitool\n      exec:  \n        - ip link add bond0 type bond mode 802.3ad\n        - ip link set eth1 down \n        - ip link set eth2 down \n        - ip link set eth1 master bond0\n        - ip link set eth2 master bond0\n        - ip addr add 172.16.10.2/24 dev bond0\n        - ip link set eth1 up\n        - ip link set eth2 up\n        - ip link set bond0 up\n        - ip route add 172.16.20.0/24 via 172.16.10.254\n    h3:\n      kind: linux\n      image: ghcr.io/srl-labs/network-multitool\n      exec:\n        - ip addr add 172.16.20.3/24 dev eth1\n        - ip route add 172.16.10.0/24 via 172.16.20.254\n  links:\n    - endpoints: [\"leaf1:e1-1\", \"spine1:e1-1\"]\n    - endpoints: [\"leaf1:e1-2\", \"spine2:e1-1\"]\n    - endpoints: [\"leaf2:e1-1\", \"spine1:e1-2\"]\n    - endpoints: [\"leaf2:e1-2\", \"spine2:e1-2\"]\n    - endpoints: [\"leaf3:e1-1\", \"spine1:e1-3\"]\n    - endpoints: [\"leaf3:e1-2\", \"spine2:e1-3\"]\n    - endpoints: [\"leaf4:e1-1\", \"spine1:e1-4\"]\n    - endpoints: [\"leaf4:e1-2\", \"spine2:e1-4\"]\n    - endpoints: [\"leaf1:e1-3\", \"h1:eth1\"]\n    - endpoints: [\"leaf2:e1-3\", \"h2:eth1\"]\n    - endpoints: [\"leaf3:e1-3\", \"h2:eth2\"]\n    - endpoints: [\"leaf4:e1-3\", \"h3:eth1\"]\n</code></pre> Note <p>The host (image used is <code>ghcr.io/srl-labs/network-multitool</code>) login credentials are user/multit00l.</p> <p>The end goal of this post is to ensure that host h1 can communicate with both h2 (same subnet) and h3 (different subnet) using an asymmetric routing model. To that end, the following IPv4 addressing is used (with the IRB addressing following a distributed, anycast model):</p> Resource IPv4 scope Underlay 198.51.100.0/24 <code>system0</code> interface 192.0.2.0/24 VNI 10010 172.16.10.0/24 VNI 10020 172.16.20.0/24 host h1 172.16.10.1/24 host h2 172.16.10.2/24 host h3 172.16.20.3/24 <code>irb0.10</code> interface 172.16.10.254/24 <code>irb0.20</code> interface 172.16.20.254/24","tags":["containerlab","bgp","evpn","vxlan","srlinux","nokia"]},{"location":"blog/2024/10/09/asymmetric-routing-with-sr-linux-in-evpn-vxlan-fabrics/#reviewing-the-asymmetric-routing-model","title":"Reviewing the asymmetric routing model","text":"<p>When routing between VNIs, in a VXLAN fabric, there are two major routing models that can be used - asymmetric and symmetric. Asymmetric routing, which is the focus of this post, uses a <code>bridge-route-bridge</code> model, implying that the ingress leaf bridges the packet into the Layer 2 domain, routes it from one VLAN/VNI to another and then bridges the packet across the VXLAN fabric to the destination. </p> <p>Such a design naturally implies that both the source and the destination IRBs (and the corresponding Layer 2 domains and bridge tables) must exist on all leafs hosting servers that need to communicate with each other. While this increases the operational state on the leafs themselves (ARP state and MAC address state is stored everywhere), it does offer configuration and operational simplicity.</p> <p></p>","tags":["containerlab","bgp","evpn","vxlan","srlinux","nokia"]},{"location":"blog/2024/10/09/asymmetric-routing-with-sr-linux-in-evpn-vxlan-fabrics/#configuration-walkthrough","title":"Configuration walkthrough","text":"<p>With a basic understanding of the asymmetric routing model, let's start to configure this fabric. This configuration walkthrough includes building out the entire fabric from scratch - only the base configuration, loaded with Containerlab by default, exists on all nodes. </p>","tags":["containerlab","bgp","evpn","vxlan","srlinux","nokia"]},{"location":"blog/2024/10/09/asymmetric-routing-with-sr-linux-in-evpn-vxlan-fabrics/#point-to-point-interfaces","title":"Point-to-point interfaces","text":"<p>The underlay of the fabric includes the physically connected point-to-point interfaces between the leafs and the spines, the IPv4/IPv6 addressing used for these interfaces and a routing protocol, deployed to distribute the loopback (system0) addresses across the fabric, with the simple end goal of achieving reachability between these loopback addresses. The configuration for these point-to-point addresses is shown below from all the nodes.</p> leaf1leaf2leaf3leaf4spine1spine2 <pre><code>--{ + running }--[  ]--\nA:leaf1# info interface ethernet-1/{1,2}\n    interface ethernet-1/1 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.0/31 {\n                }\n            }\n        }\n    }\n    interface ethernet-1/2 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.2/31 {\n                }\n            }\n        }\n    }\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf2# info interface ethernet-1/{1,2}\n    interface ethernet-1/1 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.4/31 {\n                }\n            }\n        }\n    }\n    interface ethernet-1/2 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.6/31 {\n                }\n            }\n        }\n    }\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf3# info interface ethernet-1/{1,2}\n    interface ethernet-1/1 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.8/31 {\n                }\n            }\n        }\n    }\n    interface ethernet-1/2 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.10/31 {\n                }\n            }\n        }\n    }\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf4# info interface ethernet-1/{1,2}\n    interface ethernet-1/1 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.12/31 {\n                }\n            }\n        }\n    }\n    interface ethernet-1/2 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.14/31 {\n                }\n            }\n        }\n    }\n</code></pre> <pre><code>A:spine1# info interface ethernet-1/{1..4}\n    interface ethernet-1/1 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.1/31 {\n                }\n            }\n        }\n    }\n    interface ethernet-1/2 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.5/31 {\n                }\n            }\n        }\n    }\n    interface ethernet-1/3 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.9/31 {\n                }\n            }\n        }\n    }\n    interface ethernet-1/4 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.13/31 {\n                }\n            }\n        }\n    }\n</code></pre> <pre><code>A:spine2# info interface ethernet-1/{1..4}\n    interface ethernet-1/1 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.3/31 {\n                }\n            }\n        }\n    }\n    interface ethernet-1/2 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.7/31 {\n                }\n            }\n        }\n    }\n    interface ethernet-1/3 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.11/31 {\n                }\n            }\n        }\n    }\n    interface ethernet-1/4 {\n        admin-state enable\n        mtu 9100\n        subinterface 0 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 198.51.100.15/31 {\n                }\n            }\n        }\n    }\n</code></pre> Tip <p>Notice that configuration for multiple interfaces are shown with a single command using the concept of ranges. Different ways of doing this are shown with one style used for the leafs and another for the spines. With <code>interface ethernet-1{1,2}</code>, the comma-separation allows the user to enter any set of numbers (contiguous or not), which are subsequently expanded. Thus, this expands to <code>interface ethernet-1/1</code> and <code>interface ethernet-1/2</code>. On the other hand, you can also provide a contiguous range of numbers by using <code>..</code>, as shown for the spines. In that case, <code>interface ethernet-1/{1..4}</code> implies ethernet-1/1 through ethernet-1/4.</p> Note <p>Remember, by default, there is no global routing instance/table in SR Linux. A <code>network-instance</code> of type <code>default</code> must be configured and these interfaces, including the <code>system0</code> interface need to be added to this network instance for point-to-point connectivity.</p>","tags":["containerlab","bgp","evpn","vxlan","srlinux","nokia"]},{"location":"blog/2024/10/09/asymmetric-routing-with-sr-linux-in-evpn-vxlan-fabrics/#underlay-and-overlay-bgp","title":"Underlay and overlay BGP","text":"<p>For the underlay, eBGP is used to advertise the <code>system0</code> interface addresses. However, since SR Linux has adapted eBGP behavior specifically for the L2VPN EVPN AFI/SAFI (no modification of next-hop address at every eBGP hop and the default use of <code>system0</code> interface address as the next-hop when originating a route instead of the Layer 3 interface address over which the peering is formed), we can simply enable this address-family over the same peering (leveraging MP-BGP functionality). BGP is configured under the default <code>network-instance</code> since this is for the underlay in the global routing table. </p> <p>The BGP configuration from all nodes is shown below:</p> leaf1leaf2leaf3leaf4spine1spine2 <pre><code>--{ + running }--[  ]--\nA:leaf1# info network-instance default protocols bgp\n    network-instance default {\n        protocols {\n            bgp {\n                admin-state enable\n                autonomous-system 65411\n                router-id 192.0.2.11\n                afi-safi evpn {\n                    admin-state enable\n                }\n                afi-safi ipv4-unicast {\n                    admin-state enable\n                    multipath {\n                        maximum-paths 2\n                    }\n                }\n                group spine {\n                    peer-as 65500\n                    export-policy [\n                        spine-export\n                    ]\n                    import-policy [\n                        spine-import\n                    ]\n                    afi-safi evpn {\n                        admin-state enable\n                    }\n                    afi-safi ipv4-unicast {\n                        admin-state enable\n                    }\n                }\n                neighbor 198.51.100.1 {\n                    peer-group spine\n                }\n                neighbor 198.51.100.3 {\n                    peer-group spine\n                }\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf2# info network-instance default protocols bgp\n    network-instance default {\n        protocols {\n            bgp {\n                admin-state enable\n                autonomous-system 65412\n                router-id 192.0.2.12\n                afi-safi evpn {\n                    admin-state enable\n                }\n                afi-safi ipv4-unicast {\n                    admin-state enable\n                    multipath {\n                        maximum-paths 2\n                    }\n                }\n                group spine {\n                    peer-as 65500\n                    export-policy [\n                        spine-export\n                    ]\n                    import-policy [\n                        spine-import\n                    ]\n                    afi-safi evpn {\n                        admin-state enable\n                    }\n                    afi-safi ipv4-unicast {\n                        admin-state enable\n                    }\n                }\n                neighbor 198.51.100.5 {\n                    peer-group spine\n                }\n                neighbor 198.51.100.7 {\n                    peer-group spine\n                }\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf3# info network-instance default protocols bgp\n    network-instance default {\n        protocols {\n            bgp {\n                admin-state enable\n                autonomous-system 65413\n                router-id 192.0.2.13\n                afi-safi evpn {\n                    admin-state enable\n                }\n                afi-safi ipv4-unicast {\n                    admin-state enable\n                    multipath {\n                        maximum-paths 2\n                    }\n                }\n                group spine {\n                    peer-as 65500\n                    export-policy [\n                        spine-export\n                    ]\n                    import-policy [\n                        spine-import\n                    ]\n                    afi-safi evpn {\n                        admin-state enable\n                    }\n                    afi-safi ipv4-unicast {\n                        admin-state enable\n                    }\n                }\n                neighbor 198.51.100.9 {\n                    peer-group spine\n                }\n                neighbor 198.51.100.11 {\n                    peer-group spine\n                }\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf4# info network-instance default protocols bgp\n    network-instance default {\n        protocols {\n            bgp {\n                admin-state enable\n                autonomous-system 65414\n                router-id 192.0.2.14\n                afi-safi evpn {\n                    admin-state enable\n                }\n                afi-safi ipv4-unicast {\n                    admin-state enable\n                    multipath {\n                        maximum-paths 2\n                    }\n                }\n                group spine {\n                    peer-as 65500\n                    export-policy [\n                        spine-export\n                    ]\n                    import-policy [\n                        spine-import\n                    ]\n                    afi-safi evpn {\n                        admin-state enable\n                    }\n                    afi-safi ipv4-unicast {\n                        admin-state enable\n                    }\n                }\n                neighbor 198.51.100.13 {\n                    peer-group spine\n                }\n                neighbor 198.51.100.15 {\n                    peer-group spine\n                }\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ running }--[  ]--\nA:spine1# info network-instance default protocols bgp\n    network-instance default {\n        protocols {\n            bgp {\n                admin-state enable\n                autonomous-system 65500\n                router-id 192.0.2.101\n                afi-safi evpn {\n                    admin-state enable\n                    evpn {\n                        inter-as-vpn true\n                    }\n                }\n                afi-safi ipv4-unicast {\n                    admin-state enable\n                }\n                group leaf {\n                    export-policy [\n                        leaf-export\n                    ]\n                    import-policy [\n                        leaf-import\n                    ]\n                    afi-safi evpn {\n                        admin-state enable\n                    }\n                    afi-safi ipv4-unicast {\n                        admin-state enable\n                    }\n                }\n                neighbor 198.51.100.0 {\n                    peer-as 65411\n                    peer-group leaf\n                }\n                neighbor 198.51.100.4 {\n                    peer-as 65412\n                    peer-group leaf\n                }\n                neighbor 198.51.100.8 {\n                    peer-as 65413\n                    peer-group leaf\n                }\n                neighbor 198.51.100.12 {\n                    peer-as 65414\n                    peer-group leaf\n                }\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:spine2# info network-instance default protocols bgp\n    network-instance default {\n        protocols {\n            bgp {\n                admin-state enable\n                autonomous-system 65500\n                router-id 192.0.2.102\n                afi-safi evpn {\n                    admin-state enable\n                    evpn {\n                        inter-as-vpn true\n                    }\n                }\n                afi-safi ipv4-unicast {\n                    admin-state enable\n                }\n                group leaf {\n                    export-policy [\n                        leaf-export\n                    ]\n                    import-policy [\n                        leaf-import\n                    ]\n                    afi-safi evpn {\n                        admin-state enable\n                    }\n                    afi-safi ipv4-unicast {\n                        admin-state enable\n                    }\n                }\n                neighbor 198.51.100.2 {\n                    peer-as 65411\n                    peer-group leaf\n                }\n                neighbor 198.51.100.6 {\n                    peer-as 65412\n                    peer-group leaf\n                }\n                neighbor 198.51.100.10 {\n                    peer-as 65413\n                    peer-group leaf\n                }\n                neighbor 198.51.100.14 {\n                    peer-as 65414\n                    peer-group leaf\n                }\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> Note <p>On the spines, the configuration option <code>inter-as-vpn</code> must be set to <code>true</code> under the <code>protocols bgp afi-safi evpn evpn</code> hierarchy. Since the spines are not configured as VTEPs and act as pure IP forwarders in this design, there are no Layer 2 or Layer 3 VXLAN constructs created on the spines, associated to any route targets for EVPN route import. By default, such routes (which have no local route target for import) will be rejected and not advertised to other leafs. The <code>inter-as-vpn</code> configuration option overrides this behavior.</p> <p>The BGP configuration defines a peer-group called <code>spine</code> on the leafs and <code>leaf</code> on the spines to build out common configuration that can be applied across multiple neighbors. These peer-groups enable both the IPv4-unicast and EVPN address-families, using MP-BGP to establish a single peering for both families. In addition to this, <code>export</code> and <code>import</code> policies are defined, controlling what routes are exported and imported. </p> <p>The following packet capture also confirms the MP-BGP capabilities exchanged with the BGP OPEN messages, where both IPv4 unicast and L2VPN EVPN capabilities are advertised:</p> <p></p>","tags":["containerlab","bgp","evpn","vxlan","srlinux","nokia"]},{"location":"blog/2024/10/09/asymmetric-routing-with-sr-linux-in-evpn-vxlan-fabrics/#routing-policies-for-the-underlay-and-overlay","title":"Routing policies for the underlay and overlay","text":"<p>The configuration of the routing policies used for export and import of BGP routes is shown below. Since the policies for the leafs are the same across all leafs and the policies for the spines are the same across all spines, the configuration is only shown from two nodes, leaf1 and spine1, using them as references.</p> leaf1spine1 <pre><code>--{ + running }--[  ]--\nA:leaf1# info routing-policy policy spine-*\n    routing-policy {\n        policy spine-export {\n            default-action {\n                policy-result reject\n            }\n            statement loopback {\n                match {\n                    protocol local\n                }\n                action {\n                    policy-result accept\n                }\n            }\n            statement allow-evpn {\n                match {\n                    family [\n                        evpn\n                    ]\n                }\n                action {\n                    policy-result accept\n                }\n            }\n        }\n        policy spine-import {\n            default-action {\n                policy-result reject\n            }\n            statement bgp-underlay {\n                match {\n                    protocol bgp\n                    family [\n                        ipv4-unicast\n                        ipv6-unicast\n                    ]\n                }\n                action {\n                    policy-result accept\n                }\n            }\n            statement bgp-evpn-overlay {\n                match {\n                    family [\n                        evpn\n                    ]\n                }\n                action {\n                    policy-result accept\n                }\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ running }--[  ]--\nA:spine1# info routing-policy policy leaf-*\n    routing-policy {\n        policy leaf-export {\n            default-action {\n                policy-result reject\n            }\n            statement loopback {\n                match {\n                    protocol local\n                }\n                action {\n                    policy-result accept\n                }\n            }\n            statement bgp-underlay {\n                match {\n                    protocol bgp\n                    family [\n                        ipv4-unicast\n                        ipv6-unicast\n                    ]\n                }\n                action {\n                    policy-result accept\n                }\n            }\n            statement bgp-evpn-overlay {\n                match {\n                    family [\n                        evpn\n                    ]\n                }\n                action {\n                    policy-result accept\n                }\n            }\n        }\n        policy leaf-import {\n            default-action {\n                policy-result reject\n            }\n            statement bgp-underlay {\n                match {\n                    protocol bgp\n                    family [\n                        ipv4-unicast\n                        ipv6-unicast\n                    ]\n                }\n                action {\n                    policy-result accept\n                }\n            }\n            statement bgp-evpn-overlay {\n                match {\n                    family [\n                        evpn\n                    ]\n                }\n                action {\n                    policy-result accept\n                }\n            }\n        }\n    }\n--{ running }--[  ]--\n</code></pre> Tip <p>Similar to how ranges can be used to pull configuration state from multiple interfaces as an example, in this case a wildcard <code>*</code> is used to select multiple routing-policies. The wildcard <code>spine-*</code> matches both policies named <code>spine-import</code> and <code>spine-export</code>.</p>","tags":["containerlab","bgp","evpn","vxlan","srlinux","nokia"]},{"location":"blog/2024/10/09/asymmetric-routing-with-sr-linux-in-evpn-vxlan-fabrics/#host-connectivity-and-esi-lag","title":"Host connectivity and ESI LAG","text":"<p>With BGP configured, we can start to deploy the connectivity to the servers and configure the necessary VXLAN constructs for end-to-end connectivity. The interfaces, to the servers, are configured as untagged interfaces. Since host h2 is multi-homed to leaf2 and leaf3, this segment is configured as an ESI LAG. This includes:</p> <ol> <li>Mapping the physical interface to a LAG interface (<code>lag1</code>, in this case).</li> <li>The LAG interface configured with the required LACP properties - mode <code>active</code> and a system-mac of <code>00:00:00:00:23:23</code>. This LAG interface is also configured with a subinterface of type <code>bridged</code>.</li> <li>An Ethernet Segment defined under the <code>system network-instance protocols evpn ethernet-segments</code> hierarchy.</li> </ol> leaf1leaf2leaf3leaf4 <pre><code>--{ + running }--[  ]--\nA:leaf1# info interface ethernet-1/3\n    interface ethernet-1/3 {\n        admin-state enable\n        mtu 9100\n        vlan-tagging false\n        subinterface 0 {\n            type bridged\n            admin-state enable\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf2# info interface ethernet-1/3\n    interface ethernet-1/3 {\n        admin-state enable\n        ethernet {\n            aggregate-id lag1\n        }\n    }\n--{ + running }--[  ]--\n\n--{ + running }--[  ]--\nA:leaf2# info interface lag1\n    interface lag1 {\n        admin-state enable\n        vlan-tagging false\n        subinterface 0 {\n            type bridged\n            admin-state enable\n        }\n        lag {\n            lag-type lacp\n            lacp {\n                lacp-mode ACTIVE\n                system-id-mac 00:00:00:00:23:23\n            }\n        }\n    }\n--{ + running }--[  ]--\n\n--{ + running }--[  ]--\nA:leaf2# info system network-instance protocols evpn\n    system {\n        network-instance {\n            protocols {\n                evpn {\n                    ethernet-segments {\n                        bgp-instance 1 {\n                            ethernet-segment es1 {\n                                admin-state enable\n                                esi 00:00:11:11:11:11:11:11:23:23\n                                multi-homing-mode all-active\n                                interface lag1 {\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf3# info interface ethernet-1/3\n    interface ethernet-1/3 {\n        admin-state enable\n        ethernet {\n            aggregate-id lag1\n        }\n    }\n--{ + running }--[  ]--\n\nA:leaf3# info interface lag1\n    interface lag1 {\n        admin-state enable\n        vlan-tagging false\n        subinterface 0 {\n            type bridged\n            admin-state enable\n        }\n        lag {\n            lag-type lacp\n            lacp {\n                lacp-mode ACTIVE\n                system-id-mac 00:00:00:00:23:23\n            }\n        }\n    }\n--{ + running }--[  ]--\n\n--{ + running }--[  ]--\nA:leaf3# info system network-instance protocols evpn\n    system {\n        network-instance {\n            protocols {\n                evpn {\n                    ethernet-segments {\n                        bgp-instance 1 {\n                            ethernet-segment es1 {\n                                admin-state enable\n                                esi 00:00:11:11:11:11:11:11:23:23\n                                multi-homing-mode all-active\n                                interface lag1 {\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf4# info interface ethernet-1/3\n    interface ethernet-1/3 {\n        admin-state enable\n        vlan-tagging false\n        subinterface 0 {\n            type bridged\n            admin-state enable\n        }\n    }\n--{ + running }--[  ]--\n</code></pre>","tags":["containerlab","bgp","evpn","vxlan","srlinux","nokia"]},{"location":"blog/2024/10/09/asymmetric-routing-with-sr-linux-in-evpn-vxlan-fabrics/#vxlan-tunnel-interfaces","title":"VXLAN tunnel interfaces","text":"<p>On each leaf, VXLAN tunnel-interfaces are created next. In this case, two logical interfaces are created, one for VNI 10010 and another for VNI 10020 (since this is asymmetric routing, all VNIs must exist on all leafs that want to route between the respective VNIs). Since the end-goal is to have host h1 communicate with h2 and h3, only leaf1 and leaf4 are configured with VNI 10020 as well, while leaf2 and leaf3 are only configured with VNI 10010. </p> leaf1leaf2leaf3leaf4 <pre><code>A:leaf1# info tunnel-interface *\n    tunnel-interface vxlan1 {\n        vxlan-interface 1 {\n            type bridged\n            ingress {\n                vni 10010\n            }\n        }\n        vxlan-interface 2 {\n            type bridged\n            ingress {\n                vni 10020\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf2# info tunnel-interface *\n    tunnel-interface vxlan1 {\n        vxlan-interface 1 {\n            type bridged\n            ingress {\n                vni 10010\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf3# info tunnel-interface *\n    tunnel-interface vxlan1 {\n        vxlan-interface 1 {\n            type bridged\n            ingress {\n                vni 10010\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf4# info tunnel-interface *\n    tunnel-interface vxlan1 {\n        vxlan-interface 1 {\n            type bridged\n            ingress {\n                vni 10010\n            }\n        }\n        vxlan-interface 2 {\n            type bridged\n            ingress {\n                vni 10020\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre>","tags":["containerlab","bgp","evpn","vxlan","srlinux","nokia"]},{"location":"blog/2024/10/09/asymmetric-routing-with-sr-linux-in-evpn-vxlan-fabrics/#irbs-on-the-leafs","title":"IRBs on the leafs","text":"<p>IRBs are deployed using an anycast, distributed gateway model, impplying that all leafs are configured with the same IP address and MAC address for a specific IRB subinterface. These IRB subinterfaces act as the default gateway for the endpoints. For our topology, we will create two subinterfaces <code>irb0.10</code> and <code>irb0.20</code> corresponding to hosts mapped to VNIs 10010 and 10020, respectively. The configuration of these IRB interfaces is shown below:</p> leaf1leaf2leaf3leaf4 <pre><code>--{ + running }--[  ]--\nA:leaf1# info interface irb0\n    interface irb0 {\n        admin-state enable\n        subinterface 10 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 172.16.10.254/24 {\n                    anycast-gw true\n                }\n                arp {\n                    learn-unsolicited true\n                    proxy-arp true\n                    host-route {\n                        populate dynamic {\n                        }\n                        populate evpn {\n                        }\n                    }\n                    evpn {\n                        advertise dynamic {\n                        }\n                    }\n                }\n            }\n            anycast-gw {\n                anycast-gw-mac 00:00:5E:00:53:00\n            }\n        }\n        subinterface 20 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 172.16.20.254/24 {\n                    anycast-gw true\n                }\n                arp {\n                    learn-unsolicited true\n                    host-route {\n                        populate dynamic {\n                        }\n                        populate evpn {\n                        }\n                    }\n                    evpn {\n                        advertise dynamic {\n                        }\n                    }\n                }\n            }\n            anycast-gw {\n                anycast-gw-mac 00:00:5E:00:53:00\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf2# info interface irb0\n    interface irb0 {\n        admin-state enable\n        subinterface 10 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 172.16.10.254/24 {\n                    anycast-gw true\n                }\n                arp {\n                    learn-unsolicited true\n                    proxy-arp true\n                    host-route {\n                        populate dynamic {\n                        }\n                        populate evpn {\n                        }\n                    }\n                    evpn {\n                        advertise dynamic {\n                        }\n                    }\n                }\n            }\n            anycast-gw {\n                anycast-gw-mac 00:00:5E:00:53:00\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf2# info interface irb0\n    interface irb0 {\n        admin-state enable\n        subinterface 10 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 172.16.10.254/24 {\n                    anycast-gw true\n                }\n                arp {\n                    learn-unsolicited true\n                    proxy-arp true\n                    host-route {\n                        populate dynamic {\n                        }\n                        populate evpn {\n                        }\n                    }\n                    evpn {\n                        advertise dynamic {\n                        }\n                    }\n                }\n            }\n            anycast-gw {\n                anycast-gw-mac 00:00:5E:00:53:00\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf2# info interface irb0\n    interface irb0 {\n        admin-state enable\n        subinterface 10 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 172.16.10.254/24 {\n                    anycast-gw true\n                }\n                arp {\n                    learn-unsolicited true\n                    proxy-arp true\n                    host-route {\n                        populate dynamic {\n                        }\n                        populate evpn {\n                        }\n                    }\n                    evpn {\n                        advertise dynamic {\n                        }\n                    }\n                }\n            }\n            anycast-gw {\n                anycast-gw-mac 00:00:5E:00:53:00\n            }\n        }\n        subinterface 20 {\n            admin-state enable\n            ipv4 {\n                admin-state enable\n                address 172.16.20.254/24 {\n                    anycast-gw true\n                }\n                arp {\n                    learn-unsolicited true\n                    host-route {\n                        populate dynamic {\n                        }\n                        populate evpn {\n                        }\n                    }\n                    evpn {\n                        advertise dynamic {\n                        }\n                    }\n                }\n            }\n            anycast-gw {\n                anycast-gw-mac 00:00:5E:00:53:00\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <p>There is a lot going on here, so let's breakdown some of the configuration options:</p> <code>anycast-gw [true|false]</code> <p>When this is set to <code>true</code>, the IPv4 address is associated to the anycast gateway MAC address and this MAC address is used to respond to any ARP requests for that IPv4 address. This also allows the same IPv4 address to be configured on other nodes for the same broadcast domain, essentially suppressing duplicate IP detection.</p> <code>anycast-gw anycast-gw-mac [mac-address]</code> <p>The MAC address configured with this option is the anycast gateway MAC address and is associated to the IP address for that subinterface. If this is ommitted, the anycast gateway MAC address is auto-derived from the VRRP MAC address group range.</p> <code>arp learn-unsolicited [true|false]</code> <p>This enables the node to learn the IP-to-MAC binding from any ARP packet and not just ARP requests.</p> <code>arp host-route populate [dynamic|static|evpn]</code> <p>This enables the node to insert a host route (/32 for IPv4 and /128 for IPv6) in the routing table from dynaimc, static or EVPN-learnt ARP entries.</p> <code>arp evpn advertise [dynamic|static]</code> <p>This enables the node to advertise EVPN Type-2 MAC+IP routes from dynamic or static ARP entries.</p>","tags":["containerlab","bgp","evpn","vxlan","srlinux","nokia"]},{"location":"blog/2024/10/09/asymmetric-routing-with-sr-linux-in-evpn-vxlan-fabrics/#mac-vrfs-on-leafs","title":"MAC VRFs on leafs","text":"<p>Finally, MAC VRFs are created on the leafs to create a broadcast domain and corresponding bridge table for Layer 2 learning. Since, by default, a MAC VRF corresponds to a single broadcast domain and bridge table, we can map only one Layer 2 VNI to it. Thus, on leaf1 and leaf4, two MAC VRFs are created - one for VNI 10010 and another for VNI 10020. Under the MAC VRF, there are several important things to consider:</p> <ul> <li>The Layer 2 subinterface is bound to the MAC VRF using the <code>interface</code> configuration option.</li> <li>The corresponding IRB subinterface is bound to the MAC VRF using the <code>interface</code> configuration option.</li> <li>The VXLAN tunnel subinterface is bound to the MAC VRF using the <code>vxlan-interface</code> configuration option.</li> <li>BGP EVPN learning is enabled for the MAC VRF using the <code>protocols bgp-evpn</code> hierarchy and the MAC VRF is bound to an EVI (EVPN virtual instance).</li> <li>The <code>ecmp</code> configuration option determines how many VTEPs can be considered for load-balancing by the local VTEP (more on this in the validation section).</li> <li>Route distinguishers and route targets are configured for the MAC VRF using the <code>protocols bgp-vpn</code> hierarchy.</li> </ul> leaf1leaf2leaf3leaf4 <pre><code>--{ + running }--[  ]--\nA:leaf1# info network-instance macvrf*\n    network-instance macvrf1 {\n        type mac-vrf\n        admin-state enable\n        interface ethernet-1/3.0 {\n        }\n        interface irb0.10 {\n        }\n        vxlan-interface vxlan1.1 {\n        }\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.1\n                    evi 10\n                    ecmp 2\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-distinguisher {\n                        rd 192.0.2.11:1\n                    }\n                    route-target {\n                        export-rt target:10:10\n                        import-rt target:10:10\n                    }\n                }\n            }\n        }\n    }\n    network-instance macvrf2 {\n        type mac-vrf\n        admin-state enable\n        interface irb0.20 {\n        }\n        vxlan-interface vxlan1.2 {\n        }\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.2\n                    evi 20\n                    ecmp 2\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-distinguisher {\n                        rd 192.0.2.11:2\n                    }\n                    route-target {\n                        export-rt target:20:20\n                        import-rt target:20:20\n                    }\n                }\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf2# info network-instance macvrf1\n    network-instance macvrf1 {\n        type mac-vrf\n        admin-state enable\n        interface irb0.10 {\n        }\n        interface lag1.0 {\n        }\n        vxlan-interface vxlan1.1 {\n        }\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.1\n                    evi 10\n                    ecmp 2\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-distinguisher {\n                        rd 192.0.2.12:1\n                    }\n                    route-target {\n                        export-rt target:10:10\n                        import-rt target:10:10\n                    }\n                }\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf3# info network-instance macvrf1\n    network-instance macvrf1 {\n        type mac-vrf\n        admin-state enable\n        interface irb0.10 {\n        }\n        interface lag1.0 {\n        }\n        vxlan-interface vxlan1.1 {\n        }\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.1\n                    evi 10\n                    ecmp 2\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-distinguisher {\n                        rd 192.0.2.13:1\n                    }\n                    route-target {\n                        export-rt target:10:10\n                        import-rt target:10:10\n                    }\n                }\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <pre><code>--{ + running }--[  ]--\nA:leaf4# info network-instance macvrf*\n    network-instance macvrf1 {\n        type mac-vrf\n        admin-state enable\n        interface irb0.10 {\n        }\n        vxlan-interface vxlan1.1 {\n        }\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.1\n                    evi 10\n                    ecmp 2\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-distinguisher {\n                        rd 192.0.2.14:1\n                    }\n                    route-target {\n                        export-rt target:10:10\n                        import-rt target:10:10\n                    }\n                }\n            }\n        }\n    }\n    network-instance macvrf2 {\n        type mac-vrf\n        admin-state enable\n        interface ethernet-1/3.0 {\n        }\n        interface irb0.20 {\n        }\n        vxlan-interface vxlan1.2 {\n        }\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.2\n                    evi 20\n                    ecmp 2\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-distinguisher {\n                        rd 192.0.2.14:2\n                    }\n                    route-target {\n                        export-rt target:20:20\n                        import-rt target:20:20\n                    }\n                }\n            }\n        }\n    }\n--{ + running }--[  ]--\n</code></pre> <p>This completes the configuration walkthrough section of this post. Next, we'll cover the control plane and data plane validation.</p>","tags":["containerlab","bgp","evpn","vxlan","srlinux","nokia"]},{"location":"blog/2024/10/09/asymmetric-routing-with-sr-linux-in-evpn-vxlan-fabrics/#control-plane-data-plane-validation","title":"Control plane &amp; data plane validation","text":"<p>When the hosts come online, they typically send a GARP to ensure there is no duplicate IP address in their broadcast domain. This enables the locally attached leafs to learn the IP-to-MAC binding and build an ARP entry in the ARP cache table (since the <code>arp learn-unsolicited</code> configuration option is set to <code>true</code>). This, in turn, is advertised as an EVPN Type-2 MAC+IP route for remote leafs to learn this as well and eventually insert the IP-to-MAC binding as an entry in their ARP caches. </p> <p>On leaf1, we can confirm that it has learnt the IP-to-MAC binding for host h1 (locally attached) and h3 (attached to remote leaf, leaf4). </p> <pre><code>A:leaf1# show arpnd arp-entries interface irb0\n+-------------------+-------------------+-----------------+-------------------+-------------------------------------+------------------------------------------------------------------------+\n|     Interface     |   Subinterface    |    Neighbor     |      Origin       |         Link layer address          |                                 Expiry                                 |\n+===================+===================+=================+===================+=====================================+========================================================================+\n| irb0              |                10 |     172.16.10.1 |           dynamic | AA:C1:AB:CA:A0:83                   | 3 hours from now                                                       |\n| irb0              |                10 |     172.16.10.2 |              evpn | AA:C1:AB:11:BE:88                   |                                                                        |\n| irb0              |                20 |     172.16.20.3 |              evpn | AA:C1:AB:9F:EF:E2                   |                                                                        |\n+-------------------+-------------------+-----------------+-------------------+-------------------------------------+------------------------------------------------------------------------+\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  Total entries : 3 (0 static, 3 dynamic)\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n--{ + candidate shared default }--[  ]--\n</code></pre> <p>The ARP entry for host h3 (172.16.20.3) is learnt via the EVPN Type-2 MAC+IP route received from leaf4, as shown below.</p> <pre><code>--{ + running }--[  ]--\nA:leaf1# show network-instance default protocols bgp routes evpn route-type 2 ip-address 172.16.20.3 detail\n---------------------------------------------------------------------------------------------------------------------------\nShow report for the EVPN routes in network-instance  \"default\"\n---------------------------------------------------------------------------------------------------------------------------\nRoute Distinguisher: 192.0.2.14:2\nTag-ID             : 0\nMAC address        : AA:C1:AB:9F:EF:E2\nIP Address         : 172.16.20.3\nneighbor           : 198.51.100.1\nReceived paths     : 1\n  Path 1: &lt;Best,Valid,Used,&gt;\n    ESI               : 00:00:00:00:00:00:00:00:00:00\n    Label             : 10020\n    Route source      : neighbor 198.51.100.1 (last modified 4d18h49m3s ago)\n    Route preference  : No MED, No LocalPref\n    Atomic Aggr       : false\n    BGP next-hop      : 192.0.2.14\n    AS Path           :  i [65500, 65414]\n    Communities       : [target:20:20, bgp-tunnel-encap:VXLAN]\n    RR Attributes     : No Originator-ID, Cluster-List is []\n    Aggregation       : None\n    Unknown Attr      : None\n    Invalid Reason    : None\n    Tie Break Reason  : none\n  Path 1 was advertised to (Modified Attributes):\n  [ 198.51.100.3 ]\n    Route preference  : No MED, No LocalPref\n    Atomic Aggr       : false\n    BGP next-hop      : 192.0.2.14\n    AS Path           :  i [65411, 65500, 65414]\n    Communities       : [target:20:20, bgp-tunnel-encap:VXLAN]\n    RR Attributes     : No Originator-ID, Cluster-List is []\n    Aggregation       : None\n    Unknown Attr      : None\n---------------------------------------------------------------------------------------------------------------------------\nRoute Distinguisher: 192.0.2.14:2\nTag-ID             : 0\nMAC address        : AA:C1:AB:9F:EF:E2\nIP Address         : 172.16.20.3\nneighbor           : 198.51.100.3\nReceived paths     : 1\n  Path 1: &lt;Valid,&gt;\n    ESI               : 00:00:00:00:00:00:00:00:00:00\n    Label             : 10020\n    Route source      : neighbor 198.51.100.3 (last modified 4d18h49m0s ago)\n    Route preference  : No MED, No LocalPref\n    Atomic Aggr       : false\n    BGP next-hop      : 192.0.2.14\n    AS Path           :  i [65500, 65414]\n    Communities       : [target:20:20, bgp-tunnel-encap:VXLAN]\n    RR Attributes     : No Originator-ID, Cluster-List is []\n    Aggregation       : None\n    Unknown Attr      : None\n    Invalid Reason    : None\n    Tie Break Reason  : peer-router-id\n---------------------------------------------------------------------------------------------------------------------------\n--{ + running }--[  ]--\n</code></pre> <p>This is an important step for asymmetric routing. Consider a situation where host h1 wants to communicate with h3. When the IP packet hits leaf1, it will attempt to resolve the destination IP address via an ARP request, as it is directly attached locally (via the <code>irb.20</code> interface), as shown below. </p> <pre><code>--{ + running }--[  ]--\nA:leaf1# show network-instance default route-table ipv4-unicast prefix 172.16.20.0/24\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nIPv4 unicast route table of network instance default\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n+---------------------------+-------+------------+----------------------+----------+----------+---------+------------+-----------------+-----------------+-----------------+----------------------+\n|          Prefix           |  ID   | Route Type |     Route Owner      |  Active  |  Origin  | Metric  |    Pref    | Next-hop (Type) |    Next-hop     | Backup Next-hop |   Backup Next-hop    |\n|                           |       |            |                      |          | Network  |         |            |                 |    Interface    |     (Type)      |      Interface       |\n|                           |       |            |                      |          | Instance |         |            |                 |                 |                 |                      |\n+===========================+=======+============+======================+==========+==========+=========+============+=================+=================+=================+======================+\n| 172.16.20.0/24            | 10    | local      | net_inst_mgr         | True     | default  | 0       | 0          | 172.16.20.254   | irb0.20         |                 |                      |\n|                           |       |            |                      |          |          |         |            | (direct)        |                 |                 |                      |\n+---------------------------+-------+------------+----------------------+----------+----------+---------+------------+-----------------+-----------------+-----------------+----------------------+\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n--{ + running }--[  ]--\n</code></pre> <p>Since this IRB interface exists on leaf4 as well, the ARP reply will be consumed by it, never reaching leaf1, and thus, creating a failure in the ARP process. To circumvent this problem associated with an anycast, distributed IRB model, the EVPN Type-2 MAC+IP routes are used to populate the ARP cache. In addition to this, optionally, this EVPN-learnt ARP entry can also be used to inject a host route (/32 for IPv4 and /128 for IPv6) into the routing table using the <code>arp host-route populate evpn</code> configuration option (as discussed earlier). Since this is enabled in our case, we can confirm that the route 172.16.20.3/32 exists in the routing table, inserted by the arp_nd_mgr process:</p> <pre><code>--{ + running }--[  ]--\nA:leaf1# show network-instance default route-table ipv4-unicast prefix 172.16.20.3/32\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nIPv4 unicast route table of network instance default\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n+---------------------------+-------+------------+----------------------+----------+----------+---------+------------+-----------------+-----------------+-----------------+----------------------+\n|          Prefix           |  ID   | Route Type |     Route Owner      |  Active  |  Origin  | Metric  |    Pref    | Next-hop (Type) |    Next-hop     | Backup Next-hop |   Backup Next-hop    |\n|                           |       |            |                      |          | Network  |         |            |                 |    Interface    |     (Type)      |      Interface       |\n|                           |       |            |                      |          | Instance |         |            |                 |                 |                 |                      |\n+===========================+=======+============+======================+==========+==========+=========+============+=================+=================+=================+======================+\n| 172.16.20.3/32            | 10    | arp-nd     | arp_nd_mgr           | True     | default  | 0       | 1          | 172.16.20.3     | irb0.20         |                 |                      |\n|                           |       |            |                      |          |          |         |            | (direct)        |                 |                 |                      |\n+---------------------------+-------+------------+----------------------+----------+----------+---------+------------+-----------------+-----------------+-----------------+----------------------+\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n--{ + running }--[  ]--\n</code></pre> Note <p>The <code>arp host-route populate evpn</code> configuration option is purely a design choice. Since a routing lookup is based on the longest-prefix-match logic (where the longest prefix wins), the existence of the host routes ensure that when there is a routing lookup for the destination, the host route is selected instead of falling back to the subnet route, which relies on ARP resolution, making the forwarding process more efficient. However, this also implies that a host route is created for every EVPN-learnt ARP entry, which can lead to a large routing table, potentially creating an issue in large-scale fabrics.</p> <p>Let's consider two flows to understand the data plane forwarding in such a design - host h1 communicating with h2 (same subnet) and h1 communicating with h3 (different subnet). </p> <p>Since h1 is in the same subnet as h2, when communicating with h2, h1 will try to resolve its IP address directly via an ARP request. This is received on leaf1 and leaked to the CPU via <code>irb0.10</code>. Since L2 proxy-arp is not enabled, the <code>arp_nd_mgr</code> process picks up the ARP request and responds back using its own anycast gateway MAC address while suppressing the ARP request from being flooded in the fabric. A packet capture of this ARP reply is shown below.</p> <p></p> <p>Once this ARP process completes, host h1 generates an ICMP request (since we are testing communication between hosts using the <code>ping</code> tool). When this IP packet arrives on leaf1, it does a routing lookup (since the destination MAC address is owned by itself) and this routing lookup will either hit the 172.16.10.0/24 prefix or the more-specific 172.16.10.2/32 prefix (installed from the ARP entry via the EVPN Type-2 MAC+IP route), as shown below. Since this is a directly attached route, it is further resolved into a MAC address via the ARP table and then the packet is bridged towards the destination. This MAC address points to an Ethernet Segment, which in turn resolves into VTEPs 192.0.2.12 and 192.0.2.13.</p> <pre><code>A:leaf1# show network-instance default route-table ipv4-unicast route 172.16.10.2\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nIPv4 unicast route table of network instance default\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n+------------------------+-------+------------+----------------------+----------+----------+---------+------------+---------------+---------------+---------------+------------------+\n|         Prefix         |  ID   | Route Type |     Route Owner      |  Active  |  Origin  | Metric  |    Pref    |   Next-hop    |   Next-hop    | Backup Next-  | Backup Next-hop  |\n|                        |       |            |                      |          | Network  |         |            |    (Type)     |   Interface   |  hop (Type)   |    Interface     |\n|                        |       |            |                      |          | Instance |         |            |               |               |               |                  |\n+========================+=======+============+======================+==========+==========+=========+============+===============+===============+===============+==================+\n| 172.16.10.2/32         | 8     | arp-nd     | arp_nd_mgr           | True     | default  | 0       | 1          | 172.16.10.2   | irb0.10       |               |                  |\n|                        |       |            |                      |          |          |         |            | (direct)      |               |               |                  |\n+------------------------+-------+------------+----------------------+----------+----------+---------+------------+---------------+---------------+---------------+------------------+\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n--{ + candidate shared default }--[  ]--\n\nA:leaf1# show arpnd arp-entries interface irb0 ipv4-address 172.16.10.2\n+------------------+------------------+-----------------+------------------+-----------------------------------+--------------------------------------------------------------------+\n|    Interface     |   Subinterface   |    Neighbor     |      Origin      |        Link layer address         |                               Expiry                               |\n+==================+==================+=================+==================+===================================+====================================================================+\n| irb0             |               10 |     172.16.10.2 |             evpn | AA:C1:AB:11:BE:88                 |                                                                    |\n+------------------+------------------+-----------------+------------------+-----------------------------------+--------------------------------------------------------------------+\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  Total entries : 1 (0 static, 1 dynamic)\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n--{ + candidate shared default }--[  ]--\n\n--{ + candidate shared default }--[  ]--\nA:leaf1# show network-instance macvrf1 bridge-table mac-table mac AA:C1:AB:11:BE:88\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nMac-table of network instance macvrf1\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nMac                     : AA:C1:AB:11:BE:88\nDestination             : vxlan-interface:vxlan1.1 esi:00:00:11:11:11:11:11:11:23:23\nDest Index              : 322085950259\nType                    : evpn\nProgramming Status      : Success\nAging                   : N/A\nLast Update             : 2024-10-14T05:37:52.000Z\nDuplicate Detect time   : N/A\nHold down time remaining: N/A\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n--{ + candidate shared default }--[  ]--\n\nA:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination | grep -A 7 \"Ethernet Segment Destinations\"\nEthernet Segment Destinations\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n+-------------------------------+-------------------+------------------------+-----------------------------+\n|              ESI              | Destination-index |         VTEPs          | Number MACs (Active/Failed) |\n+===============================+===================+========================+=============================+\n| 00:00:11:11:11:11:11:11:23:23 | 322085950259      | 192.0.2.12, 192.0.2.13 | 1(1/0)                      |\n+-------------------------------+-------------------+------------------------+-----------------------------+\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n--{ + candidate shared default }--[  ]--\n</code></pre> <p>A packet capture of the in-flight packet (as leaf1 sends it to spine1) is shown below, which confirms that the packet ICMP request is VXLAN-encapsulated with a VNI of 10010. It also confirms that because of the L3 proxy-arp approach to suppressing ARPs in an EVPN VXLAN fabric, the source MAC address in the inner Ethernet header is the anycast gateway MAC address.</p> <p></p> <p>The communication between host h1 and h3 follows a similar pattern - the packet is received in macvrf1, mapped VNI 10010, and since the destination MAC address is the anycast MAC address owned by leaf1, it is then routed locally into VNI 10020 (since <code>irb0.20</code> is locally attached) and then bridged across to the destination, as confirmed below:</p> <pre><code>--{ + running }--[  ]--\nA:leaf1# show network-instance default route-table ipv4-unicast route 172.16.20.3\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nIPv4 unicast route table of network instance default\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n+------------------------+-------+------------+----------------------+----------+----------+---------+------------+---------------+---------------+---------------+------------------+\n|         Prefix         |  ID   | Route Type |     Route Owner      |  Active  |  Origin  | Metric  |    Pref    |   Next-hop    |   Next-hop    | Backup Next-  | Backup Next-hop  |\n|                        |       |            |                      |          | Network  |         |            |    (Type)     |   Interface   |  hop (Type)   |    Interface     |\n|                        |       |            |                      |          | Instance |         |            |               |               |               |                  |\n+========================+=======+============+======================+==========+==========+=========+============+===============+===============+===============+==================+\n| 172.16.20.3/32         | 10    | arp-nd     | arp_nd_mgr           | True     | default  | 0       | 1          | 172.16.20.3   | irb0.20       |               |                  |\n|                        |       |            |                      |          |          |         |            | (direct)      |               |               |                  |\n+------------------------+-------+------------+----------------------+----------+----------+---------+------------+---------------+---------------+---------------+------------------+\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n--{ + running }--[  ]--\n\n--{ + running }--[  ]--\nA:leaf1# show network-instance * bridge-table mac-table mac AA:C1:AB:9F:EF:E2\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nMac-table of network instance macvrf2\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nMac                     : AA:C1:AB:9F:EF:E2\nDestination             : vxlan-interface:vxlan1.2 vtep:192.0.2.14 vni:10020\nDest Index              : 322085950242\nType                    : evpn\nProgramming Status      : Success\nAging                   : N/A\nLast Update             : 2024-10-14T01:05:54.000Z\nDuplicate Detect time   : N/A\nHold down time remaining: N/A\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n--{ + running }--[  ]--\n</code></pre> Tip <p>Notice how the previous output used a wildcard for the network-instance name instead of a specific name (<code>show network-instance * bridge-table ...</code>). This is useful since the operator may not always know exactly which MAC VRF is used for forwarding, and thus, the wildcard traverses across all to determine where the MAC address is learned.</p> <p>The following packet capture confirms that the in-flight packet has been routed on the ingress leaf itself (leaf1) and the VNI, in the VXLAN header, is 10020.</p> <p></p>","tags":["containerlab","bgp","evpn","vxlan","srlinux","nokia"]},{"location":"blog/2024/10/09/asymmetric-routing-with-sr-linux-in-evpn-vxlan-fabrics/#summary","title":"Summary","text":"<p>Asymmetric routing uses a <code>bridge-route-bridge</code> model where the packet, from the source, is bridged into the ingress leaf's L2 domain, routed into the destination VLAN/VNI and the bridged across the VXLAN fabric to the destination. </p> <p>Such a model requires the existence of both source and destination IRBs and L2 bridge domains (and L2 VNIs) to exist on all leafs that want to participate in routing between the VNIs. While this is operationally simpler, it does add additional state since all leafs will have to maintain all IP-to-MAC bindings (in the ARP table) and all MAC addresses in the bridge table.</p>","tags":["containerlab","bgp","evpn","vxlan","srlinux","nokia"]},{"location":"labs/","title":"Learning labs","text":"<p>Under construction</p> <p>Labs are in the process of being built.</p>"},{"location":"references/","title":"Learning references and recommendations","text":"<p>Under construction</p> <p>More references will be added soon</p>"},{"location":"references/#blogs-and-videos-on-demand","title":"Blogs and videos on demand","text":"<p>Russ White's blog Russ White's academy Large-scale network design Learning DNS How Networks Really Work Fundamentals of IP Multicast</p>"},{"location":"references/#books","title":"Books","text":"<p>Computer Networking Problems and Solutions Modern Network Observability Network Automation with Nautobot BGP in the Data Center EVPN in the Data Center Cloud Native Data Center Networking The Art of Network Architecture Optimal Routing Design Routing TCP/IP Volume 1 Routing TCP/IP Volume 2 IP Multicast Volume 1 OSPF and IS-IS: choosing an IGP for large-scale networks Network Warrior</p>"},{"location":"blog/archive/2024/","title":"October 2024","text":""},{"location":"blog/archive/2022/","title":"December 2022","text":""},{"location":"blog/archive/2021/","title":"December 2021","text":""},{"location":"blog/category/containerlab/","title":"containerlab","text":""},{"location":"blog/category/bgp/","title":"bgp","text":""},{"location":"blog/category/evpn/","title":"evpn","text":""},{"location":"blog/category/vxlan/","title":"vxlan","text":""},{"location":"blog/category/srlinux/","title":"srlinux","text":""},{"location":"blog/category/nokia/","title":"nokia","text":""},{"location":"blog/category/junos/","title":"junos","text":""},{"location":"blog/category/cisco/","title":"cisco","text":""},{"location":"blog/category/nxos/","title":"nxos","text":""},{"location":"blog/category/multivendor/","title":"multivendor","text":""},{"location":"blog/category/juniper/","title":"juniper","text":""},{"location":"blog/category/apstra/","title":"apstra","text":""},{"location":"blog/category/cumulus/","title":"cumulus","text":""},{"location":"blog/category/arista/","title":"arista","text":""},{"location":"blog/category/sda/","title":"sda","text":""},{"location":"blog/category/lisp/","title":"lisp","text":""},{"location":"blog/category/mlag/","title":"mlag","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/archive/2022/page/2/","title":"December 2022","text":""},{"location":"blog/archive/2021/page/2/","title":"December 2021","text":""},{"location":"blog/category/cumulus/page/2/","title":"cumulus","text":""},{"location":"blog/category/lisp/page/2/","title":"lisp","text":""},{"location":"blog/category/sda/page/2/","title":"sda","text":""}]}